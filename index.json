[{"id":3,"pagetitle":"Introduction","title":"DynamicalSystems","ref":"/dynamicalsystems/stable/#DynamicalSystems","content":" DynamicalSystems  —  Module DynamicalSystems.jl  is an  award-winning  Julia software library for nonlinear dynamics and nonlinear timeseries analysis. The current repository holds the documentation and exports all packages composing DynamicalSystems.jl. To install it, run  import Pkg; Pkg.add(\"DynamicalSystems\") . DynamicalSystems.jl  is part of  JuliaDynamics , an organization dedicated to creating high quality scientific software. All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source Star us on GitHub! If you have found this library useful, please consider starring it on  GitHub . This gives us an accurate lower bound of the (satisfied) user count."},{"id":4,"pagetitle":"Introduction","title":"Introduction","ref":"/dynamicalsystems/stable/#Introduction","content":" Introduction Welcome to the documentation of  DynamicalSystems.jl ! If you have not used the library before, and would like to get started, then please read the  overarching tutorial  for the library. The  contents  page gives a summary of all packages that are part of the library. See the  learning resources  below to find out more resources about learning the library and using it in scientific research and/or education. Besides the formal algorithmic/scientific content of  DynamicalSystems.jl  (those in the  contents ) page, the library also provides an extensive suite for interactive or offline animations and visualizations dynamical systems. These are found in the  visualizations  page. The remaining of this introduction page discusses our goals with the library, how to participate as a user or developer, how to cite, and other relevant information (see the sections of the sidebar on the left)."},{"id":5,"pagetitle":"Introduction","title":"Latest news","ref":"/dynamicalsystems/stable/#Latest-news","content":" Latest news DynamicalSystems.jl moved to Julia 1.9+, and now exports visualization and interactive applications automatically once Makie (or Makie backends such as GLMakie) come into scope, using the new package extension system. The package InteractiveDynamics.jl is now obsolete."},{"id":6,"pagetitle":"Introduction","title":"Learning resources","ref":"/dynamicalsystems/stable/#learning","content":" Learning resources"},{"id":7,"pagetitle":"Introduction","title":"Textbook with DynamicalSystems.jl","ref":"/dynamicalsystems/stable/#Textbook-with-DynamicalSystems.jl","content":" Textbook with DynamicalSystems.jl We have written an undergraduate level textbook as an introduction to nonlinear dynamics. The text is written in an applied, hands-on manner, while still covering all fundamentals. The book pages are interlaced with real Julia code that uses DynamicalSystems.jl and is published in the Undergraduate Lecture Notes in Physics by Springer Nature: Nonlinear Dynamics: A concise introduction interlaced with code  by G. Datseris & U. Parlitz. Additional textbooks on nonlinear dynamics worth having a look are: Chaos in Dynamical Systems - E. Ott Nonlinear Time series Analysis - H. Kantz & T. Schreiber"},{"id":8,"pagetitle":"Introduction","title":"Course on applied nonlinear dynamics and complex systems","ref":"/dynamicalsystems/stable/#Course-on-applied-nonlinear-dynamics-and-complex-systems","content":" Course on applied nonlinear dynamics and complex systems We are developing a full course (targeting a graduate or undergraduate semester long course) on applied nonlinear dynamics, nonlinear timeseries analysis, and complex systems, using the packages of  JuliaDynamics .  DynamicalSystems.jl  is part of this course. The materials of the course are on GitHub:  https://github.com/JuliaDynamics/NonlinearDynamicsComplexSystemsCourses"},{"id":9,"pagetitle":"Introduction","title":"Our Goals","ref":"/dynamicalsystems/stable/#goals","content":" Our Goals DynamicalSystems.jl  was created with three goals in mind. The first was to fill the missing gap of a high quality and general purpose software for nonlinear dynamics, which can make the field of nonlinear dynamics accessible and reproducible. The second goal was to create a useful  library  where students and scientists from different fields may come and learn about methods of nonlinear dynamics. The third goal was to fundamentally change the perception of the role of code in both scientific education as well as research. It is rarely the case that real,  runnable  code is shown in the classroom, because it is often long and messy. This is especially hurtful for nonlinear dynamics, a field where computer-assisted exploration is critical. And published work in this field fares even worse, with the overwhelming majority of published research not sharing the code used to create the paper. This makes reproducing these papers difficult, while some times straight-out impossible. To achieve these goals we made  DynamicalSystems.jl  so that it is: Transparent : extra care is taken so that the source code of all functions is clear and easy to follow, while remaining as small and concise as possible. Intuitive : a software simple to use and understand makes experimentation easier. Easy to extend : this makes contributions more likely, and can motivate researchers to implement their method here, instead of leaving it in a cryptic script stored in some data server, never-to-be-published with the paper. Reliable : the algorithm implementations are tested extensively. Well-documented : all implemented algorithms provide a high-level scientific description of their functionality in their documentation string as well as references to scientific papers. General : all algorithms work just as well with any system, whether it is a simple continuous chaotic system, like the Lorenz model, or a high dimensional discrete system like coupled standard maps. Performant : written entirely in Julia, and taking advantage of some of the best packages within the language,  DynamicalSystems.jl  is  really fast ."},{"id":10,"pagetitle":"Introduction","title":"Citing","ref":"/dynamicalsystems/stable/#Citing","content":" Citing There is a (small) paper associated with  DynamicalSystems.jl . If we have helped you in research that led to a publication, please be kind enough to cite it, using the DOI  10.21105/joss.00598  or the following BiBTeX entry: @article{Datseris2018,\n  doi = {10.21105/joss.00598},\n  url = {https://doi.org/10.21105/joss.00598},\n  year  = {2018},\n  month = {mar},\n  volume = {3},\n  number = {23},\n  pages = {598},\n  author = {George Datseris},\n  title = {DynamicalSystems.jl: A Julia software library for chaos and nonlinear dynamics},\n  journal = {Journal of Open Source Software}\n} however, we would really appreciate it if you also cited the textbook we wrote that  DynamicalSystems.jl  accompanies: @book{DatserisParlitz2022,\n  doi = {10.1007/978-3-030-91032-7},\n  url = {https://doi.org/10.1007/978-3-030-91032-7},\n  year = {2022},\n  publisher = {Springer Nature},\n  author = {George Datseris and Ulrich Parlitz},\n  title     = \"Nonlinear dynamics: A concise introduction interlaced with code\",\n  address   = \"Cham, Switzerland\",\n  language  = \"en\",\n}"},{"id":11,"pagetitle":"Introduction","title":"Asking questions","ref":"/dynamicalsystems/stable/#Asking-questions","content":" Asking questions There are three options for asking questions: Join the official  Julia discourse  and ask a question under the category Specific Domains > Modelling & Simulations. Join our channel  #dynamics-bridged  in the  Julia Slack  workplace. Open an issue directly on the GitHub page of DynamicalSystems.jl while providing a Minimal Working Example."},{"id":12,"pagetitle":"Introduction","title":"Contributing & Donating","ref":"/dynamicalsystems/stable/#Contributing-and-Donating","content":" Contributing & Donating Be sure to visit the  Contributor Guide  page, because you can help make this package better without having to write a single line of code! Also, if you find this package helpful please consider staring it on  GitHub ! This gives us an accurate lower bound of users that this package has already helped! Finally, you can donate for the development of  DynamicalSystems.jl . You can do that by adding bounties to existing issues on the GitHub repositories (you can open new issues as well). Every issue has an automatic way to create a bounty using  Bountysource , see the first comment of each issue."},{"id":13,"pagetitle":"Introduction","title":"Issues with Bounties","ref":"/dynamicalsystems/stable/#Issues-with-Bounties","content":" Issues with Bounties Money that  DynamicalSystems.jl  obtains from awards, sponsors, or donators are converted into bounties for GitHub issues. The full list of issues that have a bounty is  available here . By solving these issues you not only contribute to open source, but you also get some pocket money to boot :)"},{"id":14,"pagetitle":"Introduction","title":"Maintainers and Contributors","ref":"/dynamicalsystems/stable/#Maintainers-and-Contributors","content":" Maintainers and Contributors The  DynamicalSystems.jl  library is maintained by  George Datseris , who is also curating and writing this documentation. The software code however is built from the contributions of several individuals. The list is too long to write and constantly update, so the best way to find out these contributions is to visit the GitHub page of each of the subpackages and checkout the \"contributors\" pages there."},{"id":15,"pagetitle":"Introduction","title":"Version numbers","ref":"/dynamicalsystems/stable/#Version-numbers","content":" Version numbers The version of  DynamicalSystems  by itself is a bit meaningless, because the module does not have any source code, besides re-exporting other modules. For transparency, the packages and versions used to build the documentation you are reading now are: using Pkg\nPkg.status([\n    \"DynamicalSystems\",\n    \"StateSpaceSets\", \"DynamicalSystemsBase\", \"RecurrenceAnalysis\", \"FractalDimensions\", \"DelayEmbeddings\", \"ComplexityMeasures\", \"TimeseriesSurrogates\", \"PredefinedDynamicalSystems\", \"Attractors\", \"ChaosTools\", \"CairoMakie\",\n    ];\n    mode = PKGMODE_MANIFEST\n) Status `~/work/DynamicalSystems.jl/DynamicalSystems.jl/docs/Manifest.toml`\n  [f3fd9213] Attractors v1.7.1\n  [13f3f980] CairoMakie v0.10.6\n  [608a59af] ChaosTools v3.0.2\n  [ab4b797d] ComplexityMeasures v2.7.2\n  [5732040d] DelayEmbeddings v2.7.1\n  [61744808] DynamicalSystems v3.2.0 `~/work/DynamicalSystems.jl/DynamicalSystems.jl`\n  [6e36e845] DynamicalSystemsBase v3.2.0\n  [4665ce21] FractalDimensions v1.5.1\n  [31e2f376] PredefinedDynamicalSystems v1.1.1\n  [639c3291] RecurrenceAnalysis v2.0.5\n  [40b095a5] StateSpaceSets v1.3.6\n  [c804724b] TimeseriesSurrogates v2.3.1 Version numbers do not strictly follow SemVer2.0 Because of the nature of the  DynamicalSystems.jl  library, the exported API contains hundreds of algorithm implementations, most of which are independent of each other. Our development approach is that breaking changes to these individual algorithms (due to e.g., better API design or better performance implementations or better default keyword arguments) can be done  without incrementing any major version numbers . We increment major version numbers only for breaking changes that have wide impact over most of the  DynamicalSystems.jl  library."},{"id":16,"pagetitle":"Introduction","title":"Other NLD-relevant packages","ref":"/dynamicalsystems/stable/#Other-NLD-relevant-packages","content":" Other NLD-relevant packages Besides DynamicalSystems.jl, the Julia programming language has a thriving ecosystem with plenty of functionality that is relevant for nonlinear dynamics. We list some useful references below: DifferentialEquations.jl  - Besides providing solvers for standard ODE systems (infastructure already used in DynamicalSystems.jl), it also has much more features like SDE solvers or uncertainty quantification. DiffEqSensitivity.jl  - Discrete and continuous local sensitivity analysis, i.e., derivatives of the solutions of ODEs, or functions of the solutions, versus parameters, hosting  various forward and adjoint methods as well as methods tailored to chaotic systems . GlobalSensitivity.jl  Global sensitivity analysis assessing the effect of any input variables over a larger domain on the output. BifurcationKit.jl  - Featureful toolkit for automated bifurcation analysis. NetworkDynamics.jl  - Package for easily simulating dynamics on networks and transforming network systems into  ODEProblem  (that can be made directly into a  ContinuousDynamicalSystem ). Agents.jl  for agent based modelling."},{"id":19,"pagetitle":"Contents","title":"Contents","ref":"/dynamicalsystems/stable/contents/#contents","content":" Contents When you do  using DynamicalSystems  in your Julia session, the module re-exports and brings into scope all submodules (Julia packages) that compose  DynamicalSystems.jl . These are listed in this page. Of course, you could be using these packages directly instead of adding  DynamicalSystems . However, doing  using DynamicalSystems  provides the environment all these packages were designed to work together in, and so we recommend to simply install  DynamicalSystems  and use that."},{"id":20,"pagetitle":"Contents","title":"Exported submodules","ref":"/dynamicalsystems/stable/contents/#Exported-submodules","content":" Exported submodules The submodules that compose  DynamicalSystems.jl  are the following packages, which are re-exported by  DynamicalSystems : Core StateSpaceSets DynamicalSystemsBase For observed/measured data ComplexityMeasures RecurrenceAnalysis DelayEmbeddings FractalDimensions TimeseriesSurrogates For dynamical system instances PredefinedDynamicalSystems ChaosTools Attractors At the very end of this page, a full list of exported names is presented."},{"id":21,"pagetitle":"Contents","title":"Core","ref":"/dynamicalsystems/stable/contents/#Core","content":" Core"},{"id":22,"pagetitle":"Contents","title":"StateSpaceSets.StateSpaceSets","ref":"/dynamicalsystems/stable/contents/#StateSpaceSets.StateSpaceSets","content":" StateSpaceSets.StateSpaceSets  —  Module StateSpaceSets.jl A Julia package that provides functionality for state space sets. These are collections of points of fixed, and known by type, size (called dimension). It is used in several projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . The main export of  StateSpaceSets  is the concrete type  StateSpaceSet . The package also provides functionality for distances, neighbor searches, sampling, and normalization. To install it you may run  import Pkg; Pkg.add(\"StateSpaceSets\") , however, there is no real reason to install this package directly as it is re-exported by all downstream packages that use it. previously StateSpaceSets.jl was part of DelayEmbeddings.jl"},{"id":23,"pagetitle":"Contents","title":"DynamicalSystemsBase.DynamicalSystemsBase","ref":"/dynamicalsystems/stable/contents/#DynamicalSystemsBase.DynamicalSystemsBase","content":" DynamicalSystemsBase.DynamicalSystemsBase  —  Module DynamicalSystemsBase.jl A Julia package that defines the  DynamicalSystem  interface and many concrete implementations used in the  DynamicalSystems.jl  ecosystem. To install it, run  import Pkg; Pkg.add(\"DynamicalSystemsBase\") . Typically, you do not want to use  DynamicalSystemsBase  directly, as downstream analysis packages re-export it. All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file."},{"id":24,"pagetitle":"Contents","title":"For observed/measured data","ref":"/dynamicalsystems/stable/contents/#For-observed/measured-data","content":" For observed/measured data"},{"id":25,"pagetitle":"Contents","title":"ComplexityMeasures.ComplexityMeasures","ref":"/dynamicalsystems/stable/contents/#ComplexityMeasures.ComplexityMeasures","content":" ComplexityMeasures.ComplexityMeasures  —  Module ComplexityMeasures.jl A Julia package that provides estimators for probabilities, entropies, and other complexity measures, in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . To install it, run  import Pkg; Pkg.add(\"ComplexityMeasures\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was called Entropies.jl."},{"id":26,"pagetitle":"Contents","title":"RecurrenceAnalysis.RecurrenceAnalysis","ref":"/dynamicalsystems/stable/contents/#RecurrenceAnalysis.RecurrenceAnalysis","content":" RecurrenceAnalysis.RecurrenceAnalysis  —  Module RecurrenceAnalysis.jl A Julia module that offers tools for computing Recurrence Plots and exploring them within the framework of Recurrence Quantification Analysis and Recurrence Network Analysis. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"RecurrenceAnalysis\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file."},{"id":27,"pagetitle":"Contents","title":"DelayEmbeddings.DelayEmbeddings","ref":"/dynamicalsystems/stable/contents/#DelayEmbeddings.DelayEmbeddings","content":" DelayEmbeddings.DelayEmbeddings  —  Module DelayEmbeddings.jl A Julia package that provides a generic interface for performing delay coordinate embeddings, as well as cutting edge algorithms for creating optimal embeddings given some data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"DelayEmbeddings\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file."},{"id":28,"pagetitle":"Contents","title":"FractalDimensions.FractalDimensions","ref":"/dynamicalsystems/stable/contents/#FractalDimensions.FractalDimensions","content":" FractalDimensions.FractalDimensions  —  Module FractalDimensions.jl A Julia package that estimates various definitions of fractal dimension from data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"FractalDimensions\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was part of ChaosTools.jl. Citation If you use this package in a publication, please cite the paper below: @ARTICLE{FractalDimensions.jl,\n  title     = \"Estimating the fractal dimension: a comparative review and open\n               source implementations\",\n  author    = \"Datseris, George and Kottlarz, Inga and Braun, Anton P and\n               Parlitz, Ulrich\",\n  publisher = \"arXiv\",\n  year      =  2021,\n  doi = {10.48550/ARXIV.2109.05937},\n  url = {https://arxiv.org/abs/2109.05937},\n}"},{"id":29,"pagetitle":"Contents","title":"TimeseriesSurrogates.TimeseriesSurrogates","ref":"/dynamicalsystems/stable/contents/#TimeseriesSurrogates.TimeseriesSurrogates","content":" TimeseriesSurrogates.TimeseriesSurrogates  —  Module TimeseriesSurrogates.jl A Julia package for generating timeseries surrogates. TimeseriesSurrogates.jl is the fastest and most featureful open source code for generating timeseries surrogates. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"TimeseriesSurrogates\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Citing Please use the following BiBTeX entry, or DOI, to cite TimeseriesSurrogates.jl: DOI: https://doi.org/10.21105/joss.04414 BiBTeX: @article{TimeseriesSurrogates.jl,\n    doi = {10.21105/joss.04414},\n    url = {https://doi.org/10.21105/joss.04414},\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {77},\n    pages = {4414},\n    author = {Kristian Agasøster Haaga and George Datseris},\n    title = {TimeseriesSurrogates.jl: a Julia package for generating surrogate data},\n    journal = {Journal of Open Source Software}\n}"},{"id":30,"pagetitle":"Contents","title":"For dynamical system instances","ref":"/dynamicalsystems/stable/contents/#For-dynamical-system-instances","content":" For dynamical system instances"},{"id":31,"pagetitle":"Contents","title":"PredefinedDynamicalSystems.PredefinedDynamicalSystems","ref":"/dynamicalsystems/stable/contents/#PredefinedDynamicalSystems.PredefinedDynamicalSystems","content":" PredefinedDynamicalSystems.PredefinedDynamicalSystems  —  Module PredefinedDynamicalSystems.jl Module which contains pre-defined dynamical systems that can be used by the  DynamicalSystems.jl  library. To install it, run  import Pkg; Pkg.add(\"PredefinedDynamicalSystems\") . Predefined systems exist as functions that return a  DynamicalSystem  instance. They are accessed like: ds = PredefinedDynamicalSystems.lorenz(u0; ρ = 32.0) The alias  Systems  is also exported as a deprecation. This module is provided purely as a convenience. It does not have any actual tests, and it is not guaranteed to be stable in future versions. It is not recommended to use this module for anything else besides on-the-spot demonstrative examples. For some systems, a Jacobian function is also defined. The naming convention for the Jacobian function is  \\$(name)_jacob . So, for the above example we have  J = Systems.lorenz_jacob . All available systems are provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file."},{"id":32,"pagetitle":"Contents","title":"ChaosTools.ChaosTools","ref":"/dynamicalsystems/stable/contents/#ChaosTools.ChaosTools","content":" ChaosTools.ChaosTools  —  Module ChaosTools.jl A Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"ChaosTools\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. ChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl."},{"id":33,"pagetitle":"Contents","title":"Attractors.Attractors","ref":"/dynamicalsystems/stable/contents/#Attractors.Attractors","content":" Attractors.Attractors  —  Module Attractors.jl A Julia module for finding attractors of arbitrary dynamical systems finding their basins of attraction or the state space fractions of the basins \"continuing\" the attractors and their basins over a parameter range finding the basin boundaries and analyzing their fractal properties tipping points related functionality for systems with known dynamic rule and more! It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"Attractors\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, Attractors.jl was part of ChaosTools.jl"},{"id":34,"pagetitle":"Contents","title":"All exported names","ref":"/dynamicalsystems/stable/contents/#All-exported-names","content":" All exported names This section lists all exported names of the  DynamicalSystems.jl  library. We do not list their documentation in any way here. This list is only meant as a quantitative listing of features, as well as perhaps helping searching via the search bar. To actually learn how to use all these exported names you need to use above-linked documentation of the respective submodules! The total exported names are: using DynamicalSystems\nall_exported_names = names(DynamicalSystems)\nlength(all_exported_names) 389 And they are: using DisplayAs\nDisplayAs.unlimited(all_exported_names) 389-element Array{Symbol, 1}:\n :..\n Symbol(\"@windowed\")\n :AAFT\n :AR1\n :AbstractBinning\n :AbstractDataset\n :AbstractEmbedding\n :AbstractRecurrenceType\n :AbstractStateSpaceSet\n :AlizadehArghami\n :AllSlopesDistribution\n :ApproximateEntropy\n :ArbitrarySteppable\n :AttractorMapper\n :Attractors\n :AttractorsBasinsContinuation\n :AttractorsViaFeaturizing\n :AttractorsViaProximity\n :AttractorsViaRecurrences\n :AutoRegressive\n :BlockShuffle\n :BruteForce\n :Centroid\n :ChaosTools\n :Chebyshev\n :CircShift\n :Cityblock\n :ClusteringConfig\n :ComplexityEstimator\n :ComplexityMeasure\n :ComplexityMeasures\n :Composite\n :ContinuousDynamicalSystem\n :ContinuousTimeDynamicalSystem\n :CoreDynamicalSystem\n :Correa\n :CountOccurrences\n :CoupledODEs\n :CrossRecurrenceMatrix\n :CrossingAccurateInterpolation\n :CrossingLinearIntersection\n :Curado\n :CycleShuffle\n :Dataset\n :DelayEmbedding\n :DelayEmbeddings\n :DeterministicIteratedMap\n :DiffEntropyEst\n :DifferentialEntropyEstimator\n :DiscEntropyEst\n :DiscreteDynamicalSystem\n :DiscreteEntropyEstimator\n :DiscreteTimeDynamicalSystem\n :Dispersion\n :Diversity\n :DynamicalSystem\n :DynamicalSystems\n :DynamicalSystemsBase\n :Ebrahimi\n :Encoding\n :EntropyDefinition\n :Euclidean\n :FAN\n :FT\n :FeaturizeGroupAcrossParameter\n :FirstElement\n :FixedRectangularBinning\n :FractalDimensions\n :Gao\n :GaussianCDFEncoding\n :GeneralizedEmbedding\n :GlobalRecurrenceRate\n :Goria\n :GroupAcrossParameter\n :GroupAcrossParameterContinuation\n :GroupViaClustering\n :GroupViaHistogram\n :GroupViaNearestFeature\n :GroupingConfig\n :Hausdorff\n :IAAFT\n :IntervalBox\n :InvariantMeasure\n :IrregularLombScargle\n :JointRecurrenceMatrix\n :KDTree\n :Kaniadakis\n :KozachenkoLeonenko\n :Kraskov\n :LargestLinearRegion\n :LinearRegression\n :LocalRecurrenceRate\n :Lord\n :MLEntropy\n :MissingDispersionPatterns\n :NLNS\n :NSAR2\n :NaiveKernel\n :NeighborNumber\n :OrdinalPatternEncoding\n :ParallelDynamicalSystem\n :PartialRandomization\n :PartialRandomizationAAFT\n :PlaneCrossing\n :PoincareMap\n :PowerSpectrum\n :PredefinedDynamicalSystems\n :Probabilities\n :ProbabilitiesEstimator\n :ProjectedDynamicalSystem\n :PseudoPeriodic\n :PseudoPeriodicTwin\n :RAFM\n :RandomCascade\n :RandomFourier\n :RandomShuffle\n :RectangularBinEncoding\n :RectangularBinning\n :RecurrenceAnalysis\n :RecurrenceMatrix\n :RecurrenceThreshold\n :RecurrenceThresholdScaled\n :RecurrencesFindAndMatch\n :RecurrencesSeededContinuation\n :Regular\n :Renyi\n :ReverseDispersion\n :SMatrix\n :SNLST\n :SVector\n :SampleEntropy\n :Shannon\n :ShuffleDimensions\n :SpatialDispersion\n :SpatialSymbolicPermutation\n :StateSpaceSet\n :StateSpaceSets\n :StatisticalComplexity\n :StretchedExponential\n :StrictlyMinimumDistance\n :StroboscopicMap\n :Surrogate\n :SurrogateTest\n :SymbolicAmplitudeAwarePermutation\n :SymbolicPermutation\n :SymbolicWeightedPermutation\n :Systems\n :TAAFT\n :TFTD\n :TFTDAAFT\n :TFTDIAAFT\n :TFTDRandomFourier\n :TFTS\n :TangentDynamicalSystem\n :TimeScaleMODWT\n :TimeseriesSurrogates\n :TransferOperator\n :Tsallis\n :ValueHistogram\n :Vasicek\n :VisitationFrequency\n :WLS\n :WaveletOverlap\n :WithinRange\n :Zhu\n :ZhuSingh\n :aggregate_attractor_fractions\n :animate_attractors_continuation\n :autocor\n :automatic_Δt_basins\n :basin_entropy\n :basins_fractal_dimension\n :basins_fractal_test\n :basins_fractions\n :basins_of_attraction\n :beta_statistic\n :boxassisted_correlation_dim\n :boxed_correlationsum\n :broomhead_king\n :colored_noise\n :columns\n :complexity\n :complexity_normalized\n :continuation\n :convert_logunit\n :coordinates\n :correlationsum\n :current_crossing_time\n :current_deviations\n :current_parameters\n :current_state\n :current_states\n :current_time\n :dataset_distance\n :datasets_sets_distances\n :decode\n :delay_afnn\n :delay_f1nn\n :delay_fnn\n :delay_ifnn\n :determinism\n :dimension\n :distancematrix\n :divergence\n :dl_average\n :dl_entropy\n :dl_max\n :dyca\n :dynamic_rule\n :embed\n :encode\n :entropy\n :entropy_approx\n :entropy_complexity\n :entropy_complexity_curves\n :entropy_dispersion\n :entropy_maximum\n :entropy_normalized\n :entropy_permutation\n :entropy_sample\n :entropy_wavelet\n :estimate_boxsizes\n :estimate_delay\n :estimate_gpd_parameters\n :estimate_period\n :estimate_r0_buenoorovio\n :estimate_r0_theiler\n :exit_entry_times\n :expansionentropy\n :exponential_decay_fit\n :extract_attractors\n :extract_features\n :extremal_index_sueveges\n :extremevaltheory_dim\n :extremevaltheory_dims\n :extremevaltheory_dims_persistences\n :extremevaltheory_local_dim_persistence\n :findlocalextrema\n :findlocalminima\n :first_return_times\n :fixedmass_correlation_dim\n :fixedmass_correlationsum\n :fixedpoints\n :gali\n :garcia_almeida_embedding\n :genembed\n :genentropy\n :generalized_dim\n :get_deviations\n :get_state\n :grassberger_proccacia_dim\n :grayscale\n :group_features\n :heatmap_basins_attractors\n :heatmap_basins_attractors!\n :higuchi_dim\n :initial_parameters\n :initial_state\n :initial_states\n :initial_time\n :integrator\n :interactive_cobweb\n :interactive_orbitdiagram\n :interactive_poincaresos\n :interactive_poincaresos_scan\n :interactive_trajectory\n :interactive_trajectory_timeseries\n :interval\n :invariantmeasure\n :isdeterministic\n :isdiscretetime\n :isinplace\n :kaplanyorke_dim\n :lambdamatrix\n :lambdaperms\n :laminarity\n :linear_region\n :linear_regions\n :linreg\n :local_growth_rates\n :lyapunov\n :lyapunov_from_data\n :lyapunovspectrum\n :match_attractor_ids!\n :match_basins_ids!\n :match_statespacesets!\n :maxima\n :mdop_embedding\n :mdop_maximum_delay\n :mean_return_times\n :meanrecurrencetime\n :minima\n :minimum_pairwise_distance\n :minmaxima\n :missing_outcomes\n :molteno_boxing\n :molteno_dim\n :n_statistic\n :nmprt\n :noiseradius\n :optimal_traditional_de\n :orbitdiagram\n :orthonormal\n :outcome_space\n :outcomes\n :parallel_integrator\n :pecora\n :pecuzal_embedding\n :periodicorbits\n :permentropy\n :plot_attractors_curves\n :plot_attractors_curves!\n :plot_basins_attractors_curves\n :plot_basins_attractors_curves!\n :plot_basins_curves\n :plot_basins_curves!\n :poincaremap\n :poincaresos\n :pointwise_correlationsums\n :pointwise_dimensions\n :predictability\n :prismdim_theiler\n :probabilities\n :probabilities!\n :probabilities_and_outcomes\n :produce_orbitdiagram\n :projected_integrator\n :pvalue\n :random_cycles\n :randomwalk\n :reconstruct\n :recurrence_threshold\n :recurrenceplot\n :recurrencerate\n :recurrencestructures\n :recursivecopy\n :reinit!\n :rematch!\n :replacement_map\n :rna\n :rqa\n :rt_average\n :rt_entropy\n :rt_max\n :scaleod\n :selfmutualinfo\n :set_deviations!\n :set_distance\n :set_parameter!\n :set_parameters!\n :set_period!\n :set_state!\n :setsofsets_distance\n :setsofsets_distances\n :skeletonize\n :slopefit\n :sorteddistances\n :standardize\n :statespace_sampler\n :step!\n :stochastic_indicator\n :successful_step\n :surrogate\n :surrogenerator\n :surroplot_path\n :swap_dict_keys!\n :takens_best_estimate_dim\n :tangent_integrator\n :testchaos01\n :textrecurrenceplot\n :tipping_probabilities\n :total_outcomes\n :trajectory\n :transfermatrix\n :transit_return_times\n :transitivity\n :trappingtime\n :trend\n :uncertainty_exponent\n :unique_keys\n :uzal_cost\n :uzal_cost_local\n :vl_average\n :vl_entropy\n :vl_max\n :windowed\n :yin\n :×\n :τrange"},{"id":37,"pagetitle":"Contributor Guide","title":"Contributor Guide","ref":"/dynamicalsystems/stable/contributors_guide/#Contributor-Guide","content":" Contributor Guide TL;DR: To contribute via pull requests you can check issues that have labels \"wanted feature\" or \"good first issue\" in the GitHub repositories of the subpackages of DynamicalSystems.jl The ultimate goal for  DynamicalSystems.jl  is to be a useful  library  for scientists working on nonlinear dynamics and to make nonlinear dynamics accessible and reproducible. Of course, for such an ambitious goal to be achieved, many of us should try to work together to improve the library! If you want to help the cause, there are many ways to contribute to the  DynamicalSystems.jl  library: Just  use it . If you encountered unexpected behavior simply report it either on our  gitter chatroom  or using the  DynamicalSystems.jl Issues  page. Suggest methods that you think should be included in our library. This should be done by opening a new issue that describes the method, gives references to papers using the method and also justifies why the method should be included. Contribute code by solving issues. The easiest issues to tackle are the ones with label  \"good first issue\" . Contribute code by implementing new methods! That is the most  awesome  way to contribute! The individual packages that compose  DynamicalSystems.jl  have plenty of issues with the tag  \"wanted feature\" , which can get you started on a big contribution! Contribute code by defining a new pre-defined dynamical system that you found useful."},{"id":38,"pagetitle":"Contributor Guide","title":"Contributing Code","ref":"/dynamicalsystems/stable/contributors_guide/#Contributing-Code","content":" Contributing Code When contributing code, you should keep these things in mind: In general, the speed of the implementation is important, but not as important as the  clarity of the implementation . One of cornerstones of all of  DynamicalSystems.jl  is to have clear and readable source code. Fortunately, Julia allows you to have perfectly readable code but also super fast ;) If necessary add comments to the code, so that somebody that knows the method, can also understand the code immediately. Try to design general, extendable functions instead of unnecessarily specialized to the case at hand. For the documentation strings of new methods and systems please follow the convention of the documentation strings of DynamicalSystems.jl. Specifically, the first section should describe the function in a couple of sentences, its positional arguments and its return value. The next section  ## Keyword Arguments  describes the keywords. The next section  ## Description  describes the algorithm in detail if need be. Lastly, papers that are relevant to the method must be cited. Have a look at the documentation strings of  lyapunov  and  lyapunovspectrum  to get an idea."},{"id":39,"pagetitle":"Contributor Guide","title":"Documentation string style","ref":"/dynamicalsystems/stable/contributors_guide/#Documentation-string-style","content":" Documentation string style Documentation strings are the most important thing in your pull request/code. The number 1 priority of DynamicalSystems.jl is highest possible quality of documentation and utmost transparency, and the best way to achieve this is with good documentation strings. In DynamicalSystems.jl we recommend that documentation strings are structured in the following way (and this is also the recommendation we give in the  Good Scientific Code Workshop ). Clear call signature in code syntax, including expected input types if necessary. The call signature should ONLY include only the most important information, not list out in detail every keyword! Brief summary of the function [Optional] Return value and type if not obvious [Optional] References to related functions if sensible [Optional] Keyword arguments list if the function has some [Optional] Detailed discussion of functionality if function behavior is scientifically involved [Optional] Citations to relevant scientific papers! The syntax of the documentation strings follows Documenter.jl protocol. Please see the documentation string of the  lyapunov  function and use the same structure."},{"id":44,"pagetitle":"Overarching tutorial","title":"Overarching tutorial for DynamicalSystems.jl","ref":"/dynamicalsystems/stable/tutorial/#tutorial","content":" Overarching tutorial for DynamicalSystems.jl This page serves as a short, but to-the-point, introduction to the  DynamicalSystems.jl  library. It outlines the core components, and how they establish an interface that is used by the rest of the library. It also provides a couple of usage examples to connect the various packages of the library together. Going through this tutorial should take you about 20 minutes."},{"id":45,"pagetitle":"Overarching tutorial","title":"Installation","ref":"/dynamicalsystems/stable/tutorial/#Installation","content":" Installation To install  DynamicalSystems.jl , simply do: using Pkg; Pkg.add(\"DynamicalSystems\") As discussed in the  contents  page, this installs several packages for the Julia language, that are all exported under a common name. To use them, simply do: using DynamicalSystems in your Julia session."},{"id":46,"pagetitle":"Overarching tutorial","title":"Core components","ref":"/dynamicalsystems/stable/tutorial/#Core-components","content":" Core components The individual packages that compose  DynamicalSystems  interact flawlessly with each other because of the following two components: The  StateSpaceSet , which represents numerical data. They can be observed or measured from experiments, sampled trajectories of dynamical systems, or just unordered sets in a state space. A  StateSpaceSet  is a container of equally-sized points, representing multivariate timeseries or multivariate datasets. Timeseries, which are univariate sets, are represented by the  AbstractVector{<:Real}  Julia base type. The  DynamicalSystem , which is the abstract representation of a dynamical system with a known dynamic evolution rule.  DynamicalSystem  defines an extendable interface, but typically one uses concrete implementations such as  DeterministicIteratedMap  or  CoupledODEs ."},{"id":47,"pagetitle":"Overarching tutorial","title":"Making dynamical systems","ref":"/dynamicalsystems/stable/tutorial/#Making-dynamical-systems","content":" Making dynamical systems In the majority of cases, to make a dynamical system one needs three things: The dynamic rule  f : A Julia function that provides the instructions of how to evolve the dynamical system in time. The state  u : An array-like container that contains the variables of the dynamical system and also defines the starting state of the system. The parameters  p : An arbitrary container that parameterizes  f . For most concrete implementations of  DynamicalSystem  there are two ways of defining  f, u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f ."},{"id":48,"pagetitle":"Overarching tutorial","title":"Example: Henon map","ref":"/dynamicalsystems/stable/tutorial/#Example:-Henon-map","content":" Example: Henon map Let's make the Henon map, defined as \\[\\begin{aligned}\nx_{n+1} &= 1 - ax^2_n+y_n \\\\\ny_{n+1} & = bx_n\n\\end{aligned}\\] with parameters  $a = 1.4, b = 0.3$ . First, we define the dynamic rule as a standard Julia function. Since the dynamical system is only two-dimensional, we should use the  out-of-place  form that returns an  SVector  with the next state: using DynamicalSystems\n\nfunction henon_rule(u, p, n) # here `n` is \"time\", but we don't use it.\n    x, y = u # system state\n    a, b = p # system parameters\n    xn = 1.0 - a*x^2 + y\n    yn = b*x\n    return SVector(xn, yn)\nend henon_rule (generic function with 1 method) Then, we define initial state and parameters u0 = [0.2, 0.3]\np0 = [1.4, 0.3] 2-element Vector{Float64}:\n 1.4\n 0.3 Lastly, we give these three to the  DeterministicIteratedMap : henon = DeterministicIteratedMap(henon_rule, u0, p0) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          0\n state:         [0.2, 0.3]\n henon  is a  DynamicalSystem , one of the two core structures of the library. They can evolved interactively, and queried, using the interface defined by  DynamicalSystem . The simplest thing you can do with a  DynamicalSystem  is to get its trajectory: total_time = 10_000\nX, t = trajectory(henon, total_time) (2-dimensional StateSpaceSet{Float64} with 10001 points, 0:1:10000) X 2-dimensional StateSpaceSet{Float64} with 10001 points\n  0.2        0.3\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311\n  0.540361   0.151562\n  0.742777   0.162108\n  0.389703   0.222833\n  1.01022    0.116911\n -0.311842   0.303065\n  ⋮         \n -0.582534   0.328346\n  0.853262  -0.17476\n -0.194038   0.255978\n  1.20327   -0.0582113\n -1.08521    0.36098\n -0.287758  -0.325562\n  0.558512  -0.0863275\n  0.476963   0.167554\n  0.849062   0.143089 X  is a  StateSpaceSet , the second of the core structures of the library. We'll see below how, and where, to use a  StateSpaceset , but for now let's just do a scatter plot using CairoMakie\nscatter(X[:, 1], X[:, 2])"},{"id":49,"pagetitle":"Overarching tutorial","title":"Example: Lorenz96","ref":"/dynamicalsystems/stable/tutorial/#Example:-Lorenz96","content":" Example: Lorenz96 Let's also make another dynamical system, the Lorenz96 model: \\[\\frac{dx_i}{dt} = (x_{i+1}-x_{i-2})x_{i-1} - x_i + F\\] for  $i \\in \\{1, \\ldots, N\\}$  and  $N+j=j$ . Here, instead of a discrete time map we have  $N$  coupled ordinary differential equations. However, creating the dynamical system works out just like above, but using  CoupledODEs  instead of  DeterministicIteratedMap . First, we make the dynamic rule function. Since this dynamical system can be arbitrarily high-dimensional, we prefer to use the  in-place  form for  f , overwriting in place the rate of change in a pre-allocated container. It is  customary  to append the name of functions that modify their arguments in-place with a bang ( ! ). function lorenz96_rule!(du, u, p, t)\n    F = p[1]; N = length(u)\n    # 3 edge cases\n    du[1] = (u[2] - u[N - 1]) * u[N] - u[1] + F\n    du[2] = (u[3] - u[N]) * u[1] - u[2] + F\n    du[N] = (u[1] - u[N - 2]) * u[N - 1] - u[N] + F\n    # then the general case\n    for n in 3:(N - 1)\n        du[n] = (u[n + 1] - u[n - 2]) * u[n - 1] - u[n] + F\n    end\n    return nothing # always `return nothing` for in-place form!\nend lorenz96_rule! (generic function with 1 method) then, like before, we define an initial state and parameters, and initialize the system N = 6\nu0 = range(0.1, 1; length = N)\np0 = [8.0]\nlorenz96 = CoupledODEs(lorenz96_rule!, u0, p0) 6-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      true\n dynamic rule:  lorenz96_rule!\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [8.0]\n time:          0.0\n state:         [0.1, 0.28, 0.46, 0.64, 0.82, 1.0]\n and, again like before, we may obtain a trajectory the same way total_time = 12.5\nsampling_time = 0.02\nY, t = trajectory(lorenz96, total_time; Ttr = 2.2, Δt = sampling_time)\nY 6-dimensional StateSpaceSet{Float64} with 626 points\n  3.15368   -4.40493  0.0311581  0.486735  1.89895   4.15167\n  2.71382   -4.39303  0.395019   0.66327   2.0652    4.32045\n  2.25088   -4.33682  0.693967   0.879701  2.2412    4.46619\n  1.7707    -4.24045  0.924523   1.12771   2.42882   4.58259\n  1.27983   -4.1073   1.08656    1.39809   2.62943   4.66318\n  0.785433  -3.94005  1.18319    1.6815    2.84384   4.70147\n  0.295361  -3.74095  1.2205     1.96908   3.07224   4.69114\n -0.181932  -3.51222  1.20719    2.25296   3.3139    4.62628\n -0.637491  -3.25665  1.154      2.5267    3.56698   4.50178\n -1.06206   -2.9781   1.07303    2.7856    3.82827   4.31366\n  ⋮                                                  ⋮\n  3.17245    2.3759   3.01796    7.27415   7.26007  -0.116002\n  3.29671    2.71146  3.32758    7.5693    6.75971  -0.537853\n  3.44096    3.09855  3.66908    7.82351   6.13876  -0.922775\n  3.58387    3.53999  4.04452    8.01418   5.39898  -1.25074\n  3.70359    4.03513  4.45448    8.1137    4.55005  -1.5042\n  3.78135    4.57879  4.89677    8.09013   3.61125  -1.66943\n  3.80523    5.16112  5.36441    7.90891   2.61262  -1.73822\n  3.77305    5.7684   5.84318    7.53627   1.59529  -1.71018\n  3.6934     6.38507  6.30923    6.94454   0.61023  -1.59518 We can't scatterplot something 6-dimensional but we can visualize all timeseries fig = Figure()\nax = Axis(fig[1, 1]; xlabel = \"time\", ylabel = \"variable\")\nfor var in columns(Y)\n    lines!(ax, t, var)\nend\nfig"},{"id":50,"pagetitle":"Overarching tutorial","title":"ODE solving","ref":"/dynamicalsystems/stable/tutorial/#ODE-solving","content":" ODE solving Continuous time dynamical systems are evolved through DifferentialEquations.jl. When initializing a  CoupledODEs  you can tune the solver properties to your heart's content using any of the  ODE solvers  and any of the  common solver options . For example: using OrdinaryDiffEq # accessing the ODE solvers\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\nlorenz96_vern = ContinuousDynamicalSystem(lorenz96_rule!, u0, p0; diffeq) 6-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      true\n dynamic rule:  lorenz96_rule!\n ODE solver:    Vern9\n ODE kwargs:    (abstol = 1.0e-9, reltol = 1.0e-9)\n parameters:    [8.0]\n time:          0.0\n state:         [0.1, 0.28, 0.46, 0.64, 0.82, 1.0]\n Y, t = trajectory(lorenz96_vern, total_time; Ttr = 2.2, Δt = sampling_time)\nY[end] 6-element SVector{6, Float64} with indices SOneTo(6):\n  3.8390248122550252\n  6.1557095311542325\n  6.080625689025621\n  7.278588308988913\n  1.2582152212831657\n -1.5297062916833186"},{"id":51,"pagetitle":"Overarching tutorial","title":"Using dynamical systems","ref":"/dynamicalsystems/stable/tutorial/#Using-dynamical-systems","content":" Using dynamical systems You may use the  DynamicalSystem  interface to develop algorithms that utilize dynamical systems with a known evolution rule. The two main packages of the library that do this are  ChaosTools  and  Attractors . For example, you may want to compute the Lyapunov spectrum of the Lorenz96 system from above. This is as easy as calling the  lyapunovspectrum  function with  lorenz96 steps = 10_000\nlyapunovspectrum(lorenz96, steps) 6-element Vector{Float64}:\n  0.9578297436475178\n  0.0007190960151481466\n -0.15749134695138844\n -0.7575338861926539\n -1.4036253850302072\n -4.639894729260599 As expected, there is at least one positive Lyapunov exponent (before the system is chaotic) and at least one zero Lyapunov exponent, because the system is continuous time. Alternatively, you may want to estimate the basins of attraction of a multistable dynamical system. The Henon map is \"multistable\" in the sense that some initial conditions diverge to infinity, and some others converge to a chaotic attractor. Computing these basins of attraction is simple with  Attractors , and would work as follows: # define a state space grid to compute the basins on:\nxg = yg = range(-2, 2; length = 201)\n# find attractors using recurrences in state space:\nmapper = AttractorsViaRecurrences(henon, (xg, yg); sparse = false)\n# compute the full basins of attraction:\nbasins, attractors = basins_of_attraction(mapper; show_progress = false) (Int32[-1 -1 … -1 -1; -1 -1 … -1 -1; … ; -1 -1 … -1 -1; -1 -1 … -1 -1], Dict{Int32, StateSpaceSet{2, Float64}}(1 => 2-dimensional StateSpaceSet{Float64} with 511 points)) fig, ax = heatmap(xg, yg, basins)\nx, y = columns(X) # attractor of Henon map\nscatter!(ax, x, y; color = \"black\")\nfig You could also be using a  DynamicalSystem  instance directly to build your own algorithm if it isn't already implemented (and then later contribute it so it  is  implemented ;) ). A dynamical system can be evolved forwards in time using  step! : henon 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          5\n state:         [-1.5266434026801804e8, -3132.7519146699206]\n Notice how the time is not 0, because  henon  has already been stepped when we called the function  basins_of_attraction  with it. We can step it more: step!(henon) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          6\n state:         [-3.262896110526e16, -4.579930208040541e7]\n step!(henon, 2) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          8\n state:         [-3.110262842032839e66, -4.4715262317959936e32]\n For more information on how to directly use  DynamicalSystem  instances, see the documentation of  DynamicalSystemsBase ."},{"id":52,"pagetitle":"Overarching tutorial","title":"State space sets","ref":"/dynamicalsystems/stable/tutorial/#State-space-sets","content":" State space sets Let's recall that the output of the  trajectory  function is a  StateSpaceSet : X 2-dimensional StateSpaceSet{Float64} with 10001 points\n  0.2        0.3\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311\n  0.540361   0.151562\n  0.742777   0.162108\n  0.389703   0.222833\n  1.01022    0.116911\n -0.311842   0.303065\n  ⋮         \n -0.582534   0.328346\n  0.853262  -0.17476\n -0.194038   0.255978\n  1.20327   -0.0582113\n -1.08521    0.36098\n -0.287758  -0.325562\n  0.558512  -0.0863275\n  0.476963   0.167554\n  0.849062   0.143089 It is printed like a matrix where each column is the timeseries of each dynamic variable. In reality, it is a vector of statically sized vectors (for performance reasons). When indexed with 1 index, it behaves like a vector of vectors X[1] 2-element SVector{2, Float64} with indices SOneTo(2):\n 0.2\n 0.3 X[2:5] 2-dimensional StateSpaceSet{Float64} with 4 points\n  1.244      0.06\n -1.10655    0.3732\n -0.341035  -0.331965\n  0.505208  -0.102311 When indexed with two indices, it behaves like a matrix X[2:5, 2] 4-element Vector{Float64}:\n  0.06\n  0.3732\n -0.3319651199999999\n -0.10231059085086688 When iterated, it iterates over the contained points for (i, point) in enumerate(X)\n    @show point\n    i > 5 && break\nend point = [0.2, 0.3]\npoint = [1.244, 0.06]\npoint = [-1.1065503999999997, 0.3732]\npoint = [-0.34103530283622296, -0.3319651199999999]\npoint = [0.5052077711071681, -0.10231059085086688]\npoint = [0.5403605603672313, 0.1515623313321504] map(point -> point[1] + point[2], X) 10001-element Vector{Float64}:\n  0.5\n  1.304\n -0.7333503999999997\n -0.6730004228362229\n  0.40289718025630117\n  0.6919228916993818\n  0.9048851501617762\n  0.6125365596336813\n  1.1271278272148746\n -0.008777065619615998\n  ⋮\n -0.2541879392427324\n  0.678501271515278\n  0.061940665344374896\n  1.145056192451011\n -0.7242249528790483\n -0.6133198017049188\n  0.47218423998951875\n  0.6445165778497133\n  0.9921511619004666 The columns of the set are obtained with the convenience  columns  function x, y = columns(X)\nsummary.((x, y)) (\"10001-element Vector{Float64}\", \"10001-element Vector{Float64}\")"},{"id":53,"pagetitle":"Overarching tutorial","title":"Using state space sets","ref":"/dynamicalsystems/stable/tutorial/#Using-state-space-sets","content":" Using state space sets Several packages of the library deal with  StateSpaceSets . You could use  ComplexityMeasures  to obtain the entropy, or other complexity measures, of a given set. Below, we obtain the entropy of the natural density of the chaotic attractor by partitioning into a histogram of approximately  50  bins per dimension: prob_est = ValueHistogram(50)\nentropy(prob_est, X) 7.825799208736613 Alternatively, you could use  FractalDimensions  to get the fractal dimensions of the chaotic attractor of the henon map using the Grassberger-Procaccia algorithm: grassberger_proccacia_dim(X) 1.2232922815092426 Or, you could obtain a recurrence matrix of a state space set with  RecurrenceAnalysis R = RecurrenceMatrix(Y, 8.0)\nRg = grayscale(R)\nrr = recurrencerate(R)\nheatmap(Rg; colormap = :grays,\n    axis = (title = \"recurrence rate = $(rr)\", aspect = 1,)\n)"},{"id":54,"pagetitle":"Overarching tutorial","title":"More nonlinear timeseries analysis","ref":"/dynamicalsystems/stable/tutorial/#More-nonlinear-timeseries-analysis","content":" More nonlinear timeseries analysis A  trajectory  of a known dynamical system is one way to obtain a  StateSpaceSet . However, another common way is via a delay coordinates embedding of a measured/observed timeseries. For example, we could use  optimal_traditional_de  from  DelayEmbeddings  to create an optimized delay coordinates embedding of a timeseries w = Y[:, 1] # first variable of Lorenz96\n𝒟, τ, e = optimal_traditional_de(w)\n𝒟 5-dimensional StateSpaceSet{Float64} with 558 points\n  3.15369   -2.40036    1.60497   2.90499  5.72572\n  2.71384   -2.24811    1.55832   3.04987  5.6022\n  2.2509    -2.02902    1.50499   3.20633  5.38629\n  1.77073   -1.75077    1.45921   3.37699  5.07029\n  1.27986   -1.42354    1.43338   3.56316  4.65003\n  0.785468  -1.05974    1.43672   3.76473  4.12617\n  0.295399  -0.673567   1.47423   3.98019  3.50532\n -0.181891  -0.280351   1.54635   4.20677  2.80048\n -0.637447   0.104361   1.64932   4.44054  2.03084\n -1.06201    0.465767   1.77622   4.67654  1.22067\n  ⋮                                        \n  7.42111    9.27879   -1.23936   5.15945  3.25618\n  7.94615    9.22663   -1.64222   5.24344  3.34749\n  8.40503    9.13776   -1.81947   5.26339  3.46932\n  8.78703    8.99491   -1.77254   5.22631  3.60343\n  9.08701    8.77963   -1.51823   5.13887  3.72926\n  9.30562    8.47357   -1.08603   5.00759  3.82705\n  9.4488     8.06029   -0.514333  4.83928  3.88137\n  9.52679    7.52731    0.153637  4.6414   3.88458\n  9.55278    6.86845    0.873855  4.42248  3.83902 and compare fig = Figure()\naxs = [Axis3(fig[1, i]) for i in 1:2]\nfor (S, ax) in zip((Y, 𝒟), axs)\n    lines!(ax, S[:, 1], S[:, 2], S[:, 3])\nend\nfig Since  𝒟  is just another state space set, we could be using any of the above analysis pipelines on it just as easily. The last package to mention here is  TimeseriesSurrogates , which ties with all other observed/measured data analysis by providing a framework for confidence/hypothesis testing. For example, if we had a measured timeseries but we were not sure whether it represents a deterministic system with structure in the state space, or mostly noise, we could do a surrogate test. For this, we use  surrogenerator  and  RandomFourier  from  TimeseriesSurrogates , and the  generalized_dim  from  FractalDimensions  (because it performs better in noisy sets) x = X[:, 1] # Henon map timeseries\n# contaminate with noise\nusing Random: Xoshiro\nrng = Xoshiro(1234)\nx .+= randn(rng, length(x))/100\n# compute noise-contaminated fractal dim.\nΔ_orig = generalized_dim(embed(x, 2, 1)) 1.3801073957979793 And we do the surrogate test surrogate_method = RandomFourier()\nsgen = surrogenerator(x, surrogate_method, rng)\n\nΔ_surr = map(1:1000) do i\n    s = sgen()\n    generalized_dim(embed(s, 2, 1))\nend 1000-element Vector{Float64}:\n 1.8297218640747606\n 1.8449246422083758\n 1.827998413768852\n 1.8301078704767055\n 1.810704251592814\n 1.8324978359365702\n 1.8300954188512213\n 1.8416570396197014\n 1.8575804541517287\n 1.821435647618282\n ⋮\n 1.8768262063118628\n 1.8287938131103985\n 1.8435474417451545\n 1.8129026565561648\n 1.8551700924676993\n 1.8283378225272842\n 1.8211346475312316\n 1.8369834750866052\n 1.8368699339310082 and visualize the test result fig, ax = hist(Δ_surr)\nvlines!(ax, Δ_orig)\nfig since the real value is outside the distribution we have confidence the data are not pure noise."},{"id":55,"pagetitle":"Overarching tutorial","title":"Core components reference","ref":"/dynamicalsystems/stable/tutorial/#Core-components-reference","content":" Core components reference"},{"id":56,"pagetitle":"Overarching tutorial","title":"StateSpaceSets.StateSpaceSet","ref":"/dynamicalsystems/stable/tutorial/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple."},{"id":57,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description Note The documentation of  DynamicalSystem  follows chapter 1 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. A  ds::DynamicalSystem representes a flow Φ in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is a standard Julia function, see below. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f  defined as a standard Julia function.  Observed  or  measured  data from a dynamical system are represented using  StateSpaceSet  and are finite. Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. Construction instructions on  f  and  u Most of the concrete implementations of  DynamicalSystem , with the exception of  ArbitrarySteppable , have two ways of implementing the dynamic rule  f , and as a consequence the type of the state  u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its  effective dimensionality  is  dimension(ds)+1 . API The API that the interface of  DynamicalSystem  employs is the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can quieried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state current_parameters initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace succesful_step API - alter status reinit! set_state! set_parameter! set_parameters!"},{"id":58,"pagetitle":"Overarching tutorial","title":"Dynamical system implementations","ref":"/dynamicalsystems/stable/tutorial/#Dynamical-system-implementations","content":" Dynamical system implementations"},{"id":59,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem ."},{"id":60,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.CoupledODEs","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . DifferentialEquations.jl keyword arguments and interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false)), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  , however the majority of downstream functions in DynamicalSystems.jl assume that  f  is differentiable. The convenience constructor  CoupledODEs(prob::ODEProblem, diffeq)  and  CoupledODEs(ds::CoupledODEs, diffeq)  are also available. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available."},{"id":61,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap ."},{"id":62,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.PoincareMap","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap)"},{"id":63,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\npds = # same as in above example..."},{"id":64,"pagetitle":"Overarching tutorial","title":"DynamicalSystemsBase.ArbitrarySteppable","ref":"/dynamicalsystems/stable/tutorial/#DynamicalSystemsBase.ArbitrarySteppable","content":" DynamicalSystemsBase.ArbitrarySteppable  —  Type ArbitrarySteppable <: DiscreteTimeDynamicalSystem\nArbitrarySteppable(\n    model, step!, extract_state, extract_parameters, reset_model!;\n    isdeterministic = true, set_state = reinit!,\n) A dynamical system generated by an arbitrary \"model\" that can be stepped  in-place  with some function  step!(model)  for 1 step. The state of the model is extracted by the  extract_state(model) -> u  function The parameters of the model are extracted by the  extract_parameters(model) -> p  function. The system may be re-initialized, via  reinit! , with the  reset_model!  user-provided function that must have the call signature reset_model!(model, u, p) given a (potentially new) state  u  and parameter container  p , both of which will default to the initial ones in the  reinit!  call. ArbitrarySteppable  exists to provide the DynamicalSystems.jl interface to models from other packages that could be used within the DynamicalSystems.jl library.  ArbitrarySteppable  follows the  DynamicalSystem  interface with the following adjustments: initial_time  is always 0, as time counts the steps the model has taken since creation or last  reinit!  call. set_state!  is the same as  reinit!  by default. If not, the keyword argument  set_state  is a function  set_state(model, u)  that sets the state of the model to  u . The keyword  isdeterministic  should be set properly, as it decides whether downstream algorithms should error or not."},{"id":65,"pagetitle":"Overarching tutorial","title":"Learn more","ref":"/dynamicalsystems/stable/tutorial/#Learn-more","content":" Learn more To learn more, you need to visit the documentation pages of the individual packages. See the  contents  page for more! DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":68,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive GUIs, animations, visualizations","ref":"/dynamicalsystems/stable/visualizations/#visualizations","content":" Interactive GUIs, animations, visualizations Using the functionality of package extensions in Julia v1.9+, DynamicalSystems.jl provides various visualization tools as soon as the  Makie  package comes into scope (i.e., when  using Makie  or any of its backends like  GLMakie ). The main functionality is  interactive_trajectory  that allows building custom GUI apps for visualizing the time evolution of dynamical systems. The remaining GUI applications in this page are dedicated to more specialized scenarios."},{"id":69,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive- or animated trajectory evolution","ref":"/dynamicalsystems/stable/visualizations/#Interactive-or-animated-trajectory-evolution","content":" Interactive- or animated trajectory evolution"},{"id":70,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_trajectory","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_trajectory","content":" DynamicalSystems.interactive_trajectory  —  Function interactive_trajectory(ds::DynamicalSystem [, u0s]; kwargs...) → fig, dsobs Create a panel (a new Makie figure) that contains an axis showing the evolving trajectories of  ds  while optionally allowing for interactively starting/stopping the time evolution and/or changing the system parameters via sliders. Return the newly created figure  fig  that may include interactivity-related buttons, and a  dsobs::DynamicalSystemObservable  that facilities the creation of custom animations and/or interactive applications, see the custom animations section below. The given dynamical system is always cast into a  ParallelDynamicalSystem . The trajectories from the initial conditions in  u0s  (a vector of vectors) are all evolved and visualized in parallel. By default only the current state of the system is used. See also  interactive_trajectory_timeseries . Interactivity and time stepping keywords add_controls = true : If  true , below the main axis containing the trajectories some controls for animating the trajectories live are added: run, pause, and how many steps to evolve for per animation step. The plotted trajectories can always be evolved manually using the custom animations etup that we describe below;  add_controls  only concerns the buttons and interactivity added to the created panel. parameter_sliders = nothing : If given, it must be a dictionary, mapping parameter indices (of the parameter container of  ds ) into ranges. Each combination of index and range becomes a slider that can be interactively controlled to alter a system parameter on the fly during time evolution. Note that all parameter systems can always be altered on the fly using the custom animation setup that we describe below;  parameter_sliders  only conserns the buttons and interactivity added to the created panel. parameter_names = Dict(keys(ps) .=> keys(ps)) : Dictionary mapping parameter keys to labels. Only valid if  parameter_sliders  is given. Δt : Time step of time evolution. Defaults to 1 for discrete time, 0.01 for continuous time systems. pause = nothing : If given, it must be a real number. This number is given to the  sleep  function, which is called between each plot update. Visualization keywords colors : The color for each initial condition (and resulting trajectory). idxs = 1:min(length(transform(u0s[1])), 3) : Which variables to plot (up to three can be chosen). tail = 1000 : Length of plotted trajectory (in units of  Δt ). fade = true : The trajectories in state space are faded to full transparency if  true . markersize = 15 : Size of markers of trajectory endpoints. For discrete systems half of that is used for the trajectory tail. plotkwargs = NamedTuple() : A named tuple of keyword arguments propagated to the state space plot ( lines  for continuous,  scatter  for discrete systems).  plotkwargs  can also be a vector of named tuples, in which case each initial condition gets different arguments. Layouting keywords idxs = 1:min(dimension(ds), 3) : Which variables of  ds  should be plotted. If three indices are given, the trajectory plot is also 3D, otherwise 2D. Note that no transformation of the dynamical system is done, you can use a  ProjectedDynamicalSystem  if you want to visualize an arbitrary transformation of  ds . lims : A tuple of tuples (min, max) for the axis limits. If not given, they are automatically deduced by evolving each of  u0s  100 units and picking most extreme values (limits are  not  adjusted by default during the live animations) figure, axis : both can be named tuples with arbitrary keywords propagated to the generation of the  Figure  and state space  Axis  instances. Custom animations The second return argument  dsobs  is a  DynamicalSystemObservable . The trajectories plotted in the main panel are linked to observables that are fields of the  dsobs . Specifically, the field  dsobs.state_obserable  is an observable containing the final state of each of the trajectories, i.e., a vector of vectors like  u0s .  dsobs.param_observable  is an observable of the system parameters. These observables are triggered by the interactive GUI buttons (the first two when the system is stepped in time, the last one when the parameters are updated). However, these observables, and hence the corresponding plotted trajectories that are  map ed from these observables, can be updated via the formal API of  DynamicalSystem : step!(dsobs, n::Int = 1) will step the system for  n  steps of  Δt  time, and only update the plot on the last step.  set_parameter!(dsobs, index, value)  will update the system parameter and then trigger the parameter observable. Lastly,  set_state!(dsobs, new_u [, i])  will set the  i -th system state and clear the trajectory plot to the new initial condition. This information can be used to create custom animations and/or interactive apps. In principle, the only thing a user has to do is create new observables from the existing ones using e.g. the  on  function and plot these new observables. Various examples are provided in the online documentation. source"},{"id":71,"pagetitle":"Animations, GUIs, Visuals","title":"Example 1: interactive trajectory animation","ref":"/dynamicalsystems/stable/visualizations/#Example-1:-interactive-trajectory-animation","content":" Example 1: interactive trajectory animation using DynamicalSystems, CairoMakie\nF, G, a, b = 6.886, 1.347, 0.255, 4.0\nds = PredefinedDynamicalSystems.lorenz84(; F, G, a, b)\n\nu1 = [0.1, 0.1, 0.1] # periodic\nu2 = u1 .+ 1e-3     # fixed point\nu3 = [-1.5, 1.2, 1.3] .+ 1e-9 # chaotic\nu4 = [-1.5, 1.2, 1.3] .+ 21e-9 # chaotic 2\nu0s = [u1, u2, u3, u4]\n\nfig, dsobs = interactive_trajectory(\n    ds, u0s; tail = 1000, fade = true,\n    idxs = [1,3],\n)\n\nfig We could interact with this plot live, like in the example video above. We can also progress the visuals via code as instructed by  interactive_trajectory  utilizing the second returned argument  dsobs : step!(dsobs, 2000)\nfig (if you progress the visuals via code you probably want to give  add_controls = false  as a keyword to  interactive_trajectory )"},{"id":72,"pagetitle":"Animations, GUIs, Visuals","title":"Example 2: Adding parameter-dependent elements to a plot","ref":"/dynamicalsystems/stable/visualizations/#Example-2:-Adding-parameter-dependent-elements-to-a-plot","content":" Example 2: Adding parameter-dependent elements to a plot In this advanced example we add plot elements to the provided figure, and also utilize the parameter observable in  dsobs  to add animated plot elements that update whenever a parameter updates. The final product of this snippet is in fact the animation at the top of the docstring of  interactive_trajectory_panel . We start with an interactive trajectory panel of the Lorenz63 system, in which we also add sliders for interactively changing parameter values using DynamicalSystems, CairoMakie\n\nps = Dict(\n    1 => 1:0.1:30,\n    2 => 10:0.1:50,\n    3 => 1:0.01:10.0,\n)\npnames = Dict(1 => \"σ\", 2 => \"ρ\", 3 => \"β\")\n\nlims = ((-30, 30), (-30, 30), (0, 100))\n\nds = PredefinedDynamicalSystems.lorenz()\n\nu1 = [10,20,40.0]\nu3 = [20,10,40.0]\nu0s = [u1, u3]\n\nfig, dsobs = interactive_trajectory(\n    ds, u0s; parameter_sliders = ps, pnames, lims\n)\n\nfig If now one interactively clicked (if using GLMakie) the parameter sliders and then update, the system parameters would be updated accordingly. We can also add new plot elements that depend on the parameter values using the  dsobs : # Fixed points of the lorenz system (without the origin)\nlorenz_fixedpoints(ρ,β) = [\n    Point3f(sqrt(β*(ρ-1)), sqrt(β*(ρ-1)), ρ-1),\n    Point3f(-sqrt(β*(ρ-1)), -sqrt(β*(ρ-1)), ρ-1),\n]\n\n# add an observable trigger to the system parameters\nfpobs = map(dsobs.param_observable) do params\n    σ, ρ, β = params\n    return lorenz_fixedpoints(ρ, β)\nend\n\n# If we want to plot directly on the trajectory axis, we need to\n# extract it from the figure. The first entry of the figure is a grid layout\n# containing the axis and the GUI controls. The [1,1] entry of the layout\n# is the axis containing the trajectory plot\n\nax = content(fig[1,1][1,1])\nscatter!(ax, fpobs; markersize = 10, marker = :diamond, color = :red)\n\nfig Now, after the live animation \"run\" button is pressed, we can interactively change the parameter ρ and click update, in which case both the dynamical system's ρ parameter will change, but also the location of the red diamonds. We can also change the parameters non-interactively using  set_parameter! set_parameter!(dsobs, 2, 50.0)\n\nfig set_parameter!(dsobs, 2, 10.0)\n\nfig Note that the sliders themselves did not change, as this functionality is for \"offline\" creation of animations where one doesn't interact with sliders. The keyword  add_controls  should be given as  false  in such scenarios."},{"id":73,"pagetitle":"Animations, GUIs, Visuals","title":"Example 3: Observed timeseries of the system","ref":"/dynamicalsystems/stable/visualizations/#Example-3:-Observed-timeseries-of-the-system","content":" Example 3: Observed timeseries of the system using DynamicalSystems, CairoMakie\nusing LinearAlgebra: norm, dot\n\n# Dynamical system and initial conditions\nds = Systems.thomas_cyclical(b = 0.2)\nu0s = ([3, 1, 1.], [1, 3, 1.], [1, 1, 3.])\n\n# Observables we get timeseries of:\nf1 = 3\nf2 = function distance_from_symmetry(u)\n    v = SVector{3}(1/√3, 1/√3, 1/√3)\n    t = dot(v, u)\n    return norm(u - t*v)\nend\nfs = [f1, f2]\n\nfig, dsobs = interactive_trajectory_timeseries(ds, fs, u0s;\n    idxs = [1, 2], timeseries_names = [\"x3\", \"dist. symm.\"],\n    timeseries_ylims = [(-2, 4), (0, 5)],\n    lims = ((-2, 4), (-2, 4)),\n    Δt = 0.05, tail = 500,\n    add_controls = false\n)\n\nfig we can progress the simulation: step!(dsobs, 200)\nfig or we can even make a nice video out of it: record(fig, \"thomas_cycl.mp4\", 1:100) do i\n    step!(dsobs, 10)\nend \"thomas_cycl.mp4\""},{"id":74,"pagetitle":"Animations, GUIs, Visuals","title":"Cobweb Diagrams","ref":"/dynamicalsystems/stable/visualizations/#Cobweb-Diagrams","content":" Cobweb Diagrams"},{"id":75,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_cobweb","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_cobweb","content":" DynamicalSystems.interactive_cobweb  —  Function interactive_cobweb(ds::DiscreteDynamicalSystem, prange, O::Int = 3; kwargs...) Launch an interactive application for exploring cobweb diagrams of 1D discrete dynamical systems. Two slides control the length of the plotted trajectory and the current parameter value. The parameter values are obtained from the given  prange . In the cobweb plot, higher order iterates of the dynamic rule  f  are plotted as well, starting from order 1 all the way to the given order  O . Both the trajectory in the cobweb, as well as any iterate  f  can be turned off by using some of the buttons. Keywords fkwargs = [(linewidth = 4.0, color = randomcolor()) for i in 1:O] : plotting keywords for each of the plotted iterates of  f trajcolor = :black : color of the trajectory pname = \"p\" : name of the parameter slider pindex = 1 : parameter index xmin = 0, xmax = 1 : limits the state of the dynamical system can take Tmax = 1000 : maximum trajectory length x0s = range(xmin, xmax; length = 101) : Possible values for the x0 slider. source The animation at the top of this section was done with using DynamicalSystems, GLMakie\n\n# the second range is a convenience for intermittency example of logistic\nrrange = 1:0.001:4.0\n# rrange = (rc = 1 + sqrt(8); [rc, rc - 1e-5, rc - 1e-3])\n\nlo = Systems.logistic(0.4; r = rrange[1])\ninteractive_cobweb(lo, rrange, 5)"},{"id":76,"pagetitle":"Animations, GUIs, Visuals","title":"Orbit Diagrams","ref":"/dynamicalsystems/stable/visualizations/#Orbit-Diagrams","content":" Orbit Diagrams Notice that orbit diagrams and bifurcation diagrams are different things in DynamicalSystems.jl"},{"id":77,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_orbitdiagram","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_orbitdiagram","content":" DynamicalSystems.interactive_orbitdiagram  —  Function interactive_orbitdiagram(\n    ds::DynamicalSystem, p_index, pmin, pmax, i::Int = 1;\n    u0 = nothing, parname = \"p\", title = \"\"\n) Open an interactive application for exploring orbit diagrams (ODs) of discrete time dynamical systems. Requires  DynamicalSystems . In essense, the function presents the output of  orbitdiagram  of the  i th variable of the  ds , and allows interactively zooming into it. Keywords control the name of the parameter, the initial state (used for  any  parameter) or whether to add a title above the orbit diagram. Interaction The application is separated in the \"OD plot\" (left) and the \"control panel\" (right). On the OD plot you can interactively click and drag with the left mouse button to select a region in the OD. This region is then  re-computed  at a higher resolution. The options at the control panel are straight-forward, with n  amount of steps recorded for the orbit diagram (not all are in the zoomed region!) t  transient steps before starting to record steps d  density of x-axis (the parameter axis) α  alpha value for the plotted points. Notice that at each update  n*t*d  steps are taken. You have to press  update  after changing these parameters. Press  reset  to bring the OD in the original state (and variable). Pressing  back  will go back through the history of your exploration History is stored when the \"update\" button is pressed or a region is zoomed in. You can even decide which variable to get the OD for by choosing one of the variables from the wheel! Because the y-axis limits can't be known when changing variable, they reset to the size of the selected variable. Accessing the data What is plotted on the application window is a  true  orbit diagram, not a plotting shorthand. This means that all data are obtainable and usable directly. Internally we always scale the orbit diagram to [0,1]² (to allow  Float64  precision even though plotting is  Float32 -based). This however means that it is necessary to transform the data in real scale. This is done through the function  scaleod  which accepts the 5 arguments returned from the current function: figure, oddata = interactive_orbitdiagram(...)\nps, us = scaleod(oddata) source"},{"id":78,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.scaleod","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.scaleod","content":" DynamicalSystems.scaleod  —  Function scaleod(oddata) -> ps, us Given the return values of  interactive_orbitdiagram , produce orbit diagram data scaled correctly in data units. Return the data as a vector of parameter values and a vector of corresponding variable values. source The animation at the top of this section was done with i = p_index = 1\nds, p_min, p_max, parname = Systems.henon(), 0.8, 1.4, \"a\"\nt = \"orbit diagram for the Hénon map\"\n\noddata = interactive_orbitdiagram(ds, p_index, p_min, p_max, i;\n                                  parname = parname, title = t)\n\nps, us = scaleod(oddata)"},{"id":79,"pagetitle":"Animations, GUIs, Visuals","title":"Interactive Poincaré Surface of Section","ref":"/dynamicalsystems/stable/visualizations/#Interactive-Poincaré-Surface-of-Section","content":" Interactive Poincaré Surface of Section"},{"id":80,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_poincaresos","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_poincaresos","content":" DynamicalSystems.interactive_poincaresos  —  Function interactive_poincaresos(cds, plane, idxs, complete; kwargs...) Launch an interactive application for exploring a Poincaré surface of section (PSOS) of the continuous dynamical system  cds . Requires  DynamicalSystems . The  plane  can only be the  Tuple  type accepted by  DynamicalSystems.poincaresos , i.e.  (i, r)  for the  i th variable crossing the value  r .  idxs  gives the two indices of the variables to be displayed, since the PSOS plot is always a 2D scatterplot. I.e.  idxs = (1, 2)  will plot the 1st versus 2nd variable of the PSOS. It follows that  plane[1] ∉ idxs  must be true. complete  is a three-argument  function  that completes the new initial state during interactive use, see below. The function returns:  figure, laststate  with the latter being an observable containing the latest initial  state . Keyword Arguments direction, rootkw  : Same use as in  DynamicalSystems.poincaresos . tfinal = (1000.0, 10.0^4)  : A 2-element tuple for the range of values for the total integration time (chosen interactively). color  : A  function  of the system's initial condition, that returns a color to plot the new points with. The color must be  RGBf/RGBAf .  A random color is chosen by default. labels = (\"u₁\" , \"u₂\")  : Scatter plot labels. scatterkwargs = () : Named tuple of keywords passed to  scatter . diffeq = NamedTuple()  : Any extra keyword arguments are passed into  init  of DiffEq. Interaction The application is a standard scatterplot, which shows the PSOS of the system, initially using the system's  u0 . Two sliders control the total evolution time and the size of the marker points (which is always in pixels). Upon clicking within the bounds of the scatter plot your click is transformed into a new initial condition, which is further evolved and its PSOS is computed and then plotted into the scatter plot. Your click is transformed into a full  D -dimensional initial condition through the function  complete . The first two arguments of the function are the positions of the click on the PSOS. The third argument is the value of the variable the PSOS is defined on. To be more exact, this is how the function is called: x, y = mouseclick; z = plane[2]\nnewstate = complete(x, y, z) The  complete  function can throw an error for ill-conditioned  x, y, z . This will be properly handled instead of breaking the application. This  newstate  is also given to the function  color  that gets a new color for the new points. source To generate the animation at the start of this section you can run using InteractiveDynamics, GLMakie, OrdinaryDiffEq, DynamicalSystems\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\n\nhh = Systems.henonheiles()\n\npotential(x, y) = 0.5(x^2 + y^2) + (x^2*y - (y^3)/3)\nenergy(x,y,px,py) = 0.5(px^2 + py^2) + potential(x,y)\nconst E = energy(get_state(hh)...)\n\nfunction complete(y, py, x)\n    V = potential(x, y)\n    Ky = 0.5*(py^2)\n    Ky + V ≥ E && error(\"Point has more energy!\")\n    px = sqrt(2(E - V - Ky))\n    ic = [x, y, px, py]\n    return ic\nend\n\nplane = (1, 0.0) # first variable crossing 0\n\n# Coloring points using the Lyapunov exponent\nfunction λcolor(u)\n    λ = lyapunovs(hh, 4000; u0 = u)[1]\n    λmax = 0.1\n    return RGBf(0, 0, clamp(λ/λmax, 0, 1))\nend\n\nstate, scene = interactive_poincaresos(hh, plane, (2, 4), complete;\nlabels = (\"q₂\" , \"p₂\"),  color = λcolor, diffeq...)"},{"id":81,"pagetitle":"Animations, GUIs, Visuals","title":"Scanning a Poincaré Surface of Section","ref":"/dynamicalsystems/stable/visualizations/#Scanning-a-Poincaré-Surface-of-Section","content":" Scanning a Poincaré Surface of Section"},{"id":82,"pagetitle":"Animations, GUIs, Visuals","title":"DynamicalSystems.interactive_poincaresos_scan","ref":"/dynamicalsystems/stable/visualizations/#DynamicalSystems.interactive_poincaresos_scan","content":" DynamicalSystems.interactive_poincaresos_scan  —  Function interactive_poincaresos_scan(A::StateSpaceSet, j::Int; kwargs...)\ninteractive_poincaresos_scan(As::Vector{StateSpaceSet}, j::Int; kwargs...) Launch an interactive application for scanning a Poincare surface of section of  A  like a \"brain scan\", where the plane that defines the section can be arbitrarily moved around via a slider. Return  figure, ax3D, ax2D . The input dataset must be 3 dimensional, and here the crossing plane is always chosen to be when the  j -th variable of the dataset crosses a predefined value. The slider automatically gets all possible values the  j -th variable can obtain. If given multiple datasets, the keyword  colors  attributes a color to each one, e.g.  colors = [JULIADYNAMICS_COLORS[mod1(i, 6)] for i in 1:length(As)] . The keywords  linekw, scatterkw  are named tuples that are propagated as keyword arguments to the line and scatter plot respectively, while the keyword  direction = -1  is propagated to the function  DyamicalSystems.poincaresos . source The animation at the top of this page was done with using GLMakie, DynamicalSystems\nusing OrdinaryDiffEq: Vern9\n\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\nds = PredefinedDynamicalSystems.henonheiles()\nds = CoupledODEs(ds, diffeq)\n\nu0s = [\n    [0.0, -0.25, 0.42081, 0.0],\n    [0.0, 0.1, 0.5, 0.0],\n    [0.0, -0.31596, 0.354461, 0.0591255]\n]\n# inputs\ntrs = [trajectory(ds, 10000, u0)[1][:, SVector(1,2,3)] for u0 ∈ u0s]\nj = 2 # the dimension of the plane\n\ninteractive_poincaresos_scan(trs, j; linekw = (transparency = true,))"},{"id":85,"pagetitle":"Numerical Data","title":"Numerical Data","ref":"/statespacesets/stable/#Numerical-Data","content":" Numerical Data"},{"id":86,"pagetitle":"Numerical Data","title":"StateSpaceSets","ref":"/statespacesets/stable/#StateSpaceSets","content":" StateSpaceSets  —  Module StateSpaceSets.jl A Julia package that provides functionality for state space sets. These are collections of points of fixed, and known by type, size (called dimension). It is used in several projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . The main export of  StateSpaceSets  is the concrete type  StateSpaceSet . The package also provides functionality for distances, neighbor searches, sampling, and normalization. To install it you may run  import Pkg; Pkg.add(\"StateSpaceSets\") , however, there is no real reason to install this package directly as it is re-exported by all downstream packages that use it. previously StateSpaceSets.jl was part of DelayEmbeddings.jl source Timeseries and datasets The word \"timeseries\" can be confusing, because it can mean a univariate (also called scalar or one-dimensional) timeseries or a multivariate (also called multi-dimensional) timeseries. To resolve this confusion, in  DynamicalSystems.jl  we have the following convention:  \"timeseries\"  is always univariate! it refers to a one-dimensional vector of numbers, which exists with respect to some other one-dimensional vector of numbers that corresponds to a time vector. On the other hand, we use the word  \"dataset\"  is used to refer to a  multi-dimensional  timeseries, which is of course simply a group/set of one-dimensional timeseries represented as a  StateSpaceSet . In some documentation strings we use the word \"trajectory\" instead of \"dataset\", which means an ordered multivariate timeseries. This is typically the output of the function  trajectory , or the delay embedding of a timeseries via  embed , both of which are also represented as a  StateSpaceSet ."},{"id":87,"pagetitle":"Numerical Data","title":"StateSpaceSet","ref":"/statespacesets/stable/#StateSpaceSet","content":" StateSpaceSet Trajectories, and in general sets in state space, are represented by a structure called  StateSpaceSet  in  DynamicalSystems.jl  (while timeseries are always standard Julia  Vector s). It is recommended to always  standardize  datasets."},{"id":88,"pagetitle":"Numerical Data","title":"StateSpaceSets.StateSpaceSet","ref":"/statespacesets/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. source"},{"id":89,"pagetitle":"Numerical Data","title":"StateSpaceSets.standardize","ref":"/statespacesets/stable/#StateSpaceSets.standardize","content":" StateSpaceSets.standardize  —  Function standardize(d::StateSpaceSet) → r Create a standardized version of the input set where each column is transformed to have mean 0 and standard deviation 1. source standardize(x::AbstractVector{<:Real}) = (x - mean(x))/std(x) source In essence a  StateSpaceSet  is simply a wrapper for a  Vector  of  SVector s. However, it is visually represented as a matrix, similarly to how numerical data would be printed on a spreadsheet (with time being the  column  direction). It also offers a lot more functionality than just pretty-printing. Besides the examples in the documentation string, you can e.g. iterate over data points using DynamicalSystems\nhen = Systems.henon()\ndata = trajectory(hen, 10000) # this returns a dataset\nfor point in data\n    # stuff\nend Most functions from  DynamicalSystems.jl  that manipulate and use multidimensional data are expecting a  StateSpaceSet . This allows us to define efficient methods that coordinate well with each other, like e.g.  embed ."},{"id":90,"pagetitle":"Numerical Data","title":"StateSpaceSet Functions","ref":"/statespacesets/stable/#StateSpaceSet-Functions","content":" StateSpaceSet Functions"},{"id":91,"pagetitle":"Numerical Data","title":"StateSpaceSets.minima","ref":"/statespacesets/stable/#StateSpaceSets.minima","content":" StateSpaceSets.minima  —  Function minima(dataset) Return an  SVector  that contains the minimum elements of each timeseries of the dataset. source"},{"id":92,"pagetitle":"Numerical Data","title":"StateSpaceSets.maxima","ref":"/statespacesets/stable/#StateSpaceSets.maxima","content":" StateSpaceSets.maxima  —  Function maxima(dataset) Return an  SVector  that contains the maximum elements of each timeseries of the dataset. source"},{"id":93,"pagetitle":"Numerical Data","title":"StateSpaceSets.minmaxima","ref":"/statespacesets/stable/#StateSpaceSets.minmaxima","content":" StateSpaceSets.minmaxima  —  Function minmaxima(dataset) Return  minima(dataset), maxima(dataset)  without doing the computation twice. source"},{"id":94,"pagetitle":"Numerical Data","title":"StateSpaceSets.columns","ref":"/statespacesets/stable/#StateSpaceSets.columns","content":" StateSpaceSets.columns  —  Function columns(ssset) -> x, y, z, ... Return the individual columns of the state space set allocated as  Vector s. Equivalent with  collect(eachcol(ssset)) . source"},{"id":95,"pagetitle":"Numerical Data","title":"StateSpaceSet distances","ref":"/statespacesets/stable/#StateSpaceSet-distances","content":" StateSpaceSet distances"},{"id":96,"pagetitle":"Numerical Data","title":"Two datasets","ref":"/statespacesets/stable/#Two-datasets","content":" Two datasets"},{"id":97,"pagetitle":"Numerical Data","title":"StateSpaceSets.set_distance","ref":"/statespacesets/stable/#StateSpaceSets.set_distance","content":" StateSpaceSets.set_distance  —  Function set_distance(ssset1, ssset2 [, distance]) Calculate a distance between two  StateSpaceSet s, i.e., a distance defined between sets of points, as dictated by  distance . Possible  distance  types are: Centroid , which is the default, and 100s of times faster than the rest Hausdorff StrictlyMinimumDistance source"},{"id":98,"pagetitle":"Numerical Data","title":"StateSpaceSets.Hausdorff","ref":"/statespacesets/stable/#StateSpaceSets.Hausdorff","content":" StateSpaceSets.Hausdorff  —  Type Hausdorff(metric = Euclidean()) A distance that can be used in  set_distance . The  Hausdorff distance  is the greatest of all the distances from a point in one set to the closest point in the other set. The distance is calculated with the metric given to  Hausdorff  which defaults to Euclidean. Hausdorff  is 2x slower than  StrictlyMinimumDistance , however it is a proper metric in the space of sets of state space sets. source"},{"id":99,"pagetitle":"Numerical Data","title":"StateSpaceSets.Centroid","ref":"/statespacesets/stable/#StateSpaceSets.Centroid","content":" StateSpaceSets.Centroid  —  Type Centroid(metric = Euclidean()) A distance that can be used in  set_distance . The  Centroid  method returns the distance (according to  metric ) between the  centroids  (a.k.a. centers of mass) of the sets. metric  can be any function that takes in two static vectors are returns a positive definite number to use as a distance (and typically is a  Metric  from Distances.jl). source"},{"id":100,"pagetitle":"Numerical Data","title":"Sets of datasets","ref":"/statespacesets/stable/#Sets-of-datasets","content":" Sets of datasets"},{"id":101,"pagetitle":"Numerical Data","title":"StateSpaceSets.setsofsets_distances","ref":"/statespacesets/stable/#StateSpaceSets.setsofsets_distances","content":" StateSpaceSets.setsofsets_distances  —  Function setsofsets_distances(a₊, a₋ [, distance]) → distances Calculate distances between sets of  StateSpaceSet s. Here   a₊, a₋  are containers of  StateSpaceSet s, and the returned distances are dictionaries of distances. Specifically,  distances[i][j]  is the distance of the set in the  i  key of  a₊  to the  j  key of  a₋ . Notice that distances from  a₋  to  a₊  are not computed at all (assumming symmetry in the distance function). The  distance  can be as in  set_distance , or it can be an arbitrary function that takes as input two state space sets and returns any positive-definite number as their \"distance\". source"},{"id":102,"pagetitle":"Numerical Data","title":"StateSpaceSet I/O","ref":"/statespacesets/stable/#StateSpaceSet-I/O","content":" StateSpaceSet I/O Input/output functionality for an  AbstractStateSpaceSet  is already achieved using base Julia, specifically  writedlm  and  readdlm . To write and read a dataset, simply do: using DelimitedFiles\n\ndata = StateSpaceSet(rand(1000, 2))\n\n# I will write and read using delimiter ','\nwritedlm(\"data.txt\", data, ',')\n\n# Don't forget to convert the matrix to a StateSpaceSet when reading\ndata = StateSpaceSet(readdlm(\"data.txt\", ',', Float64))"},{"id":103,"pagetitle":"Numerical Data","title":"Neighborhoods","ref":"/statespacesets/stable/#Neighborhoods","content":" Neighborhoods Neighborhoods refer to the common act of finding points in a dataset that are nearby a given point (which typically belongs in the dataset).  DynamicalSystems.jl  bases this interface on  Neighborhood.jl . You can go to its documentation if you are interested in finding neighbors in a dataset for e.g. a custom algorithm implementation. For  DynamicalSystems.jl , what is relevant are the two types of neighborhoods that exist:"},{"id":104,"pagetitle":"Numerical Data","title":"Neighborhood.NeighborNumber","ref":"/statespacesets/stable/#Neighborhood.NeighborNumber","content":" Neighborhood.NeighborNumber  —  Type NeighborNumber(k::Int) <: SearchType Search type representing the  k  nearest neighbors of the query (or approximate neighbors, depending on the search structure)."},{"id":105,"pagetitle":"Numerical Data","title":"Neighborhood.WithinRange","ref":"/statespacesets/stable/#Neighborhood.WithinRange","content":" Neighborhood.WithinRange  —  Type WithinRange(r::Real) <: SearchType Search type representing all neighbors with distance  ≤ r  from the query (according to the search structure's metric)."},{"id":106,"pagetitle":"Numerical Data","title":"Theiler window","ref":"/statespacesets/stable/#Theiler-window","content":" Theiler window The Theiler window is a concept that is useful when finding neighbors in a dataset that is coming from the sampling of a continuous dynamical system. As demonstrated in the figure below, it tries to eliminate spurious \"correlations\" (wrongly counted neighbors) due to a potentially dense sampling of the trajectory (e.g. by giving small sampling time in  trajectory ). The figure below demonstrates a typical  WithinRange  search around the black point with index  i . Black, red and green points are found neighbors, but points within indices  j  that satisfy  |i-j| ≤ w  should  not  be counted as \"true\" neighbors. These neighbors are typically the same around  any  state space point, and thus wrongly bias calculations by providing a non-zero baseline of neighbors. For the sketch below,  w=3  would have been used. Typically a good choice for  w  coincides with the choice an optimal delay time, see  estimate_delay , for any of the timeseries of the dataset."},{"id":107,"pagetitle":"Numerical Data","title":"Samplers","ref":"/statespacesets/stable/#Samplers","content":" Samplers"},{"id":108,"pagetitle":"Numerical Data","title":"StateSpaceSets.statespace_sampler","ref":"/statespacesets/stable/#StateSpaceSets.statespace_sampler","content":" StateSpaceSets.statespace_sampler  —  Function statespace_sampler(region [, seed = 42]) → sampler, isinside A function that facilitates sampling points randomly and uniformly in a state space  region . It generates two functions: sampler  is a 0-argument function that when called generates a random point inside a state space  region . The point is always a  Vector  for type stability irrespectively of dimension. Generally, the generated point should be  copied  if it needs to be stored. (i.e., calling  sampler()  utilizes a shared vector)  sampler  is a thread-safe function. isinside  is a 1-argument function that returns  true  if the given state space point is inside the  region . The  region  can be an instance of any of the following types (input arguments if not specified are vectors of length  D , with  D  the state space dimension): HSphere(radius::Real, center) : points  inside  the hypersphere (boundary excluded). Convenience method  HSphere(radius::Real, D::Int)  makes the center a  D -long vector of zeros. HSphereSurface(radius, center) : points on the hypersphere surface. Same convenience method as above is possible. HRectangle(mins, maxs) : points in [min, max) for the bounds along each dimension. The random number generator is always  Xoshiro  with the given  seed . source statespace_sampler(grid::NTuple{N, AbstractRange} [, seed]) If given a  grid  that is a tuple of  AbstractVector s, the minimum and maximum of the vectors are used to make an  HRectangle  region. source"},{"id":109,"pagetitle":"Numerical Data","title":"StateSpaceSets.HSphere","ref":"/statespacesets/stable/#StateSpaceSets.HSphere","content":" StateSpaceSets.HSphere  —  Type HSphere(r::Real, center::Vector)\nHSphere(r::Real, D::Int) A state space region denoting all points  within  a hypersphere. source"},{"id":110,"pagetitle":"Numerical Data","title":"StateSpaceSets.HSphereSurface","ref":"/statespacesets/stable/#StateSpaceSets.HSphereSurface","content":" StateSpaceSets.HSphereSurface  —  Type HSphereSurface(r::Real, center::Vector)\nHSphereSurface(r::Real, D::Int) A state space region denoting all points  on the surface  (boundary) of a hypersphere. source"},{"id":111,"pagetitle":"Numerical Data","title":"StateSpaceSets.HRectangle","ref":"/statespacesets/stable/#StateSpaceSets.HRectangle","content":" StateSpaceSets.HRectangle  —  Type HRectangle(mins::Vector, maxs::Vector) A state space region denoting all points  within  the hyperrectangle. source"},{"id":116,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.jl","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.jl","content":" DynamicalSystemsBase.jl"},{"id":117,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase","content":" DynamicalSystemsBase  —  Module DynamicalSystemsBase.jl A Julia package that defines the  DynamicalSystem  interface and many concrete implementations used in the  DynamicalSystems.jl  ecosystem. To install it, run  import Pkg; Pkg.add(\"DynamicalSystemsBase\") . Typically, you do not want to use  DynamicalSystemsBase  directly, as downstream analysis packages re-export it. All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source"},{"id":118,"pagetitle":"DynamicalSystemsBase.jl","title":"The  DynamicalSystem  API","ref":"/dynamicalsystemsbase/stable/#The-DynamicalSystem-API","content":" The  DynamicalSystem  API"},{"id":119,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description Note The documentation of  DynamicalSystem  follows chapter 1 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. A  ds::DynamicalSystem representes a flow Φ in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is a standard Julia function, see below. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f  defined as a standard Julia function.  Observed  or  measured  data from a dynamical system are represented using  StateSpaceSet  and are finite. Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. Construction instructions on  f  and  u Most of the concrete implementations of  DynamicalSystem , with the exception of  ArbitrarySteppable , have two ways of implementing the dynamic rule  f , and as a consequence the type of the state  u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its  effective dimensionality  is  dimension(ds)+1 . API The API that the interface of  DynamicalSystem  employs is the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can quieried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state current_parameters initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace succesful_step API - alter status reinit! set_state! set_parameter! set_parameters! source"},{"id":120,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_state","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_state","content":" DynamicalSystemsBase.current_state  —  Function current_state(ds::DynamicalSystem) → u Return the current state of  ds . This state is mutated when  ds  is mutated. source"},{"id":121,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_state","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_state","content":" DynamicalSystemsBase.initial_state  —  Function initial_state(ds::DynamicalSystem) → u0 Return the initial state of  ds . This state is never mutated and is set when initializing  ds . source"},{"id":122,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_parameters","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_parameters","content":" DynamicalSystemsBase.current_parameters  —  Function current_parameters(ds::DynamicalSystem) → p Return the current parameter container of  ds . This is mutated in functions that need to evolve  ds  across a parameter range. The following convenience syntax is also possible: current_parameters(ds::DynamicalSystem, index) which will give the specific parameter from the container at the given  index  (which works for arrays, dictionaries, or composite types if  index  is  Symbol ). source"},{"id":123,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_parameters","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_parameters","content":" DynamicalSystemsBase.initial_parameters  —  Function initial_parameters(ds::DynamicalSystem) → p0 Return the initial parameter container of  ds . This is never mutated and is set when initializing  ds . source"},{"id":124,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.isdeterministic","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.isdeterministic","content":" DynamicalSystemsBase.isdeterministic  —  Function isdeterministic(ds::DynamicalSystem) → true/false Return  true  if  ds  is deterministic, i.e., the dynamic rule contains no randomness. This is information deduced from the type of  ds . source"},{"id":125,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.isdiscretetime","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.isdiscretetime","content":" DynamicalSystemsBase.isdiscretetime  —  Function isdiscretetime(ds::DynamicalSystem) → true/false Return  true  if  ds  operates in discrete time, or  false  if it is in continuous time. This is information deduced from the type of  ds . source"},{"id":126,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.dynamic_rule","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.dynamic_rule","content":" DynamicalSystemsBase.dynamic_rule  —  Function dynamic_rule(ds::DynamicalSystem) → f Return the dynamic rule of  ds . This is never mutated and is set when initializing  ds . source"},{"id":127,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_time","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_time","content":" DynamicalSystemsBase.current_time  —  Function current_time(ds::DynamicalSystem) → t Return the current time that  ds  is at. This is mutated when  ds  is evolved. source"},{"id":128,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_time","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_time","content":" DynamicalSystemsBase.initial_time  —  Function initial_time(ds::DynamicalSystem) → t0 Return the initial time defined for  ds . This is never mutated and is set when initializing  ds . source"},{"id":129,"pagetitle":"DynamicalSystemsBase.jl","title":"SciMLBase.isinplace","ref":"/dynamicalsystemsbase/stable/#SciMLBase.isinplace-Tuple{DynamicalSystem}","content":" SciMLBase.isinplace  —  Method isinplace(ds::DynamicalSystem) → true/false Return  true  if the dynamic rule of  ds  is in-place, i.e., a function mutating the state in place. If  true , the state is typically  Array , if  false , the state is typically  SVector . A front-end user will most likely not care about this information, but a developer may care. source"},{"id":130,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.successful_step","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.successful_step","content":" DynamicalSystemsBase.successful_step  —  Function successful_step(ds::DynamicalSystem) -> true/false Return  true  if the last  step!  call to  ds  was successful,  false  otherwise. For continuous time systems this uses DifferentialEquations.jl error checking, for discrete time it checks if any variable is  Inf  or  NaN . source"},{"id":131,"pagetitle":"DynamicalSystemsBase.jl","title":"SciMLBase.reinit!","ref":"/dynamicalsystemsbase/stable/#SciMLBase.reinit!-Tuple{DynamicalSystem, Vararg{Any}}","content":" SciMLBase.reinit!  —  Method reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds Reset the status of  ds , so that it is as if it has be just initialized with initial state  u . Practically every function of the ecosystem that evolves  ds  first calls this function on it. Besides the new initial state  u , you can also configure the keywords  t0 = initial_time(ds)  and  p = current_parameters(ds) . Note the default settings: the state and time are the initial, but the parameters are the current. The special method  reinit!(ds, ::Nothing; kwargs...)  is also available, which does nothing and leaves the system as is. This is so that downstream functions that call  reinit!  can still be used without resetting the system but rather continuing from its exact current state. source"},{"id":132,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_state!","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_state!","content":" DynamicalSystemsBase.set_state!  —  Function set_state!(ds::DynamicalSystem, u) Set the state of  ds  to  u , which must match dimensionality with that of  ds . Also ensure that the change is notified to whatever integration protocol is used. source"},{"id":133,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_parameter!","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_parameter!","content":" DynamicalSystemsBase.set_parameter!  —  Function set_parameter!(ds::DynamicalSystem, index, value) Change a parameter of  ds  given the  index  it has in the parameter container and the  value  to set it to. This function works for both array/dictionary containers as well as composite types. In the latter case  index  needs to be a  Symbol . source"},{"id":134,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_parameters!","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_parameters!","content":" DynamicalSystemsBase.set_parameters!  —  Function set_parameters!(ds::DynamicalSystem, p = initial_parameters(ds)) Set the parameter values in the  current_parameters (ds)  to match  p . This is done as an in-place overwrite by looping over the keys of  p . Hence the keys of  p  must be a subset of the keys of  current_parameters (ds) . source"},{"id":135,"pagetitle":"DynamicalSystemsBase.jl","title":"Time evolution","ref":"/dynamicalsystemsbase/stable/#Time-evolution","content":" Time evolution"},{"id":136,"pagetitle":"DynamicalSystemsBase.jl","title":"CommonSolve.step!","ref":"/dynamicalsystemsbase/stable/#CommonSolve.step!-Tuple{DynamicalSystem, Vararg{Any}}","content":" CommonSolve.step!  —  Method step!(ds::DiscreteTimeDynamicalSystem [, n::Integer]) → ds Evolve the discrete time dynamical system for 1 or  n  steps. step!(ds::ContinuousTimeDynamicalSystem, [, dt::Real [, stop_at_tdt]]) → ds Evolve the continuous time dynamical system for one integration step. Alternatively, if a  dt  is given, then progress the integration until there is a temporal difference  ≥ dt  (so, step  at least  for  dt  time). When  true  is passed to the optional third argument, the integration advances for exactly  dt  time. source"},{"id":137,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.trajectory","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.trajectory","content":" DynamicalSystemsBase.trajectory  —  Function trajectory(ds::DynamicalSystem, T [, u0]; kwargs...) → X, t Return a dataset  X  that will contain the trajectory of the system  ds , after evolving it for total time  T .  u0  is the state given given to  reinit!  prior to time evolution and defaults to  initial_state(ds) . See  StateSpaceSet  for info on how to use  X . The returned time vector is  t = (t0+Ttr):Δt:(t0+Ttr+T) . Keyword arguments Δt :  Time step of value output. For discrete time systems it must be an integer. Defaults to  0.1  for continuous and  1  for discrete time systems. If you don't have access to unicode, the keyword  Dt  can be used instead. Ttr = 0 : Transient time to evolve the initial state before starting saving states. t0 = initial_time(ds) : Starting time. save_idxs::AbstractVector{Int} : Which variables to output in  X  (by default all). source"},{"id":138,"pagetitle":"DynamicalSystemsBase.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/dynamicalsystemsbase/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple."},{"id":139,"pagetitle":"DynamicalSystemsBase.jl","title":"DeterministicIteratedMap","ref":"/dynamicalsystemsbase/stable/#DeterministicIteratedMap","content":" DeterministicIteratedMap"},{"id":140,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . source"},{"id":141,"pagetitle":"DynamicalSystemsBase.jl","title":"CoupledODEs","ref":"/dynamicalsystemsbase/stable/#CoupledODEs","content":" CoupledODEs"},{"id":142,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.CoupledODEs","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . DifferentialEquations.jl keyword arguments and interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(; stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false),), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  , however the majority of downstream functions in DynamicalSystems.jl assume that  f  is differentiable. The convenience constructor  CoupledODEs(prob::ODEProblem, diffeq)  and  CoupledODEs(ds::CoupledODEs, diffeq)  are also available. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available. source"},{"id":143,"pagetitle":"DynamicalSystemsBase.jl","title":"StroboscopicMap","ref":"/dynamicalsystemsbase/stable/#StroboscopicMap","content":" StroboscopicMap"},{"id":144,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap . source"},{"id":145,"pagetitle":"DynamicalSystemsBase.jl","title":"PoincareMap","ref":"/dynamicalsystemsbase/stable/#PoincareMap","content":" PoincareMap"},{"id":146,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.PoincareMap","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap) source"},{"id":147,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_crossing_time","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_crossing_time","content":" DynamicalSystemsBase.current_crossing_time  —  Function current_crossing_time(pmap::PoincareMap) → tcross Return the time of the latest crossing of the Poincare section. source"},{"id":148,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.poincaresos","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.poincaresos","content":" DynamicalSystemsBase.poincaresos  —  Function poincaresos(A::AbstractStateSpaceSet, plane; kwargs...) → P::StateSpaceSet Calculate the Poincaré surface of section of the given dataset with the given  plane  by performing linear interpolation betweeen points that sandwich the hyperplane. Argument  plane  and keywords  direction, warning, save_idxs  are the same as in  PoincareMap . source poincaresos(ds::CoupledODEs, plane, T = 1000.0; kwargs...) → P::StateSpaceSet Return the iterations of  ds  on the Poincaré surface of section with the  plane , by evolving  ds  up to a total of  T . Return a  StateSpaceSet  of the points that are on the surface of section. This function initializes a  PoincareMap  and steps it until its  current_crossing_time  exceeds  T . You can also use  trajectory  with  PoincareMap  to get a sequence of  N::Int  points instead. The keywords  Ttr, save_idxs  act as in  trajectory . See  PoincareMap  for  plane  and all other keywords. source"},{"id":149,"pagetitle":"DynamicalSystemsBase.jl","title":"TangentDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#TangentDynamicalSystem","content":" TangentDynamicalSystem"},{"id":150,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.CoreDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.CoreDynamicalSystem","content":" DynamicalSystemsBase.CoreDynamicalSystem  —  Type CoreDynamicalSystem Union type meaning either  DeterministicIteratedMap  or  CoupledODEs , which are the core systems whose dynamic rule  f  is known analytically. This type is used for deciding whether a creation of a  TangentDynamicalSystem  is possible or not. source"},{"id":151,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.TangentDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.TangentDynamicalSystem","content":" DynamicalSystemsBase.TangentDynamicalSystem  —  Type TangentDynamicalSystem <: DynamicalSystem\nTangentDynamicalSystem(ds::CoreDynamicalSystem; kwargs...) A dynamical system that bundles the evolution of  ds  (which must be an  CoreDynamicalSystem ) and  k  deviation vectors that are evolved according to the  dynamics in the tangent space  (also called linearized dynamics or the tangent dynamics). The state of  ds must  be an  AbstractVector  for  TangentDynamicalSystem . TangentDynamicalSystem  follows the  DynamicalSystem  interface with the following adjustments: reinit!  takes an additional keyword  Q0  (with same default as below) The additional functions  current_deviations  and  set_deviations!  are provided for the deviation vectors. Keyword arguments k  or  Q0 :  Q0  represents the initial deviation vectors (each column = 1 vector). If  k::Int  is given, a matrix  Q0  is created with the first  k  columns of the identity matrix. Otherwise  Q0  can be given directly as a matrix. It must hold that  size(Q, 1) == dimension(ds) . You can use  orthonormal  for random orthonormal vectors. By default  k = dimension(ds)  is used. u0 = current_state(ds) : Starting state. J  and  J0 : See section \"Jacobian\" below. Description Let  $u$  be the state of  ds , and  $y$  a deviation (or perturbation) vector. These two are evolved in parallel according to \\[\\begin{array}{rcl}\n\\frac{d\\vec{x}}{dt} &=& f(\\vec{x}) \\\\\n\\frac{dY}{dt} &=& J_f(\\vec{x}) \\cdot Y\n\\end{array}\n\\quad \\mathrm{or}\\quad\n\\begin{array}{rcl}\n\\vec{x}_{n+1} &=& f(\\vec{x}_n) \\\\\nY_{n+1} &=& J_f(\\vec{x}_n) \\cdot Y_n.\n\\end{array}\\] for continuous or discrete time respectively. Here  $f$  is the  dynamic_rule (ds)  and  $J_f$  is the Jacobian of  $f$ . Jacobian The keyword  J  provides the Jacobian function. It must be a Julia function in the same form as  f , the  dynamic_rule . Specifically,  J(u, p, n) -> M::SMatrix  for the out-of-place version or  J(M, u, p, n)  for the in-place version acting in-place on  M . in both cases  M  is a matrix whose columns are the deviation vectors. By default  J = nothing .  In this case  J  is constructed automatically using the module  ForwardDiff , hence its limitations also apply here. Even though  ForwardDiff  is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so it is recommended. Additionally, automatic and in-place Jacobians cannot be time dependent. The keyword  J0  allows you to pass an initialized Jacobian matrix  J0 . This is useful for large in-place systems where only a few components of the Jacobian change during the time evolution.  J0  can be a sparse or any other matrix type. If not given, a matrix of zeros is used.  J0  is ignored for out of place systems. source"},{"id":152,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_deviations","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_deviations","content":" DynamicalSystemsBase.current_deviations  —  Function current_deviations(tands::TangentDynamicalSystem) Return the deviation vectors of  tands  as a matrix with each column a vector. source"},{"id":153,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.set_deviations!","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.set_deviations!","content":" DynamicalSystemsBase.set_deviations!  —  Function set_deviations!(tands::TangentDynamicalSystem, Q) Set the deviation vectors of  tands  to be  Q , a matrix with each column a vector. source"},{"id":154,"pagetitle":"DynamicalSystemsBase.jl","title":"StateSpaceSets.orthonormal","ref":"/dynamicalsystemsbase/stable/#StateSpaceSets.orthonormal","content":" StateSpaceSets.orthonormal  —  Function orthonormal([T,] D, k) -> ws Return a matrix  ws  with  k  columns, each being an  D -dimensional orthonormal vector. T  is the return type and can be either  SMatrix  or  Matrix . If not given, it is  SMatrix  if  D*k < 100 , otherwise  Matrix ."},{"id":155,"pagetitle":"DynamicalSystemsBase.jl","title":"ProjectedDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#ProjectedDynamicalSystem","content":" ProjectedDynamicalSystem"},{"id":156,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\npds = # same as in above example... source"},{"id":157,"pagetitle":"DynamicalSystemsBase.jl","title":"ParallelDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#ParallelDynamicalSystem","content":" ParallelDynamicalSystem"},{"id":158,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ParallelDynamicalSystem","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ParallelDynamicalSystem","content":" DynamicalSystemsBase.ParallelDynamicalSystem  —  Type ParallelDynamicalSystem <: DynamicalSystem\nParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:AbstractArray}) A struct that evolves several  states  of a given dynamical system in parallel  at exactly the same times . Useful when wanting to evolve several different trajectories of the same system while ensuring that they share parameters and time vector. This struct follows the  DynamicalSystem  interface with the following adjustments: The function  current_state  is called as  current_state(pds, i::Int = 1)  which returns the  i th state. Same for  initial_state . Similarly,  set_state!  obtains a third argument  i::Int = 1  to set the  i -th state. current_states  and  initial_states  can be used to get all parallel states. reinit!  takes in a vector of states (like  states ) for  u . source"},{"id":159,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.initial_states","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.initial_states","content":" DynamicalSystemsBase.initial_states  —  Function initial_states(pds::ParallelDynamicalSystem) Return an iterator over the initial parallel states of  pds . source"},{"id":160,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.current_states","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.current_states","content":" DynamicalSystemsBase.current_states  —  Function current_states(pds::ParallelDynamicalSystem) Return an iterator over the parallel states of  pds . source"},{"id":161,"pagetitle":"DynamicalSystemsBase.jl","title":"ArbitrarySteppable","ref":"/dynamicalsystemsbase/stable/#ArbitrarySteppable","content":" ArbitrarySteppable"},{"id":162,"pagetitle":"DynamicalSystemsBase.jl","title":"DynamicalSystemsBase.ArbitrarySteppable","ref":"/dynamicalsystemsbase/stable/#DynamicalSystemsBase.ArbitrarySteppable","content":" DynamicalSystemsBase.ArbitrarySteppable  —  Type ArbitrarySteppable <: DiscreteTimeDynamicalSystem\nArbitrarySteppable(\n    model, step!, extract_state, extract_parameters, reset_model!;\n    isdeterministic = true, set_state = reinit!,\n) A dynamical system generated by an arbitrary \"model\" that can be stepped  in-place  with some function  step!(model)  for 1 step. The state of the model is extracted by the  extract_state(model) -> u  function The parameters of the model are extracted by the  extract_parameters(model) -> p  function. The system may be re-initialized, via  reinit! , with the  reset_model!  user-provided function that must have the call signature reset_model!(model, u, p) given a (potentially new) state  u  and parameter container  p , both of which will default to the initial ones in the  reinit!  call. ArbitrarySteppable  exists to provide the DynamicalSystems.jl interface to models from other packages that could be used within the DynamicalSystems.jl library.  ArbitrarySteppable  follows the  DynamicalSystem  interface with the following adjustments: initial_time  is always 0, as time counts the steps the model has taken since creation or last  reinit!  call. set_state!  is the same as  reinit!  by default. If not, the keyword argument  set_state  is a function  set_state(model, u)  that sets the state of the model to  u . The keyword  isdeterministic  should be set properly, as it decides whether downstream algorithms should error or not. source"},{"id":163,"pagetitle":"DynamicalSystemsBase.jl","title":"Parallelization","ref":"/dynamicalsystemsbase/stable/#Parallelization","content":" Parallelization Since  DynamicalSystem s are mutable, one needs to copy them before parallelizing, to avoid having to deal with complicated race conditions etc. The simplest way is with  deepcopy . Here is an example block that shows how to parallelize calling some expensive function (e.g., calculating the Lyapunov exponent) over a parameter range using  Threads : ds = DynamicalSystem(f, u, p) # some concrete implementation\nparameters = 0:0.01:1\noutputs = zeros(length(parameters))\n\n# Since `DynamicalSystem`s are mutable, we need to copy to parallelize\nsystems = [deepcopy(ds) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, ds) # we can save 1 copy\n\nThreads.@threads for i in eachindex(parameters)\n    system = systems[Threads.threadid()]\n    set_parameter!(system, 1, parameters[i])\n    outputs[i] = expensive_function(system, args...)\nend"},{"id":164,"pagetitle":"DynamicalSystemsBase.jl","title":"Examples","ref":"/dynamicalsystemsbase/stable/#Examples","content":" Examples"},{"id":165,"pagetitle":"DynamicalSystemsBase.jl","title":"Iterated map, out of place","ref":"/dynamicalsystemsbase/stable/#Iterated-map,-out-of-place","content":" Iterated map, out of place Let's make the  Hénon map  as an example. using DynamicalSystemsBase\n\nhenon_rule(x, p, n) = SVector(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nu0 = zeros(2)\np0 = [1.4, 0.3]\n\nhenon = DeterministicIteratedMap(henon_rule, u0, p0) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  henon_rule\n parameters:    [1.4, 0.3]\n time:          0\n state:         [0.0, 0.0]\n and get a trajectory X, t = trajectory(henon, 10000; Ttr = 100)\nX 2-dimensional StateSpaceSet{Float64} with 10001 points\n  0.638892   0.122624\n  0.551168   0.191668\n  0.766367   0.16535\n  0.343105   0.22991\n  1.0651     0.102931\n -0.485284   0.31953\n  0.989829  -0.145585\n -0.517251   0.296949\n  0.92238   -0.155175\n -0.346275   0.276714\n  ⋮         \n  0.676944  -0.20255\n  0.155895   0.203083\n  1.16906    0.0467685\n -0.866609   0.350718\n  0.299302  -0.259983\n  0.614603   0.0897906\n  0.560959   0.184381\n  0.743836   0.168288\n  0.393679   0.223151"},{"id":166,"pagetitle":"DynamicalSystemsBase.jl","title":"Coupled ODEs, in place","ref":"/dynamicalsystemsbase/stable/#Coupled-ODEs,-in-place","content":" Coupled ODEs, in place Let's make the Lorenz system  Hénon map  as an example. The system is small, and therefore should utilize the out of place syntax, but for the case of example, we will use the in-place syntax. We'll also use a high accuracy solver from OrdinaryDiffEq.jl. using DynamicalSystemsBase\nusing OrdinaryDiffEq: Vern9\n\n@inbounds function lorenz_rule!(du, u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du[1] = σ*(u[2]-u[1])\n    du[2] = u[1]*(ρ-u[3]) - u[2]\n    du[3] = u[1]*u[2] - β*u[3]\n    return nothing\nend\n\nu0 = [0, 10.0, 0]\np0 = [10, 28, 8/3]\ndiffeq = (alg = Vern9(), abstol = 1e-9, reltol = 1e-9)\n\nlorenz = CoupledODEs(lorenz_rule!, u0, p0; diffeq) and get a trajectory X, t = trajectory(lorenz, 1000; Δt = 0.05, Ttr = 10)\nX"},{"id":167,"pagetitle":"DynamicalSystemsBase.jl","title":"Advanced example","ref":"/dynamicalsystemsbase/stable/#Advanced-example","content":" Advanced example This is an advanced example of making an in-place implementation of coupled  standard maps . It will utilize a handcoded Jacobian, a sparse matrix for the Jacobinan, a default initial Jacobian matrix, as well as function-like-objects as the dynamic rule. Coupled standard maps is a deterministic iterated map that can have arbitrary number of equations of motion, since you can couple  N  standard maps which are 2D maps, like so: \\[\\theta_{i}' = \\theta_i + p_{i}' \\\\\np_{i}' = p_i + k_i\\sin(\\theta_i) - \\Gamma \\left[\\sin(\\theta_{i+1} - \\theta_{i}) + \\sin(\\theta_{i-1} - \\theta_{i}) \\right]\\] To model this, we will make a dedicated  struct , which is parameterized on the number of coupled maps: struct CoupledStandardMaps{N}\n    idxs::SVector{N, Int}\n    idxsm1::SVector{N, Int}\n    idxsp1::SVector{N, Int}\nend (what these fields are will become apparent later) We initialize the struct with the amount of standard maps we want to couple, and we also define appropriate parameters: M = 5  # couple number\nu0 = 0.001rand(2M) #initial state\nks = 0.9ones(M) # nonlinearity parameters\nΓ = 1.0 # coupling strength\np = (ks, Γ) # parameter container\n\n# Create struct:\nSV = SVector{M, Int}\nidxs = SV(1:M...) # indexes of thetas\nidxsm1 = SV(circshift(idxs, +1)...)  #indexes of thetas - 1\nidxsp1 = SV(circshift(idxs, -1)...)  #indexes of thetas + 1\n# So that:\n# x[i] ≡ θᵢ\n# x[[idxsp1[i]]] ≡ θᵢ+₁\n# x[[idxsm1[i]]] ≡ θᵢ-₁\ncsm = CoupledStandardMaps{M}(idxs, idxsm1, idxsp1) Main.CoupledStandardMaps{5}([1, 2, 3, 4, 5], [5, 1, 2, 3, 4], [2, 3, 4, 5, 1]) We will now use this struct to define a  function-like-object , a Type that also acts as a function function (f::CoupledStandardMaps{N})(xnew::AbstractVector, x, p, n) where {N}\n    ks, Γ = p\n    @inbounds for i in f.idxs\n\n        xnew[i+N] = mod2pi(\n            x[i+N] + ks[i]*sin(x[i]) -\n            Γ*(sin(x[f.idxsp1[i]] - x[i]) + sin(x[f.idxsm1[i]] - x[i]))\n        )\n\n        xnew[i] = mod2pi(x[i] + xnew[i+N])\n    end\n    return nothing\nend We will use  the same struct  to create a function for the Jacobian: function (f::CoupledStandardMaps{M})(\n    J::AbstractMatrix, x, p, n) where {M}\n\n    ks, Γ = p\n    # x[i] ≡ θᵢ\n    # x[[idxsp1[i]]] ≡ θᵢ+₁\n    # x[[idxsm1[i]]] ≡ θᵢ-₁\n    @inbounds for i in f.idxs\n        cosθ = cos(x[i])\n        cosθp= cos(x[f.idxsp1[i]] - x[i])\n        cosθm= cos(x[f.idxsm1[i]] - x[i])\n        J[i+M, i] = ks[i]*cosθ + Γ*(cosθp + cosθm)\n        J[i+M, f.idxsm1[i]] = - Γ*cosθm\n        J[i+M, f.idxsp1[i]] = - Γ*cosθp\n        J[i, i] = 1 + J[i+M, i]\n        J[i, f.idxsm1[i]] = J[i+M, f.idxsm1[i]]\n        J[i, f.idxsp1[i]] = J[i+M, f.idxsp1[i]]\n    end\n    return nothing\nend This is possible because the system state is a  Vector  while the Jacobian is a  Matrix , so multiple dispatch can differentiate between the two. Notice in addition, that the Jacobian function accesses  only half the elements of the matrix . This is intentional, and takes advantage of the fact that the other half is constant. We can leverage this further, by making the Jacobian a sparse matrix. Because the  DynamicalSystem  constructors allow us to give in a pre-initialized Jacobian matrix, we take advantage of that and create: using SparseArrays\nJ = zeros(eltype(u0), 2M, 2M)\n# Set ∂/∂p entries (they are eye(M,M))\n# And they dont change they are constants\nfor i in idxs\n    J[i, i+M] = 1\n    J[i+M, i+M] = 1\nend\nsparseJ = sparse(J)\n\ncsm(sparseJ, u0, p, 0) # apply Jacobian to initial state\nsparseJ 10×10 SparseArrays.SparseMatrixCSC{Float64, Int64} with 40 stored entries:\n  3.9  -1.0    ⋅     ⋅   -1.0  1.0   ⋅    ⋅    ⋅    ⋅ \n -1.0   3.9  -1.0    ⋅     ⋅    ⋅   1.0   ⋅    ⋅    ⋅ \n   ⋅   -1.0   3.9  -1.0    ⋅    ⋅    ⋅   1.0   ⋅    ⋅ \n   ⋅     ⋅   -1.0   3.9  -1.0   ⋅    ⋅    ⋅   1.0   ⋅ \n -1.0    ⋅     ⋅   -1.0   3.9   ⋅    ⋅    ⋅    ⋅   1.0\n  2.9  -1.0    ⋅     ⋅   -1.0  1.0   ⋅    ⋅    ⋅    ⋅ \n -1.0   2.9  -1.0    ⋅     ⋅    ⋅   1.0   ⋅    ⋅    ⋅ \n   ⋅   -1.0   2.9  -1.0    ⋅    ⋅    ⋅   1.0   ⋅    ⋅ \n   ⋅     ⋅   -1.0   2.9  -1.0   ⋅    ⋅    ⋅   1.0   ⋅ \n -1.0    ⋅     ⋅   -1.0   2.9   ⋅    ⋅    ⋅    ⋅   1.0 Now we are ready to create our dynamical system ds = DeterministicIteratedMap(csm, u0, p) 10-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      true\n dynamic rule:  CoupledStandardMaps\n parameters:    ([0.9, 0.9, 0.9, 0.9, 0.9], 1.0)\n time:          0\n state:         [0.00011344809241959375, 0.0006504739063002315, 0.0005770114186720249, 0.00021472051946678838, 0.0005560367648929367, 0.0006340503380402208, 0.0009160261845680782, 0.00074732589590968, 0.0001660338117954866, 0.0008529457953716969]\n Of course, the reason we went through all this trouble was to make a  TangentDynamicalSystem , that can actually use the Jacobian function. tands = TangentDynamicalSystem(ds; J = csm, J0 = sparseJ, k = M) 10-dimensional TangentDynamicalSystem\n deterministic:     true\n discrete time:     true\n in-place:          true\n dynamic rule:      CoupledStandardMaps\n jacobian:          CoupledStandardMaps\n deviation vectors: 5\n parameters:        ([0.9, 0.9, 0.9, 0.9, 0.9], 1.0)\n time:              0\n state:             [0.00011344809241959375, 0.0006504739063002315, 0.0005770114186720249, 0.00021472051946678838, 0.0005560367648929367, 0.0006340503380402208, 0.0009160261845680782, 0.00074732589590968, 0.0001660338117954866, 0.0008529457953716969]\n step!(tands, 5)\ncurrent_deviations(tands) 10×5 view(::Matrix{Float64}, :, 2:6) with eltype Float64:\n  3341.12   -2371.4      734.408    665.697  -2301.89\n -2555.31    3707.63   -2661.4      841.162    736.803\n   843.191  -2721.88    3826.0    -2668.62     790.861\n   687.269    787.566  -2501.73    3495.34   -2400.05\n -2245.09     663.599    684.03   -2265.73    3230.42\n  2687.35   -1947.1      623.08     554.576  -1877.87\n -2130.41    3052.45   -2235.97     729.435    625.478\n   731.456  -2296.21    3170.29   -2243.22     679.333\n   576.086    676.041  -2076.91    2840.95   -1975.68\n -1821.27     552.49     572.86   -1841.84    2577.11 (the deviation vectors will increase in magnitude rapidly because the dynamical system is chaotic) DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":172,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.jl","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.jl","content":" PredefinedDynamicalSystems.jl"},{"id":173,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems","content":" PredefinedDynamicalSystems  —  Module PredefinedDynamicalSystems.jl Module which contains pre-defined dynamical systems that can be used by the  DynamicalSystems.jl  library. To install it, run  import Pkg; Pkg.add(\"PredefinedDynamicalSystems\") . Predefined systems exist as functions that return a  DynamicalSystem  instance. They are accessed like: ds = PredefinedDynamicalSystems.lorenz(u0; ρ = 32.0) The alias  Systems  is also exported as a deprecation. This module is provided purely as a convenience. It does not have any actual tests, and it is not guaranteed to be stable in future versions. It is not recommended to use this module for anything else besides on-the-spot demonstrative examples. For some systems, a Jacobian function is also defined. The naming convention for the Jacobian function is  \\$(name)_jacob . So, for the above example we have  J = Systems.lorenz_jacob . All available systems are provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source All currently implemented predefined systems are listed below:"},{"id":174,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.antidots","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.antidots","content":" PredefinedDynamicalSystems.antidots  —  Function antidots([u]; B = 1.0, d0 = 0.3, c = 0.2) An antidot \"superlattice\" is a Hamiltonian system that corresponds to a smoothened periodic Sinai billiard with disk diameter  d0  and smooth factor  c [Datseris2019] . This version is the two dimensional classical form of the system, with quadratic dynamical rule and a perpendicular magnetic field. Notice that the dynamical rule is with respect to the velocity instead of momentum, i.e.: \\[\\begin{aligned}\n\\dot{x} &= v_x \\\\\n\\dot{y} &= v_y \\\\\n\\dot{v_x} &= B v_y - U_x \\\\\n\\dot{v_y} &= -B v_x - U_y \\\\\n\\end{aligned}\\] with  $U$  the potential energy: \\[U = \\left(\\tfrac{1}{c^4}\\right) \\left[\\tfrac{d_0}{2} + c - r_a\\right]^4\\] if  $r_a = \\sqrt{(x \\mod 1)^2 + (y \\mod 1)^2} < \\frac{d_0}{2} + c$  and 0 otherwise. That is, the potential is periodic with period 1 in both  $x, y$  and normalized such that for energy value of 1 it is a circle of diameter  $d_0$ . The magnetic field is also normalized such that for value  B = 1  the cyclotron diameter is 1. source"},{"id":175,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.arnoldcat","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.arnoldcat","content":" PredefinedDynamicalSystems.arnoldcat  —  Function arnoldcat(u0 = [0.001245, 0.00875]) \\[f(x,y) = (2x+y,x+y) \\mod 1\\] Arnold's cat map. A chaotic map from the torus into itself, used by Vladimir Arnold in the 1960s. [1] [1] : Arnol'd, V. I., & Avez, A. (1968). Ergodic problems of classical mechanics. source"},{"id":176,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.betatransformationmap","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.betatransformationmap","content":" PredefinedDynamicalSystems.betatransformationmap  —  Function betatransformationmap(u0 = 0.25; β=2.0)-> ds The beta transformation, also called the generalized Bernoulli map, or the βx map, is described by \\[\\begin{aligned}\nx_{n+1} = \\beta x (\\mod 1).\n\\end{aligned}\\] The parameter β controls the dynamics of the map. Its Lyapunov exponent can be analytically shown to be λ = ln(β)  [Ott2002] . At β=2, it becomes the dyadic transformation, also known as the bit shift map, the 2x mod 1 map, the Bernoulli map or the sawtooth map. The typical trajectory for this case is chaotic, though there are countably infinite periodic orbits  [Ott2002] . source"},{"id":177,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.chua","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.chua","content":" PredefinedDynamicalSystems.chua  —  Function chua(u0 = [0.7, 0.0, 0.0]; a = 15.6, b = 25.58, m0 = -8/7, m1 = -5/7) \\[\\begin{aligned}\n\\dot{x} &= a [y - h(x)]\\\\\n\\dot{y} &= x - y+z \\\\\n\\dot{z} &= b y\n\\end{aligned}\\] where  $h(x)$  is defined by \\[h(x) = m_1 x + \\frac 1 2 (m_0 - m_1)(|x + 1| - |x - 1|)\\] This is a 3D continuous system that exhibits chaos. Chua designed an electronic circuit with the expressed goal of exhibiting chaotic motion, and this system is obtained by rescaling the circuit units to simplify the form of the equation.  [Chua1992] The parameters are  $a$ ,  $b$ ,  $m_0$ , and  $m_1$ . Setting  $a = 15.6$ ,  $m_0 = -8/7$  and  $m_1 = -5/7$ , and varying the parameter  $b$  from  $b = 25$  to  $b = 51$ , one observes a classic period-doubling bifurcation route to chaos.  [Chua2007] The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":178,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.coupled_roessler","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.coupled_roessler","content":" PredefinedDynamicalSystems.coupled_roessler  —  Function coupled_roessler(u0=[1, -2, 0, 0.11, 0.2, 0.1];\nω1 = 0.18, ω2 = 0.22, a = 0.2, b = 0.2, c = 5.7, k1 = 0.115, k2 = 0.0) Two coupled Rössler oscillators, used frequently in the study of chaotic synchronization. The parameter container has the parameters in the same order as stated in this function's documentation string. The equations are: \\[\\begin{aligned}\n\\dot{x_1} &= -\\omega_1 y_1-z_1 \\\\\n\\dot{y_1} &= \\omega_1 x+ay_1 + k_1(y_2 - y_1) \\\\\n\\dot{z_1} &= b + z_1(x_1-c) \\\\\n\\dot{x_2} &= -\\omega_2 y_2-z_2 \\\\\n\\dot{y_2} &= \\omega_2 x+ay_2 + k_2(y_1 - y_2) \\\\\n\\dot{z_2} &= b + z_2(x_2-c) \\\\\n\\end{aligned}\\] source"},{"id":179,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.coupledstandardmaps","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.coupledstandardmaps","content":" PredefinedDynamicalSystems.coupledstandardmaps  —  Function coupledstandardmaps(M::Int, u0 = 0.001rand(2M); ks = ones(M), Γ = 1.0) \\[\\begin{aligned}\n\\theta_{i}' &= \\theta_i + p_{i}' \\\\\np_{i}' &= p_i + k_i\\sin(\\theta_i) - \\Gamma \\left[\n\\sin(\\theta_{i+1} - \\theta_{i}) + \\sin(\\theta_{i-1} - \\theta_{i})\n\\right]\n\\end{aligned}\\] A discrete system of  M  nonlinearly coupled standard maps, first introduced in [1] to study diffusion and chaos thresholds. The  total  dimension of the system is  2M . The maps are coupled through  Γ  and the  i -th map has a nonlinear parameter  ks[i] . The first  M  parameters are the  ks , the  M+1 th parameter is  Γ . The first  M  entries of the state are the angles, the last  M  are the momenta. [1] : H. Kantz & P. Grassberger, J. Phys. A  21 , pp 127–133 (1988) source"},{"id":180,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.double_pendulum","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.double_pendulum","content":" PredefinedDynamicalSystems.double_pendulum  —  Function double_pendulum(u0 = [π/2, 0, 0, 0.5];\n                G=10.0, L1 = 1.0, L2 = 1.0, M1 = 1.0, M2 = 1.0) Famous chaotic double pendulum system (also used for our logo!). Keywords are gravity ( G ), lengths of each rod ( L1  and  L2 ) and mass of each ball ( M1  and  M2 ). Everything is assumed in SI units. The variables order is  $[θ₁, ω₁, θ₂, ω₂]$  and they satisfy: \\[\\begin{aligned}\nθ̇₁ &= ω₁ \\\\\nω̇₁ &= [M₂ L₁ ω₁² \\sin φ \\cos φ + M₂ G \\sin θ₂ \\cos φ +\n       M₂ L₂ ω₂² \\sin φ - (M₁ + M₂) G \\sin θ₁] / (L₁ Δ) \\\\\nθ̇₂ &= ω₂ \\\\\nω̇₂ &= [-M₂ L₂ ω₂² \\sin φ \\cos φ + (M₁ + M₂) G \\sin θ₁ \\cos φ -\n         (M₁ + M₂) L₁ ω₁² \\sin φ - (M₁ + M₂) G \\sin Θ₂] / (L₂ Δ)\n\\end{aligned}\\] where  $φ = θ₂-θ₁$  and  $Δ = (M₁ + M₂) - M₂ \\cos² φ$ . Jacobian is created automatically (thus methods that use the Jacobian will be slower)! (please contribute the Jacobian in LaTeX :smile:) The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":181,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.duffing","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.duffing","content":" PredefinedDynamicalSystems.duffing  —  Function duffing(u0 = [0.1, 0.25]; ω = 2.2, f = 27.0, d = 0.2, β = 1) The (forced) duffing oscillator, that satisfies the equation \\[\\ddot{x} + d \\dot{x} + β x + x^3 = f \\cos(\\omega t)\\] with  f, ω  the forcing strength and frequency and  d  the damping. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":182,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.fitzhugh_nagumo","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.fitzhugh_nagumo","content":" PredefinedDynamicalSystems.fitzhugh_nagumo  —  Function fitzhugh_nagumo(u = 0.5ones(2); a=3.0, b=0.2, ε=0.01, I=0.0) Famous excitable system which emulates the firing of a neuron, with rule \\[\\begin{aligned}\n\\dot{v} &= av(v-b)(1-v) - w + I \\\\\n\\dot{w} &= \\varepsilon(v - w)\n\\end{aligned}\\] More details in the  Scholarpedia  entry. source"},{"id":183,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.forced_pendulum","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.forced_pendulum","content":" PredefinedDynamicalSystems.forced_pendulum  —  Function forced_pendulum(u0 = [0.1, 0.25]; ω = 2.2, f = 27.0, d = 0.2) The standard forced damped pendulum with a sine response force. duffing oscillator, that satisfies the equation \\[\\ddot{x} + d \\dot{x} + \\sin(x) = f \\cos(\\omega t)\\] with  f, ω  the forcing strength and frequency and  d  the damping. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":184,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.gissinger","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.gissinger","content":" PredefinedDynamicalSystems.gissinger  —  Function gissinger(u0 = [3, 0.5, 1.5]; μ = 0.119, ν = 0.1, Γ = 0.9) \\[\\begin{aligned}\n\\dot{Q} &= \\mu Q - VD \\\\\n\\dot{D} &= -\\nu D + VQ \\\\\n\\dot{V} &= \\Gamma -V + QD\n\\end{aligned}\\] A continuous system that models chaotic reversals due to Gissinger  [Gissinger2012] , applied to study the reversals of the magnetic field of the Earth. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":185,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.grebogi_map","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.grebogi_map","content":" PredefinedDynamicalSystems.grebogi_map  —  Function grebogi_map(u0 = [0.2, 0.]; a = 1.32, b=0.9, J₀=0.3) \\[\\begin{aligned}\n\\theta_{n+1} &= \\theta_n +   a\\sin 2 \\theta_n -b \\sin 4 \\theta_n -x_n\\sin \\theta_n\\\\\nx_{n+1} &= -J_0 \\cos \\theta_n\n\\end{aligned}\\] This map has two fixed point at  (0,-J_0)  and  (π,J_0)  which are attracting for  |1+2a-4b|<1 . There is a chaotic transient dynamics before the dynamical systems settles at a fixed point. This map illustrate the fractalization of the basins boundary and its uncertainty exponent  α  is roughly 0.2. source"},{"id":186,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.guckenheimer_holmes","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.guckenheimer_holmes","content":" PredefinedDynamicalSystems.guckenheimer_holmes  —  Function guckenheimer_holmes(u0=[-0.55582369,0.05181624,0.37766104];\n    a = 0.4,\n    b = 20.25,\n    c = 3,\n    d = 1.6,\n    e = 1.7,\n    f = 0.44) \\[\\begin{aligned}\n\\dot{x} &= ax - by + czx + dz(x^2 + y^2)\\\\\n\\dot{y} &= ay + bx + czy\\\\\n\\dot{z} &= e - z^2 - f(x^2 + y^2) - az^3\n\\end{aligned}\\] A nonlinear oscillator  [GuckenheimerHolmes1983] . source"},{"id":187,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.halvorsen","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.halvorsen","content":" PredefinedDynamicalSystems.halvorsen  —  Function halvorsen(u0=[-8.6807408,-2.4741399,0.070775762]; a = 1.4, b = 4.0) \\[\\begin{aligned}\n\\dot{x} &= -a*x - b*(y + z) - y^2\\\\\n\\dot{y} &= -a*y - b*(z + x) - z^2\\\\\n\\dot{z} &= -a*z - b*(x + y) - x^2\n\\end{aligned}\\] An algebraically-simple chaotic system with quadratic nonlinearity  [Sprott2010] . source"},{"id":188,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.henon","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.henon","content":" PredefinedDynamicalSystems.henon  —  Function henon(u0=zeros(2); a = 1.4, b = 0.3) \\[\\begin{aligned}\nx_{n+1} &= 1 - ax^2_n+y_n \\\\\ny_{n+1} & = bx_n\n\\end{aligned}\\] The Hénon map is a two-dimensional mapping due to Hénon [1] that can display a strange attractor (at the default parameters). In addition, it also displays many other aspects of chaos, like period doubling or intermittency, for other parameters. According to the author, it is a system displaying all the properties of the Lorentz system (1963) while being as simple as possible. Default values are the ones used in the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : M. Hénon, Commun.Math. Phys.  50 , pp 69 (1976) source"},{"id":189,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.henonheiles","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.henonheiles","content":" PredefinedDynamicalSystems.henonheiles  —  Function henonheiles(u0=[0, -0.25, 0.42081,0]) \\[\\begin{aligned}\n\\dot{x} &= p_x \\\\\n\\dot{y} &= p_y \\\\\n\\dot{p}_x &= -x -2 xy \\\\\n\\dot{p}_y &= -y - (x^2 - y^2)\n\\end{aligned}\\] The Hénon–Heiles system  [HénonHeiles1964]  is a conservative dynamical system and was introduced as a simplification of the motion of a star around a galactic center. It was originally intended to study the existence of a \"third integral of motion\" (which would make this 4D system integrable). In that search, the authors encountered chaos, as the third integral existed for only but a few initial conditions. The default initial condition is a typical chaotic orbit. The function  Systems.henonheiles_ics(E, n)  generates a grid of  n×n  initial conditions, all having the same energy  E . source"},{"id":190,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hindmarshrose","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hindmarshrose","content":" PredefinedDynamicalSystems.hindmarshrose  —  Function hindmarshrose(u0=[-1.0, 0.0, 0.0]; a=1, b=3, c=1, d=5, r=0.001, s=4, xr=-8/5, I=2.0) -> ds \\[\\begin{aligned}\n\\dot{x} &= y - ax^3 + bx^2 +I - z, \\\\\n\\dot{y} &= c - dx^2 -y, \\\\\n\\dot{z} &= r(s(x - x_r) - z)\n\\end{aligned}\\] The Hindmarsh-Rose model reproduces the bursting behavior of a neuron's membrane potential, characterized by a fast sequence of spikes followed by a quiescent period. The  x  variable describes the membane potential, whose behavior can be controlled by the applied current  I ; the  y  variable describes the sodium and potassium ionic currents, and  z  describes an adaptation current  [HindmarshRose1984] . The default parameter values are taken from  [HindmarshRose1984] , chosen to lead to periodic bursting. source"},{"id":191,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hindmarshrose_two_coupled","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hindmarshrose_two_coupled","content":" PredefinedDynamicalSystems.hindmarshrose_two_coupled  —  Function hindmarshrose_two_coupled(u0=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6];\na = 1.0, b = 3.0, d = 5.0, r = 0.001, s = 4.0, xr = -1.6, I = 4.0,\nk1 = -0.17, k2 = -0.17, k_el = 0.0, xv = 2.0) \\[\\begin{aligned}\n\\dot x_{i} = y_{i} + bx^{2}_{i} - ax^{3}_{i} - z_{i} + I - k_{i}(x_{i} - v_{s})\\Gamma(x_{j}) + k(x_{j} - x_{i})\\\\\n\\dot y_{i} = c - d x^{2}_{i} - y_{i}\\\\\n\\dot z_{i} = r[s(x_{i} - x_{R}) - z_{i}]\\\\\n\\i,j=1,2 (i\\neq j).\\\\\n\\end{aligned}\\] The two coupled Hindmarsh Rose element by chemical and electrical synapse. it is modelling the dynamics of a neuron's membrane potential. The default parameter values are taken from article \"Dragon-king-like extreme events in coupled bursting neurons\", DOI:https://doi.org/10.1103/PhysRevE.97.062311. source"},{"id":192,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hodgkinhuxley","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hodgkinhuxley","content":" PredefinedDynamicalSystems.hodgkinhuxley  —  Function hodgkinhuxley(u0=[-60.0, 0.0, 0.0, 0.0]; I = 12.0, Vna = 50.0, Vk = -77.0, Vl = -54.4, gna = 120.0,gk = 36.0, gl = 0.3) -> ds \\[\\begin{aligned}\nC_m \\frac{dV_m}{dt} &= -\\overline{g}_\\mathrm{K} n^4 (V_m - V_\\mathrm{K}) - \\overline{g}_\\mathrm{Na} m^3 h(V_m - V_\\mathrm{Na}) - \\overline{g}_l (V_m - Vl) + I\\\\\n\\dot{n} &= \\alpha_n(V_m)(1-n) - \\beta_n(V_m)n \\\\\n\\dot{m} &= \\alpha_m(V_m)(1-m) - \\beta_m(V_m)m \\\\\n\\dot{h} &= \\alpha_h(V_m)(1-h) - \\beta_h(V_m)h \\\\\n\\alpha_n(V_m) = \\frac{0.01(V+55)}{1 - \\exp(\\frac{1V+55}{10})} \\quad\n\\alpha_m(V_m) = \\frac{0.1(V+40)}{1 - \\exp(\\frac{V+40}{10})} \\quad\n\\alpha_h(V_m) = 0.07 \\exp(-\\frac{(V+65)}{20}) \\\\\n\\beta_n(V_m) = 0.125 \\exp(-\\frac{V+65}{80}) \\quad\n\\beta_m(V_m) = 4 \\exp(-\\frac{V+65}{18}) \\quad\n\\beta_h(V_m) = \\frac{1}{1 + \\exp(-\\frac{V+35}{10})}\n\\end{aligned}\\] The Nobel-winning four-dimensional dynamical system due to Hodgkin and Huxley  [HodgkinHuxley1952] , which describes the electrical spiking activity (action potentials) in neurons. A complete description of all parameters and variables is given in  [HodgkinHuxley1952] ,  [Ermentrout2010] , and  [Abbott2005] . The equations and default parameters used here are taken from  [Ermentrout2010] [Abbott2005] . They differ slightly from the original paper  [HodgkinHuxley1952] , since they were changed to shift the resting potential to -65 mV, instead of the 0mV in the original paper. Varying the injected current I from  I = -5   to   I = 12  takes the neuron from quiescent to a single spike, and to a tonic (repetitive) spiking. This is due to a subcritical Hopf bifurcation, which occurs close to  I = 9.5 . source"},{"id":193,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_bao","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_bao","content":" PredefinedDynamicalSystems.hyper_bao  —  Function function hyper_bao(u0 = [5.0, 8.0, 12.0, 21.0];\n    a = 36.0,\n    b = 3.0,\n    c = 20.5,\n    d = 0.1,\n    k = 21.0) \\[\\begin{aligned}\n\\dot{x} &= a (y - x) + w\\\\\n\\dot{y} &= c y - x z\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= k x - d y z\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system[^Bo-Cheng2008]. [^Bo-Cheng2008]:     Bo-Cheng, B., & Zhong, L. (2008).     A hyperchaotic attractor coined from chaotic Lü system.     Chinese Physics Letters, 25(7), 2396. source"},{"id":194,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_cai","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_cai","content":" PredefinedDynamicalSystems.hyper_cai  —  Function function hyper_cai(u0 = [1.0, 1.0, 20.0, 10.0];\n    a = 27.5,\n    b = 3.0,\n    c = 19.3,\n    d = 2.9,\n    e = 3.3) \\[\\begin{aligned}\n\\dot{x} &= a (y - x)\\\\\n\\dot{y} &= b x + c y - x z + w\\\\\n\\dot{z} &= -d z + y^2\\\\\n\\dot{w} &= -e x\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Finance system [Cai2007] . source"},{"id":195,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_jha","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_jha","content":" PredefinedDynamicalSystems.hyper_jha  —  Function function hyper_jha(u0 = [0.1, 0.1, 0.1, 0.1];\n    a = 10.0,\n    b = 28.0,\n    c = 8/3,\n    d = 1.3) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= x*(b - z) - y\\\\\n\\dot{z} &= x*y - c*z\\\\\n\\dot{w} &= d*w -x*z\n\\end{aligned}\\] An extension of the Lorenz system showchasing hyperchaos [Hussain2015] . source"},{"id":196,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_lorenz","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_lorenz","content":" PredefinedDynamicalSystems.hyper_lorenz  —  Function function hyper_lorenz(u0 = [-10.0, -6.0, 0.0, 10.0];\n    a = 10.0,\n    b = 28.0,\n    c = 8/3,\n    d = -1.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= x*(b - z) - y\\\\\n\\dot{z} &= x*y - c*z\\\\\n\\dot{w} &= d*w -y*z\n\\end{aligned}\\] An extension of the Lorenz system showchasing hyperchaos [Wang2008] . An hyperchaotic system is characterized by two positive Lyapunov exponents. source"},{"id":197,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_lu","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_lu","content":" PredefinedDynamicalSystems.hyper_lu  —  Function function hyper_lu(u0 = [5.0, 8.0, 12.0, 21.0];\n    a = 36,\n    b = 3.0,\n    c = 20.0,\n    d = 1.3) \\[\\begin{aligned}\n\\dot{x} &= a (y - x) + w\\\\\n\\dot{y} &= c y - x z\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= d w + x z\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system [Chen2006] . source"},{"id":198,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_pang","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_pang","content":" PredefinedDynamicalSystems.hyper_pang  —  Function function hyper_pang(u0 = [1.0, 1.0, 10.0, 1.0];\n    a = 36,\n    b = 3.0,\n    c = 20.0,\n    d = 2.0) \\[\\begin{aligned}\n\\dot{x} &= a (y - x)\\\\\n\\dot{y} &= -x z + c y + w\\\\\n\\dot{z} &= x y - b z\\\\\n\\dot{w} &= -d x - d y\n\\end{aligned}\\] A system showchasing hyperchaos obtained from the Lu system [Pang2011] . source"},{"id":199,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_qi","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_qi","content":" PredefinedDynamicalSystems.hyper_qi  —  Function function hyper_qi(u0 = [10.0, 15.0, 20.0, 22.0];\n    a = 50.0,\n    b = 24.0,\n    c = 13,\n    d = 8,\n    e = 33,\n    f = 30) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + y*z\\\\\n\\dot{y} &= b*(x + y) - xz\\\\\n\\dot{z} &= - c*z - e*w + x*y\\\\\n\\dot{w} &= -d*w + f*z +x*y\n\\end{aligned}\\] A hyperchaotic dynamical systems, showcasing a wide range of different behaviors, including rich bifurcations in different directions [Qi2008] . source"},{"id":200,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_roessler","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_roessler","content":" PredefinedDynamicalSystems.hyper_roessler  —  Function hyper_roessler(u0 = [-10.0, -6.0, 0.0, 10.0];\n    a = 0.25,\n    b = 3.0,\n    c = 0.5,\n    d = 0.05) \\[\\begin{aligned}\n\\dot{x} &= -y - z\\\\\n\\dot{y} &= x + a*y + w\\\\\n\\dot{z} &= b + x*z\\\\\n\\dot{w} &= -c*z + d*w\n\\end{aligned}\\] An extension of the Rössler system showchasing hyperchaos [Rossler1979] . An hyperchaotic system is characterized by two positive Lyapunov exponents. source"},{"id":201,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_wang","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_wang","content":" PredefinedDynamicalSystems.hyper_wang  —  Function function hyper_wang(u0 = [5.0, 1.0, 30.0, 1.0];\n    a = 10.0,\n    b = 40.0,\n    c = 2.5,\n    d = 10.6,\n    e = 4.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x)\\\\\n\\dot{y} &= -x*z + b*x + w\\\\\n\\dot{z} &= e*x^2 - c*z\\\\\n\\dot{w} &= -d*x\n\\end{aligned}\\] An extension of the Wang system showchasing hyperchaos [Wang2009] . source"},{"id":202,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.hyper_xu","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.hyper_xu","content":" PredefinedDynamicalSystems.hyper_xu  —  Function function hyper_xu(u0 = [2.0, -1.0, -2.0, -10.0];\n    a = 10.0,\n    b = 40.0,\n    c = 2.5,\n    d = 2.0,\n    e = 16.0) \\[\\begin{aligned}\n\\dot{x} &= a*(y - x) + w\\\\\n\\dot{y} &= b*x + e*x*z\\\\\n\\dot{z} &= - c*z - x*y\\\\\n\\dot{w} &= x*z - d*y\n\\end{aligned}\\] A system showchasing hyperchaos [Letellier2007] . source"},{"id":203,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ikedamap","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ikedamap","content":" PredefinedDynamicalSystems.ikedamap  —  Function ikedamap(u0=[1.0, 1.0]; a=1.0, b=1.0, c=0.4, d =6.0) -> ds \\[\\begin{aligned}\nt &= c - \\frac{d}{1 + x_n^2 + y_n^2} \\\\\nx_{n+1} &= a + b(x_n \\cos(t) - y\\sin(t)) \\\\\ny_{n+1} &= b(x\\sin(t) + y \\cos(t))\n\\end{aligned}\\] The Ikeda map was proposed by Ikeda as a model to explain the propagation of light into a ring cavity  [Skiadas2008] . It generates a variety of nice-looking, interesting attractors. The default parameters are chosen to give a unique chaotic attractor. A double attractor can be obtained with parameters  [a,b,c,d] = [6, 0.9, 3.1, 6] , and a triple attractor can be obtained with  [a,b,c,d] = [6, 9, 2.22, 6] [Skiadas2008] . [Skiadas2008]  : \"Chaotic Modelling and Simulation: Analysis of Chaotic Models, Attractors and Forms\", CRC Press (2008). source"},{"id":204,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.kuramoto","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.kuramoto","content":" PredefinedDynamicalSystems.kuramoto  —  Function kuramoto(D = 20, u0 = range(0, 2π; length = D);\n    K = 0.3, ω = range(-1, 1; length = D)\n) The Kuramoto model [Kuramoto1975]  of  D  coupled oscillators with equation \\[\\dot{\\phi}_i = \\omega_i + \\frac{K}{D}\\sum_{j=1}^{D} \\sin(\\phi_j - \\phi_i)\\] source"},{"id":205,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.logistic","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.logistic","content":" PredefinedDynamicalSystems.logistic  —  Function logistic(x0 = 0.4; r = 4.0) \\[x_{n+1} = rx_n(1-x_n)\\] The logistic map is an one dimensional unimodal mapping due to May [1] and is used by many as the archetypal example of how chaos can arise from very simple equations. Originally intentend to be a discretized model of polulation dynamics, it is now famous for its bifurcation diagram, an immensely complex graph that that was shown be universal by Feigenbaum [2]. The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : R. M. May, Nature  261 , pp 459 (1976) [2] : M. J. Feigenbaum, J. Stat. Phys.  19 , pp 25 (1978) source"},{"id":206,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz","content":" PredefinedDynamicalSystems.lorenz  —  Function lorenz(u0=[0.0, 10.0, 0.0]; σ = 10.0, ρ = 28.0, β = 8/3) -> ds \\[\\begin{aligned}\n\\dot{X} &= \\sigma(Y-X) \\\\\n\\dot{Y} &= -XZ + \\rho X -Y \\\\\n\\dot{Z} &= XY - \\beta Z\n\\end{aligned}\\] The famous three dimensional system due to Lorenz  [Lorenz1963] , shown to exhibit so-called \"deterministic nonperiodic flow\". It was originally invented to study a simplified form of atmospheric convection. Currently, it is most famous for its strange attractor (occuring at the default parameters), which resembles a butterfly. For the same reason it is also associated with the term \"butterfly effect\" (a term which Lorenz himself disliked) even though the effect applies generally to dynamical systems. Default values are the ones used in the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":207,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz84","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz84","content":" PredefinedDynamicalSystems.lorenz84  —  Function lorenz84(u = [0.1, 0.1, 0.1]; F=6.846, G=1.287, a=0.25, b=4.0) Lorenz-84's low order atmospheric general circulation model \\[\\begin{aligned}\n\\dot x = − y^2 − z^2 − ax + aF, \\\\\n\\dot y = xy − y − bxz + G, \\\\\n\\dot z = bxy + xz − z. \\\\\n\\end{aligned}\\] This system has interesting multistability property in the phase space. For the default parameter set we have four coexisting attractors that gives birth to interesting fractalized phase space as shown in  [Freire2008] . One can see this by doing: ds = Systems.lorenz84(rand(3))\nxg = yg = range(-1.0, 2.0; length=300)\nzg = range(-1.5, 1.5; length=30)\nbsn, att = basins_of_attraction((xg, yg, zg), ds; mx_chk_att=4) source"},{"id":208,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz96","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz96","content":" PredefinedDynamicalSystems.lorenz96  —  Function lorenz96(N::Int, u0 = rand(M); F=0.01) \\[\\frac{dx_i}{dt} = (x_{i+1}-x_{i-2})x_{i-1} - x_i + F\\] N  is the chain length,  F  the forcing. Jacobian is created automatically. (parameter container only contains  F ) source"},{"id":209,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenz_bounded","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenz_bounded","content":" PredefinedDynamicalSystems.lorenz_bounded  —  Function lorenz_bounded(u0=[-13.284881, -12.444334, 34.188198];\n    beta = 2.667,\n    r = 64.0,\n    rho = 28.0,\n    sigma = 10.0\n) \\[\\begin{aligned}\n\\dot{X} &= \\sigma(Y-X)f(X,Y,Z) \\\\\n\\dot{Y} &= (-XZ + \\rho X -Y)f(X,Y,Z) \\\\\n\\dot{Z} &= (XY - \\beta Z)f(X,Y,Z)\n\\end{aligned}\\] Lorenz system bounded by a confining potential  [SprottXiong2015] . source"},{"id":210,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lorenzdl","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lorenzdl","content":" PredefinedDynamicalSystems.lorenzdl  —  Function lorenzdl(u = [0.1, 0.1, 0.1]; R=4.7) Diffusionless Lorenz system: it is  probably  the simplest rotationnaly invariant chaotic flow. \\[\\begin{aligned}\n\\dot x = y − x, \\\\\n\\dot y = -xz, \\\\\n\\dot z = xy - R. \\\\\n\\end{aligned}\\] For  R=4.7  this system has two coexisting Malasoma strange attractors that are linked together as shown in  [Sprott2014] . The fractal boundary between the basins of attractor can be visualized with a Poincaré section at  z=0 : ds = Systems.lorenzdl()\nxg = yg = range(-10.0, 10.0; length=300)\npmap = poincaremap(ds, (3, 0.), Tmax=1e6; idxs = 1:2)\nbsn, att = basins_of_attraction((xg, yg), pmap) source"},{"id":211,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.lotkavolterra","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.lotkavolterra","content":" PredefinedDynamicalSystems.lotkavolterra  —  Function lotkavolterra(u0=[10.0, 5.0]; α = 1.5, β = 1, δ=1, γ=3) -> ds \\[\\begin{aligned}\n\\dot{x} &= \\alpha x - \\beta xy, \\\\\n\\dot{y} &= \\delta xy - \\gamma y\n\\end{aligned}\\] The famous Lotka-Volterra model is a simple ecological model describing the interaction between a predator and a prey species (or also parasite and host species). It has been used independently in fields such as epidemics, ecology, and economics  [Hoppensteadt2006] , and is not to be confused with the Competitive Lotka-Volterra model, which describes competitive interactions between species. The  x  variable describes the number of prey, while  y  describes the number of predator. The default parameters are taken from  [Weisstein] , which lead to typical periodic oscillations. source"},{"id":212,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.magnetic_pendulum","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.magnetic_pendulum","content":" PredefinedDynamicalSystems.magnetic_pendulum  —  Function magnetic_pendulum(u=[0.7,0.7,0,0]; d=0.3, α=0.2, ω=0.5, N=3, γs=fill(1.0,N)) Create a pangetic pendulum with  N  magnetics, equally distributed along the unit circle, with dynamical rule \\[\\begin{aligned}\n\\ddot{x} &= -\\omega ^2x - \\alpha \\dot{x} - \\sum_{i=1}^N \\frac{\\gamma_i (x - x_i)}{D_i^3} \\\\\n\\ddot{y} &= -\\omega ^2y - \\alpha \\dot{y} - \\sum_{i=1}^N \\frac{\\gamma_i (y - y_i)}{D_i^3} \\\\\nD_i &= \\sqrt{(x-x_i)^2  + (y-y_i)^2 + d^2}\n\\end{aligned}\\] where α is friction, ω is eigenfrequency, d is distance of pendulum from the magnet's plane and γ is the magnetic strength. source"},{"id":213,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.manneville_simple","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.manneville_simple","content":" PredefinedDynamicalSystems.manneville_simple  —  Function manneville_simple(x0 = 0.4; ε = 1.1) \\[x_{n+1} = [ (1+\\varepsilon)x_n + (1-\\varepsilon)x_n^2 ] \\mod 1\\] A simple 1D map due to Mannevile [Manneville1980]  that is useful in illustrating the concept and properties of intermittency. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":214,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.more_chaos_example","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.more_chaos_example","content":" PredefinedDynamicalSystems.more_chaos_example  —  Function more_chaos_example(u = rand(3)) A three dimensional chaotic system introduced in  [Sprott2020]  with rule \\[\\begin{aligned}\n\\dot{x} &= y \\\\\n\\dot{y} &= -x - \\textrm{sign}(z)y \\\\\n\\dot{z} &= y^2 - \\exp(-x^2)\n\\end{aligned}\\] It is noteworthy because its strange attractor is multifractal with fractal dimension ≈ 3. source"},{"id":215,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.morris_lecar","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.morris_lecar","content":" PredefinedDynamicalSystems.morris_lecar  —  Function morris_lecar(u0=[0.1, 0.1]; I = 0.15, V3 = 0.1, V1 = -0.00, V2 = 0.15, V4 = 0.1,\n    VCa = 1, VL = -0.5, VK = -0.7, gCa = 1.2, gK = 2, gL = 0.5, τ = 3) -> ds The Morris-Lecar model is ubiquitously used in computational neuroscience as a  simplified model for neuronal dynamics  (2D), and can also be in general as an excitable system  [IzhikevichBook] . It uses the formalism of the more complete Hodgkin-Huxley model (4D), and can be viewed as a simplification thereof, with variables V for the membrane potential and N for the recovery of the Potassium current. Its original parameters were obtained from experimental studies of the giant muscle fiber in the Pacific barnacle  [MorrisLecar1981] . Its evolution is given by: \\[\\begin{aligned}\n\\dot{V} &= -g_{Ca} M(V) (V - V_{Ca}) - g_K N (V - V_K) - g_L (V - V_L) + I \\\\\n\\dot{N} &= (-N + G(V)) / \tau \\\\\n\\end{aligned}\\] with \\[\\begin{aligned}\nM(V) = 0.5 (1 + \\tanh((x-V1)/V2)) \\\\\nG(V) = 0.5 (1 + \\tanh((x-V3)/V4)) \\\\\\] source"},{"id":216,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.multispecies_competition","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.multispecies_competition","content":" PredefinedDynamicalSystems.multispecies_competition  —  Function multispecies_competition(option = 1) A model of competition dynamics between multiple species from Huisman and Weissing [Huisman2001] . It highlights fundamental unpredictability by having extreme multistability, fractal basin boundaries and transient chaos. TODO: write here equations when we have access to the paper (not open access). TODO: Describe initial conditions and what option 1 means. source"},{"id":217,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.nld_coupled_logistic_maps","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.nld_coupled_logistic_maps","content":" PredefinedDynamicalSystems.nld_coupled_logistic_maps  —  Function nld_coupled_logistic_maps(D = 4, u0 = range(0, 1; length=D); λ = 1.2, k = 0.08) A high-dimensional discrete dynamical system that couples  D  logistic maps with a strongly nonlinear all-to-all coupling. For the default parameters it displays several co-existing attractors. The equations are: \\[u_i' = \\lambda - u_i^2 + k \\sum_{j\\ne i} (u_j^2 - u_i^2)\\] Here the prime  $'$  denotes next state. source"},{"id":218,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.nosehoover","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.nosehoover","content":" PredefinedDynamicalSystems.nosehoover  —  Function nosehoover(u0 = [0, 0.1, 0]) \\[\\begin{aligned}\n\\dot{x} &= y \\\\\n\\dot{y} &= yz - x \\\\\n\\dot{z} &= 1 - y^2\n\\end{aligned}\\] Three dimensional conservative continuous system, discovered in 1984 during investigations in thermodynamical chemistry by Nosé and Hoover, then rediscovered by Sprott during an exhaustive search as an extremely simple chaotic system.  [Hoover1995] See Chapter 4 of \"Elegant Chaos\" by J. C. Sprott.  [Sprott2010] source"},{"id":219,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.pomeau_manneville","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.pomeau_manneville","content":" PredefinedDynamicalSystems.pomeau_manneville  —  Function pomaeu_manneville(u0 = 0.2; z = 2.5) The Pomeau-Manneville map is a one dimensional discrete map which is characteristic for displaying intermittency [1]. Specifically, for z > 2 the average time between chaotic bursts diverges, while for z > 2.5, the map iterates are long range correlated [2]. Notice that here we are providing the \"symmetric\" version: \\[x_{n+1} = \\begin{cases}\n-4x_n + 3, & \\quad x_n \\in (0.5, 1] \\\\\nx_n(1 + |2x_n|^{z-1}), & \\quad |x_n| \\le 0.5 \\\\\n-4x_n - 3, & \\quad x_n \\in [-1, 0.5)\n\\end{cases}\\] [1] : Manneville & Pomeau, Comm. Math. Phys.  74  (1980) [2] : Meyer et al., New. J. Phys  20  (2019) source"},{"id":220,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.qbh","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.qbh","content":" PredefinedDynamicalSystems.qbh  —  Function qbh([u0]; A=1.0, B=0.55, D=0.4) A conservative dynamical system with rule \\[\\begin{aligned}\n\\dot{q}_0 &= A p_0 \\\\\n\\dot{q}_2 &= A p_2 \\\\\n\\dot{p}_0 &= -A q_0 -3 \\frac{B}{\\sqrt{2}} (q_2^2 - q_0^2) - D q_0 (q_0^2 + q_2^2) \\\\\n\\dot{p}_2 &= -q_2 [A + 3\\sqrt{2} B q_0 + D (q_0^2 + q_2^2)]\n\\end{aligned}\\] This dynamical rule corresponds to a Hamiltonian used in nuclear physics to study the quadrupole vibrations of the nuclear surface  [Eisenberg1975] [Baran1998] . \\[H(p_0, p_2, q_0, q_2) = \\frac{A}{2}\\left(p_0^2+p_2^2\\right)+\\frac{A}{2}\\left(q_0^2+q_2^2\\right)\n\t\t\t +\\frac{B}{\\sqrt{2}}q_0\\left(3q_2^2-q_0^2\\right) +\\frac{D}{4}\\left(q_0^2+q_2^2\\right)^2\\] The Hamiltonian has a similar structure with the Henon-Heiles one, but it has an added fourth order term and presents a nontrivial dependence of chaoticity with the increase of energy [^Micluta-Campeanu2018]. The default initial condition is chaotic. [^Micluta-Campeanu2018]:     Micluta-Campeanu S., Raportaru M.C., Nicolin A.I., Baran V., Rom. Rep. Phys.      70 , pp 105 (2018) source"},{"id":221,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.riddled_basins","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.riddled_basins","content":" PredefinedDynamicalSystems.riddled_basins  —  Function riddled_basins(u0=[0.5, 0.6, 0, 0]; γ=0.05, x̄ = 1.9, f₀=2.3, ω =3.5, x₀=1, y₀=0) → ds \\[\\begin{aligned}\n\\dot{x} &= v_x, \\quad \\dot{y} = v_z \\\\\n\\dot{v}_x &= -\\gamma v_x - [ -4x(1-x^2) +y^2] + f_0 \\sin(\\omega t)x_0 \\\\\n\\dot{v}_y &= -\\gamma v_y - 2y (x+\\bar{x}) + f_0 \\sin(\\omega t)y_0\n\\end{aligned}\\] This 5 dimensional (time-forced) dynamical system was used by Ott et al  [OttRiddled2014]  to analyze  riddled basins of attraction . This means nearby any point of a basin of attraction of an attractor A there is a point of the basin of attraction of another attractor B. source"},{"id":222,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.rikitake","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.rikitake","content":" PredefinedDynamicalSystems.rikitake  —  Function rikitake(u0 = [1, 0, 0.6]; μ = 1.0, α = 1.0) \\[\\begin{aligned}\n\\dot{x} &= -\\mu x +yz \\\\\n\\dot{y} &= -\\mu y +x(z-\\alpha) \\\\\n\\dot{z} &= 1 - xz\n\\end{aligned}\\] Rikitake's dynamo  [Rikitake1958]  is a system that tries to model the magnetic reversal events by means of a double-disk dynamo system. source"},{"id":223,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.roessler","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.roessler","content":" PredefinedDynamicalSystems.roessler  —  Function roessler(u0=[1, -2, 0.1]; a = 0.2, b = 0.2, c = 5.7) \\[\\begin{aligned}\n\\dot{x} &= -y-z \\\\\n\\dot{y} &= x+ay \\\\\n\\dot{z} &= b + z(x-c)\n\\end{aligned}\\] This three-dimensional continuous system is due to Rössler  [Rössler1976] . It is a system that by design behaves similarly to the  lorenz  system and displays a (fractal) strange attractor. However, it is easier to analyze qualitatively, as for example the attractor is composed of a single manifold. Default values are the same as the original paper. The parameter container has the parameters in the same order as stated in this function's documentation string. source"},{"id":224,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.rulkovmap","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.rulkovmap","content":" PredefinedDynamicalSystems.rulkovmap  —  Function rulkovmap(u0=[1.0, 1.0]; α=4.1, β=0.001, σ=0.001) -> ds \\[\\begin{aligned}\nx_{n+1} &= \\frac{\\alpha}{1+x_n^2} + y_n  \\\\\ny_{n+1} &= y_n - \\sigma x_n - \\beta\n\\end{aligned}\\] The Rulkov map is a two-dimensional phenomenological model of a neuron capable of describing spikes and bursts. It was described by Rulkov  [Rulkov2002]  and is used in studies of neural networks due to its computational advantages, being fast to run. The parameters σ and β  are generally kept at  0.001 , while α is chosen to give the desired dynamics. The dynamics can be quiescent for α ∈ (0,2), spiking for α ∈ (2, 2.58), triangular bursting for α ∈ (2.58, 4), and rectangular bursting for α ∈ (4, 4.62)  [Rulkov2001] [Cao2013] . The default parameters are taken from  [Rulkov2001]  to lead to a rectangular bursting. [Rulkov2002]  : \"Modeling of spiking-bursting neural behavior using two-dimensional map\", Phys. Rev. E 65, 041922 (2002). [Rulkov2001]  : \"Regularization of Synchronized Chaotic Bursts\", Phys. Rev. Lett. 86, 183 (2001). [Cao2013]  : H. Cao and Y Wu, \"Bursting types and stable domains of Rulkov neuron network with mean field coupling\", International Journal of Bifurcation and Chaos,23:1330041 (2013). source"},{"id":225,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.sakarya","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.sakarya","content":" PredefinedDynamicalSystems.sakarya  —  Function sakarya(u0= [-2.8976045, 3.8877978, 3.07465];\n    a = 1,\n    b = 1,\n    m = 1\n) \\[\\begin{aligned}\n\\dot{x} &= ax + y + yz\\\\\n\\dot{y} &= - xz + yz \\\\\n\\dot{z} &= - z - mxy + b\n\\end{aligned}\\] A system presenting robust chaos that varies from single wing to double wings to four wings. Its attractor arises due to merging of two disjoint bistable attractors  [Li2015] . source"},{"id":226,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.shinriki","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.shinriki","content":" PredefinedDynamicalSystems.shinriki  —  Function shinriki(u0 = [-2, 0, 0.2]; R1 = 22.0) Shinriki oscillator with all other parameters (besides  R1 ) set to constants.  This is a stiff problem, be careful when choosing solvers and tolerances . source"},{"id":227,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.sprott_dissipative_conservative","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.sprott_dissipative_conservative","content":" PredefinedDynamicalSystems.sprott_dissipative_conservative  —  Function sprott_dissipative_conservative(u0 = [1.0, 0, 0]; a = 2, b = 1, c = 1) An interesting system due to Sprott [Sprott2014b]  where some initial conditios such as  [1.0, 0, 0]  lead to quasi periodic motion on a 2-torus, while for  [2.0, 0, 0]  motion happens on a (dissipative) chaotic attractor. The equations are: \\[\\begin{aligned}\n\\dot{x} &= y + axy + xz \\\\\n\\dot{y} &= 1 - 2x^2 + byz \\\\\n\\dot{z_1} &= cx - x^2 - y^2\n\\end{aligned}\\] In the original paper there were no parameters, which are added here for exploration purposes. source"},{"id":228,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.standardmap","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.standardmap","content":" PredefinedDynamicalSystems.standardmap  —  Function standardmap(u0=[0.001245, 0.00875]; k = 0.971635) \\[\\begin{aligned}\n\\theta_{n+1} &= \\theta_n + p_{n+1} \\\\\np_{n+1} &= p_n + k\\sin(\\theta_n)\n\\end{aligned}\\] The standard map (also known as Chirikov standard map) is a two dimensional, area-preserving chaotic mapping due to Chirikov [1]. It is one of the most studied chaotic systems and by far the most studied Hamiltonian (area-preserving) mapping. The map corresponds to the  Poincaré's surface of section of the kicked rotor system. Changing the non-linearity parameter  k  transitions the system from completely periodic motion, to quasi-periodic, to local chaos (mixed phase-space) and finally to global chaos. The default parameter  k  is the critical parameter where the golden-ratio torus is destroyed, as was calculated by Greene [2]. The e.o.m. considers the angle variable  θ  to be the first, and the angular momentum  p  to be the second, while both variables are always taken modulo 2π (the mapping is on the [0,2π)² torus). The parameter container has the parameters in the same order as stated in this function's documentation string. [1] : B. V. Chirikov, Preprint N.  267 , Institute of Nuclear Physics, Novosibirsk (1969) [2] : J. M. Greene, J. Math. Phys.  20 , pp 1183 (1979) source"},{"id":229,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.stommel_thermohaline","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.stommel_thermohaline","content":" PredefinedDynamicalSystems.stommel_thermohaline  —  Function stommel_thermohaline(u = [0.3, 0.2]; η1 = 3.0, η2 = 1, η3 = 0.3) Stommel's box model for Atlantic thermohaline circulation \\[\\begin{aligned}\n \\dot{T} &= \\eta_1 - T - |T-S| T \\\\\n \\dot{S} &= \\eta_2 - \\eta_3S - |T-S| S\n\\end{aligned}\\] Here  $T, S$  denote the dimensionless temperature and salinity differences respectively between the boxes (polar and equatorial ocean basins) and  $\\eta_i$  are parameters. source"},{"id":230,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.stuartlandau_oscillator","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.stuartlandau_oscillator","content":" PredefinedDynamicalSystems.stuartlandau_oscillator  —  Function stuartlandau_oscillator(u0=[1.0, 0.0]; μ=1.0, ω=1.0, b=1) -> ds The Stuart-Landau model describes a nonlinear oscillation near a Hopf bifurcation, and was proposed by Landau in 1944 to explain the transition to turbulence in a fluid  [Landau1944] . It can be written in cartesian coordinates as  [Deco2017] \\[\\begin{aligned}\n\\dot{x} &= (\\mu -x^2 -y^2)x - \\omega y - b(x^2+y^2)y \\\\\n\\dot{y} &= (\\mu -x^2 -y^2)y + \\omega x + b(x^2+y^2)x\n\\end{aligned}\\] The dynamical analysis of the system is greatly facilitated by putting it in polar coordinates, where it takes the normal form of the supercritical Hopf bifurcation)  [Strogatz2015] . \\[\\begin{aligned}\n\\dot{r} &= \\mu r - r^3, \\\\\n\\dot{\\theta} &= \\omega +br^2\n\\end{aligned}\\] The parameter  \\mu  serves as the bifurcation parameter,  \\omega  is the frequency of infinitesimal oscillations, and  b  controls the dependence of the frequency on the amplitude.  Increasing  \\mu  from negative to positive generates the supercritical Hopf bifurcation, leading from a stable spiral at the origin to a stable limit cycle with radius  \\sqrt(\\mu) . source"},{"id":231,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.swinging_atwood","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.swinging_atwood","content":" PredefinedDynamicalSystems.swinging_atwood  —  Function swinging_atwood(u0=[0.113296,1.5707963267948966,0.10992,0.17747]; m1=1.0, m2=4.5) \\[\\begin{aligned}\n\\dot{r} &= \\frac{p_r}{M+m}\\\\\n\\dot{p}_r &= -Mg + mg\\cos(\\theta)\\\\\n\\dot{\\theta} &= \\frac{p_{\\theta}}{mr^2}\\\\\n\\dot{p}_{\\theta} &= -mgr\\sin(\\theta)\n\\end{aligned}\\] A mechanical system consisting of two swinging weights connected by ropes and pulleys. This is only chaotic when  m2  is sufficiently larger than  m1 , and there are nonzero initial momenta  [Tufillaro1984] . source"},{"id":232,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.tentmap","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.tentmap","content":" PredefinedDynamicalSystems.tentmap  —  Function tentmap(u0 = 0.2; μ=2) -> ds The tent map is a piecewise linear, one-dimensional map that exhibits chaotic behavior in the interval  [0,1] [Ott2002] . Its simplicity allows it to be geometrically interpreted as generating a streching and folding process, necessary for chaos. The equations describing it are: \\[\\begin{aligned}\nx_{n+1} = \\begin{cases} \\mu x, \\quad &x_n < \\frac{1}{2} \\\\\n                         \\mu (1-x), \\quad &\\frac{1}{2} \\leq x_n\n            \\end{cases}\n\\end{aligned}\\] The parameter μ should be kept in the interval  [0,2] . At μ=2, the tent map can be brought to the logistic map with  r=4  by a change of coordinates. [Ott2002]  : E. Ott, \"Chaos in Dynamical Systems\" (2nd ed.) Cambridge: Cambridge University Press (2010). source"},{"id":233,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.thomas_cyclical","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.thomas_cyclical","content":" PredefinedDynamicalSystems.thomas_cyclical  —  Function thomas_cyclical(u0 = [1.0, 0, 0]; b = 0.2) \\[\\begin{aligned}\n\\dot{x} &= \\sin(y) - bx\\\\\n\\dot{y} &= \\sin(z) - by\\\\\n\\dot{z} &= \\sin(x) - bz\n\\end{aligned}\\] Thomas' cyclically symmetric attractor is a 3D strange attractor originally proposed by René Thomas [Thomas1999] . It has a simple form which is cyclically symmetric in the x, y, and z variables and can be viewed as the trajectory of a frictionally dampened particle moving in a 3D lattice of forces. For more see the  Wikipedia page . Reduces to the labyrinth system for  b=0 , see See discussion in Section 4.4.3 of \"Elegant Chaos\" by J. C. Sprott. source"},{"id":234,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.towel","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.towel","content":" PredefinedDynamicalSystems.towel  —  Function towel(u0 = [0.085, -0.121, 0.075]) \\[\\begin{aligned}\nx_{n+1} &= 3.8 x_n (1-x_n) -0.05 (y_n +0.35) (1-2z_n) \\\\\ny_{n+1} &= 0.1 \\left[ \\left( y_n +0.35 \\right)\\left( 1+2z_n\\right) -1 \\right]\n\\left( 1 -1.9 x_n \\right) \\\\\nz_{n+1} &= 3.78 z_n (1-z_n) + b y_n\n\\end{aligned}\\] The folded-towel map is a hyperchaotic mapping due to Rössler [1]. It is famous for being a mapping that has the smallest possible dimensions necessary for hyperchaos, having two positive and one negative Lyapunov exponent. The name comes from the fact that when plotted looks like a folded towel, in every projection. Default values are the ones used in the original paper. [1] : O. E. Rössler, Phys. Lett.  71A , pp 155 (1979) source"},{"id":235,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ueda","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ueda","content":" PredefinedDynamicalSystems.ueda  —  Function ueda(u0 = [3.0, 0]; k = 0.1, B = 12.0) \\[\\ddot{x} + k \\dot{x} + x^3 = B\\cos{t}\\] Nonautonomous Duffing-like forced oscillation system, discovered by Ueda in It is one of the first chaotic systems to be discovered. The stroboscopic plot in the (x, ̇x) plane with period 2π creates a \"broken-egg attractor\" for k = 0.1 and B = 12. Figure 5 of  [Ruelle1980]  is reproduced by using Plots\nds = Systems.ueda()\na = trajectory(ds, 2π*5e3, dt = 2π)\nscatter(a[:, 1], a[:, 2], markersize = 0.5, title=\"Ueda attractor\") For more forced oscillation systems, see Chapter 2 of \"Elegant Chaos\" by J. C. Sprott.  [Sprott2010] source"},{"id":236,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.ulam","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.ulam","content":" PredefinedDynamicalSystems.ulam  —  Function ulam(N = 100, u0 = cos.(1:N); ε = 0.6) A discrete system of  N  unidirectionally coupled maps on a circle, with equations \\[x^{(m)}_{n+1} = f(\\varepsilon x_n^{(m-1)} + (1-\\varepsilon)x_n^{(m)});\\quad f(x) = 2 - x^2\\] source"},{"id":237,"pagetitle":"PredefinedDynamicalSystems.jl","title":"PredefinedDynamicalSystems.vanderpol","ref":"/predefineddynamicalsystems/stable/#PredefinedDynamicalSystems.vanderpol","content":" PredefinedDynamicalSystems.vanderpol  —  Function vanderpol(u0=[0.5, 0.0]; μ=1.5, F=1.2, T=10) -> ds \\[\\begin{aligned}\n\\ddot{x} -\\mu (1-x^2) \\dot{x} + x = F \\cos(2\\pi t / T)\n\\end{aligned}\\] The forced van der Pol oscillator is an oscillator with a nonlinear damping term driven by a sinusoidal forcing. It was proposed by Balthasar van der Pol, in his studies of nonlinear electrical circuits used in the first radios  [Kanamaru2007] [Strogatz2015] . The unforced oscillator ( F = 0 ) has stable oscillations in the form of a limit cycle with a slow buildup followed by a sudden discharge, which van der Pol called relaxation oscillations  [Strogatz2015] [vanderpol1926] . The forced oscillator ( F > 0 ) also has periodic behavior for some parameters, but can additionally have chaotic behavior. The van der Pol oscillator is a specific case of both the FitzHugh-Nagumo neural model  [Kanamaru2007] . The default damping parameter is taken from  [Strogatz2015]  and the forcing parameters are taken from  [Kanamaru2007] , which generate periodic oscillations. Setting  $\\mu=8.53$  generates chaotic oscillations. source Datseris2019 G. Datseris  et al ,  New Journal of Physics 2019 Chua1992 Chua, Leon O. \"The genesis of Chua's circuit\", 1992. Chua2007 Leon O. Chua (2007) \"Chua circuit\", Scholarpedia, 2(10):1488. Gissinger2012 C. Gissinger, Eur. Phys. J. B  85 , 4, pp 1-12 (2012) Grebogi1983 C. Grebogi, S. W. McDonald, E. Ott and J. A. Yorke, Final state sensitivity: An obstruction to predictability, Physics Letters A, 99, 9, 1983 GuckenheimerHolmes1983 Guckenheimer, John, and Philip Holmes (1983). Nonlinear oscillations, dynamical systems, and bifurcations of vector fields. Vol. 42. Springer Science & Business Media. Sprott2010 Sprott, Julien C (2010). Elegant chaos: algebraically simple chaotic flows. World Scientific, 2010. HénonHeiles1964 Hénon, M. & Heiles, C., The Astronomical Journal  69 , pp 73–79 (1964) HindmarshRose1984 J. L. Hindmarsh and R. M. Rose (1984) \"A model of neuronal bursting using three coupled first order differential equations\", Proc. R. Soc. Lond. B 221, 87-102. HodgkinHuxley1952 A. L. Hodgkin, A.F. Huxley J. Physiol., pp. 500-544 (1952). Ermentrout2010 G. Bard Ermentrout, and David H. Terman, \"Mathematical Foundations of Neuroscience\", Springer (2010). Abbott2005 L. F. Abbott, and P. Dayan, \"Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems\", MIT Press (2005). Cai2007 Cai, G., & Huang, J. (2007). A new finance chaotic attractor. International Journal of Nonlinear Science, 3(3), 213-220. Hussain2015 Hussain, I., Gondal, M. A., & Hussain, A. (2015). Construction of dynamical non-linear components based on lorenz system and symmetric group of permutations. 3D Research, 6, 1-6. Wang2008 Wang, X., & Wang, M. (2008). A hyperchaos generated from Lorenz system. Physica A: Statistical Mechanics and its Applications, 387(14), 3751-3758. Chen2006 Chen, A., Lu, J., Lü, J., & Yu, S. (2006). Generating hyperchaotic Lü attractor via state feedback control. Physica A: Statistical Mechanics and its Applications, 364, 103-110. Pang2011 Pang, S., & Liu, Y. (2011). A new hyperchaotic system from the Lü system and its control. Journal of Computational and Applied Mathematics, 235(8), 2775-2789. Qi2008 Qi, G., van Wyk, M. A., van Wyk, B. J., & Chen, G. (2008). On a new hyperchaotic system. Physics Letters A, 372(2), 124-136. Rossler1979 Rossler, O. (1979). An equation for hyperchaos. Physics Letters A, 71(2-3), 155-157. Wang2009 Wang, Z., Sun, Y., van Wyk, B. J., Qi, G., & van Wyk, M. A. (2009). A 3-D four-wing attractor and its analysis. Brazilian Journal of Physics, 39, 547-553. Letellier2007 Letellier, C., & Rossler, O. E. (2007). Hyperchaos. Scholarpedia, 2(8), 1936. Kuramoto1975 Kuramoto, Yoshiki. International Symposium on Mathematical Problems in Theoretical Physics. 39. Lorenz1963 E. N. Lorenz, J. atmos. Sci.  20 , pp 130 (1963) Freire2008 J. G. Freire  et al ,  Multistability, phase diagrams, and intransitivity in the Lorenz-84 low-order atmospheric circulation model, Chaos 18, 033121 (2008) SprottXiong2015 Sprott, J. C., & Xiong, A. (2015). Classifying and quantifying basins of attraction. Chaos: An Interdisciplinary Journal of Nonlinear Science, 25(8), 083101. Sprott2014 J. C. Sprott,  Simplest Chaotic Flows with Involutional Symmetries, Int. Jour. Bifurcation and Chaos 24, 1450009 (2014) Hoppensteadt2006 Frank Hoppensteadt (2006) \"Predator-prey model\", Scholarpedia, 1(10):1563. Weisstein Weisstein, Eric W., \"Lotka-Volterra Equations.\" From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/Lotka-VolterraEquations.html Manneville1980 Manneville, P. (1980). Intermittency, self-similarity and 1/f spectrum in dissipative dynamical systems.  Journal de Physique, 41(11), 1235–1243 Sprott2020 Sprott, J.C. 'Do We Need More Chaos Examples?', Chaos Theory and Applications 2(2),1-3, 2020 IzhikevichBook Izhikevich, E. M., Dynamical systems in neuroscience: The geometry of excitability and bursting, 2007, MIT Press. MorrisLecar1981 Morris, C. and Lecar, H,  Voltage oscillations in the barnacle giant muscle fiber, 1981 . Huisman2001 Huisman & Weissing 2001, Fundamental Unpredictability in Multispecies Competition  The American Naturalist Vol. 157, No. 5. Hoover1995 Hoover, W. G. (1995). Remark on ‘‘Some simple chaotic flows’’.  Physical Review E ,  51 (1), 759. Sprott2010 Sprott, J. C. (2010).  Elegant chaos: algebraically simple chaotic flows . World Scientific. Eisenberg1975 Eisenberg, J.M., & Greiner, W., Nuclear theory 2 rev ed. Netherlands: North-Holland pp 80 (1975) Baran1998 Baran V. and Raduta A. A., International Journal of Modern Physics E,  7 , pp 527–551 (1998) OttRiddled2014 Ott. et al.,  The transition to chaotic attractors with riddled basins Rikitake1958 T. Rikitake Math. Proc. Camb. Phil. Soc.  54 , pp 89–105, (1958) Rössler1976 O. E. Rössler, Phys. Lett.  57A , pp 397 (1976) Li2015 Li, Chunbiao, et al (2015). A novel four-wing strange attractor born in bistability. IEICE Electronics Express 12.4. Sprott2014b J. C. Sprott. Physics Letters A, 378 Stommel1961 Stommel, Thermohaline convection with two stable regimes of flow. Tellus, 13(2) Landau1944 L. D. Landau, \"On the problem of turbulence, In Dokl. Akad. Nauk SSSR (Vol. 44, No. 8, pp. 339-349) (1944). Deco2017 G. Deco et al \"The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core\",  Sci Rep 7, 3095 (2017). Strogatz2015 Steven H. Strogatz \"Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering\", Boulder, CO :Westview Press, a member of the Perseus Books Group (2015). Tufillaro1984 Tufillaro, Nicholas B.; Abbott, Tyler A.; Griffiths, David J. (1984). Swinging Atwood's Machine. American Journal of Physics. 52 (10): 895–903. Thomas1999 Thomas, R. (1999).  International Journal of Bifurcation and Chaos ,  9 (10), 1889-1905. Ruelle1980 Ruelle, David, ‘Strange Attractors’, The Mathematical Intelligencer, 2.3 (1980), 126–37 Sprott2010 Sprott, J. C. (2010).  Elegant chaos: algebraically simple chaotic flows . World Scientific. Kanamaru2007 Takashi Kanamaru (2007) \"Van der Pol oscillator\", Scholarpedia, 2(1):2202. Strogatz2015 Steven H. Strogatz (2015) \"Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering\", Boulder, CO :Westview Press, a member of the Perseus Books Group. vanderpol1926 B. Van der Pol (1926), \"On relaxation-oscillations\", The London, Edinburgh and Dublin Phil. Mag. & J. of Sci., 2(7), 978–992."},{"id":242,"pagetitle":"ChaosTools.jl","title":"ChaosTools.jl","ref":"/chaostools/stable/#ChaosTools.jl","content":" ChaosTools.jl"},{"id":243,"pagetitle":"ChaosTools.jl","title":"ChaosTools","ref":"/chaostools/stable/#ChaosTools","content":" ChaosTools  —  Module ChaosTools.jl A Julia module that offers various tools for analysing nonlinear dynamics and chaotic behaviour. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"ChaosTools\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. ChaosTools.jl is the jack-of-all-trades package of the DynamicalSystems.jl library: methods that are not extensive enough to be a standalone package are added here. You should see the full DynamicalSystems.jl library for other packages that may contain functionality you are looking for but did not find in ChaosTools.jl. source Accompanying textbook A good background for understanding the methods of ChaosTools.jl is the following textbook:  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022."},{"id":244,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.jl reference","ref":"/chaostools/stable/#DynamicalSystemsBase.jl-reference","content":" DynamicalSystemsBase.jl reference As many docstrings in ChaosTools.jl point to the different  DynamicalSystem  types, they are also provided here for reference. DynamicalSystem DeterministicIteratedMap CoupledODEs CoreDynamicalSystem StroboscopicMap PoincareMap TangentDynamicalSystem ParallelDynamicalSystem ProjectedDynamicalSystem reinit!"},{"id":245,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/chaostools/stable/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description Note The documentation of  DynamicalSystem  follows chapter 1 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. A  ds::DynamicalSystem representes a flow Φ in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is a standard Julia function, see below. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f  defined as a standard Julia function.  Observed  or  measured  data from a dynamical system are represented using  StateSpaceSet  and are finite. Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. Construction instructions on  f  and  u Most of the concrete implementations of  DynamicalSystem , with the exception of  ArbitrarySteppable , have two ways of implementing the dynamic rule  f , and as a consequence the type of the state  u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its  effective dimensionality  is  dimension(ds)+1 . API The API that the interface of  DynamicalSystem  employs is the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can quieried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state current_parameters initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace succesful_step API - alter status reinit! set_state! set_parameter! set_parameters!"},{"id":246,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/chaostools/stable/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem ."},{"id":247,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.CoupledODEs","ref":"/chaostools/stable/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . DifferentialEquations.jl keyword arguments and interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false)), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  , however the majority of downstream functions in DynamicalSystems.jl assume that  f  is differentiable. The convenience constructor  CoupledODEs(prob::ODEProblem, diffeq)  and  CoupledODEs(ds::CoupledODEs, diffeq)  are also available. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available."},{"id":248,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.CoreDynamicalSystem","ref":"/chaostools/stable/#DynamicalSystemsBase.CoreDynamicalSystem","content":" DynamicalSystemsBase.CoreDynamicalSystem  —  Type CoreDynamicalSystem Union type meaning either  DeterministicIteratedMap  or  CoupledODEs , which are the core systems whose dynamic rule  f  is known analytically. This type is used for deciding whether a creation of a  TangentDynamicalSystem  is possible or not."},{"id":249,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/chaostools/stable/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap ."},{"id":250,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.PoincareMap","ref":"/chaostools/stable/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap)"},{"id":251,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.TangentDynamicalSystem","ref":"/chaostools/stable/#DynamicalSystemsBase.TangentDynamicalSystem","content":" DynamicalSystemsBase.TangentDynamicalSystem  —  Type TangentDynamicalSystem <: DynamicalSystem\nTangentDynamicalSystem(ds::CoreDynamicalSystem; kwargs...) A dynamical system that bundles the evolution of  ds  (which must be an  CoreDynamicalSystem ) and  k  deviation vectors that are evolved according to the  dynamics in the tangent space  (also called linearized dynamics or the tangent dynamics). The state of  ds must  be an  AbstractVector  for  TangentDynamicalSystem . TangentDynamicalSystem  follows the  DynamicalSystem  interface with the following adjustments: reinit!  takes an additional keyword  Q0  (with same default as below) The additional functions  current_deviations  and  set_deviations!  are provided for the deviation vectors. Keyword arguments k  or  Q0 :  Q0  represents the initial deviation vectors (each column = 1 vector). If  k::Int  is given, a matrix  Q0  is created with the first  k  columns of the identity matrix. Otherwise  Q0  can be given directly as a matrix. It must hold that  size(Q, 1) == dimension(ds) . You can use  orthonormal  for random orthonormal vectors. By default  k = dimension(ds)  is used. u0 = current_state(ds) : Starting state. J  and  J0 : See section \"Jacobian\" below. Description Let  $u$  be the state of  ds , and  $y$  a deviation (or perturbation) vector. These two are evolved in parallel according to \\[\\begin{array}{rcl}\n\\frac{d\\vec{x}}{dt} &=& f(\\vec{x}) \\\\\n\\frac{dY}{dt} &=& J_f(\\vec{x}) \\cdot Y\n\\end{array}\n\\quad \\mathrm{or}\\quad\n\\begin{array}{rcl}\n\\vec{x}_{n+1} &=& f(\\vec{x}_n) \\\\\nY_{n+1} &=& J_f(\\vec{x}_n) \\cdot Y_n.\n\\end{array}\\] for continuous or discrete time respectively. Here  $f$  is the  dynamic_rule (ds)  and  $J_f$  is the Jacobian of  $f$ . Jacobian The keyword  J  provides the Jacobian function. It must be a Julia function in the same form as  f , the  dynamic_rule . Specifically,  J(u, p, n) -> M::SMatrix  for the out-of-place version or  J(M, u, p, n)  for the in-place version acting in-place on  M . in both cases  M  is a matrix whose columns are the deviation vectors. By default  J = nothing .  In this case  J  is constructed automatically using the module  ForwardDiff , hence its limitations also apply here. Even though  ForwardDiff  is very fast, depending on your exact system you might gain significant speed-up by providing a hand-coded Jacobian and so it is recommended. Additionally, automatic and in-place Jacobians cannot be time dependent. The keyword  J0  allows you to pass an initialized Jacobian matrix  J0 . This is useful for large in-place systems where only a few components of the Jacobian change during the time evolution.  J0  can be a sparse or any other matrix type. If not given, a matrix of zeros is used.  J0  is ignored for out of place systems."},{"id":252,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.ParallelDynamicalSystem","ref":"/chaostools/stable/#DynamicalSystemsBase.ParallelDynamicalSystem","content":" DynamicalSystemsBase.ParallelDynamicalSystem  —  Type ParallelDynamicalSystem <: DynamicalSystem\nParallelDynamicalSystem(ds::DynamicalSystem, states::Vector{<:AbstractArray}) A struct that evolves several  states  of a given dynamical system in parallel  at exactly the same times . Useful when wanting to evolve several different trajectories of the same system while ensuring that they share parameters and time vector. This struct follows the  DynamicalSystem  interface with the following adjustments: The function  current_state  is called as  current_state(pds, i::Int = 1)  which returns the  i th state. Same for  initial_state . Similarly,  set_state!  obtains a third argument  i::Int = 1  to set the  i -th state. current_states  and  initial_states  can be used to get all parallel states. reinit!  takes in a vector of states (like  states ) for  u ."},{"id":253,"pagetitle":"ChaosTools.jl","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/chaostools/stable/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\npds = # same as in above example..."},{"id":254,"pagetitle":"ChaosTools.jl","title":"SciMLBase.reinit!","ref":"/chaostools/stable/#SciMLBase.reinit!-Tuple{DynamicalSystem, Vararg{Any}}","content":" SciMLBase.reinit!  —  Method reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds Reset the status of  ds , so that it is as if it has be just initialized with initial state  u . Practically every function of the ecosystem that evolves  ds  first calls this function on it. Besides the new initial state  u , you can also configure the keywords  t0 = initial_time(ds)  and  p = current_parameters(ds) . Note the default settings: the state and time are the initial, but the parameters are the current. The special method  reinit!(ds, ::Nothing; kwargs...)  is also available, which does nothing and leaves the system as is. This is so that downstream functions that call  reinit!  can still be used without resetting the system but rather continuing from its exact current state. DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":257,"pagetitle":"Detecting & Categorizing Chaos","title":"Detecting & Categorizing Chaos","ref":"/chaostools/stable/chaos_detection/#Detecting-and-Categorizing-Chaos","content":" Detecting & Categorizing Chaos Being able to detect and distinguish chaotic from regular behavior is crucial in the study of dynamical systems. Most of the time a positive maximum  lyapunov  exponent and a bounded system indicate chaos. However, the convergence of the Lyapunov exponent can be slow, or even misleading, as the types of chaotic behavior vary with respect to their predictability. There are some alternatives, some more efficient and some more accurate in characterizing chaotic and regular motion."},{"id":258,"pagetitle":"Detecting & Categorizing Chaos","title":"Generalized Alignment Index","ref":"/chaostools/stable/chaos_detection/#Generalized-Alignment-Index","content":" Generalized Alignment Index \"GALI\" for sort, is a method that relies on the fact that initially orthogonal deviation vectors tend to align towards the direction of the maximum Lyapunov exponent for chaotic motion. It is one of the most recent and cheapest methods for distinguishing chaotic and regular behavior, introduced first in 2007 by Skokos, Bountis & Antonopoulos."},{"id":259,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.gali","ref":"/chaostools/stable/chaos_detection/#ChaosTools.gali","content":" ChaosTools.gali  —  Function gali(ds::DynamicalSystem, T, k::Int; kwargs...) -> GALI_k, t Compute  $\\text{GALI}_k$ [Skokos2007]  for a given  k  up to time  T . Return  $\\text{GALI}_k(t)$  and time vector  $t$ . The third argument sets the order of  gali .  gali  function simply initializes a  TangentDynamicalSystem  with  k  deviation vectors and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a  TangentDynamicalSystem  if you have a hand-coded Jacobian. Keyword arguments threshold = 1e-12 : If  GALI_k  falls below the  threshold  iteration is terminated. Δt = 1 : Time-step between deviation vector normalizations. For continuous systems this is approximate. u0 : Initial state for the system. Defaults to  current_state(ds) . Description The Generalized Alignment Index,  $\\text{GALI}_k$ , is an efficient (and very fast) indicator of chaotic or regular behavior type in  $D$ -dimensional Hamiltonian systems ( $D$  is number of variables). The  asymptotic  behavior of  $\\text{GALI}_k(t)$  depends critically on the type of orbit resulting from the initial condition. If it is a chaotic orbit, then \\[\\text{GALI}_k(t) \\sim\n\\exp\\left[\\sum_{j=1}^k (\\lambda_1 - \\lambda_j)t \\right]\\] with  $\\lambda_j$  being the  j -th Lyapunov exponent (see  lyapunov ,  lyapunovspectrum ). If on the other hand the orbit is regular, corresponding to movement in  $d$ -dimensional torus with  $1 \\le d \\le D/2$  then it holds \\[\\text{GALI}_k(t) \\sim\n    \\begin{cases}\n      \\text{const.}, & \\text{if} \\;\\; 2 \\le k \\le d  \\; \\; \\text{and}\n      \\; \\;d > 1 \\\\\n      t^{-(k - d)}, & \\text{if} \\;\\;  d < k \\le D - d \\\\\n      t^{-(2k - D)}, & \\text{if} \\;\\;  D - d < k \\le D\n    \\end{cases}\\] Traditionally, if  $\\text{GALI}_k(t)$  does not become less than the  threshold  until  T  the given orbit is said to be chaotic, otherwise it is regular. Our implementation is not based on the original paper, but rather in the method described in [Skokos2016b] , which uses the product of the singular values of  $A$ , a matrix that has as  columns  the deviation vectors. source gali(tands::TangentDynamicalSystem, T; threshold = 1e-12, Δt = 1) The low-level method that is called by  gali(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  tands . The order of  $\\text{GALI}_k$  computed is the amount of deviation vectors in  tands . Also use this method if you have a hand-coded Jacobian to pass when creating  tands . source"},{"id":260,"pagetitle":"Detecting & Categorizing Chaos","title":"GALI example","ref":"/chaostools/stable/chaos_detection/#GALI-example","content":" GALI example As an example let's use the Henon-Heiles system using ChaosTools, CairoMakie\nusing OrdinaryDiffEq: Vern9\n\nfunction henonheiles_rule(u, p, t)\n    SVector(u[3], u[4],\n        -u[1] - 2u[1]*u[2],\n        -u[2] - (u[1]^2 - u[2]^2),\n    )\nend\nfunction henonheiles_jacob(u, p, t)\n    SMatrix{4,4}(0, 0, -1 - 2u[2], -2u[1], 0, 0,\n     -2u[1], -1 + 2u[2], 1, 0, 0, 0, 0, 1, 0, 0)\nend\n\nu0=[0, -0.25, 0.42081, 0]\nΔt = 1.0\ndiffeq = (abstol=1e-9, retol=1e-9, alg = Vern9(), maxiters = typemax(Int))\nsp = [0, .295456, .407308431, 0] # stable periodic orbit: 1D torus\nqp = [0, .483000, .278980390, 0] # quasiperiodic orbit: 2D torus\nch = [0, -0.25, 0.42081, 0]      # chaotic orbit\nds = CoupledODEs(henonheiles_rule, sp) 4-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  henonheiles_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    SciMLBase.NullParameters()\n time:          0.0\n state:         [0.0, 0.295456, 0.407308431, 0.0]\n Let's see what happens with a quasi-periodic orbit: tr = trajectory(ds, 10000.0, qp; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"qp\", markersize=2)\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nfor k in [2,3,4]\n    g, t = gali(ds, 10000.0, k; u0 = qp, Δt)\n    logt = log.(t)\n    lines!(ax, logt, g; label=\"GALI_$(k)\")\n    if k == 2\n        lines!(ax, logt, 1 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    else\n        lines!(ax, logt, 100 ./ t.^(2k-4); label=\"slope -$(2k-4)\")\n    end\nend\nylims!(ax, 1e-12, 2)\nfig And here is GALI of a continuous system with a chaotic orbit tr = trajectory(ds, 10000.0, ch; Δt)[1]\nfig, ax = scatter(tr[:,1], tr[:,3]; label=\"ch\", markersize=2, color = (Main.COLORS[1], 0.5))\naxislegend(ax)\n\nax = Axis(fig[1,2]; yscale = log)\nls = lyapunovspectrum(ds, 5000; Δt, u0 = ch)\nfor k in [2,3,4]\n    ex = sum(ls[1] - ls[j] for j in 2:k)\n    g, t = gali(ds, 1000, k; u0 = ch, Δt)\n    lines!(t, exp.(-ex.*t); label=\"exp. k=$k\")\n    lines!(t, g; label=\"GALI_$(k)\")\nend\nylims!(ax, 1e-16, 1)\nfig"},{"id":261,"pagetitle":"Detecting & Categorizing Chaos","title":"Using GALI","ref":"/chaostools/stable/chaos_detection/#Using-GALI","content":" Using GALI No-one in their right mind would try to fit power-laws in order to distinguish between chaotic and regular behavior, like the above examples. These were just proofs that the method works as expected. The most common usage of  $\\text{GALI}_k$  is to define a (sufficiently) small amount of time and a (sufficiently) small threshold and see whether  $\\text{GALI}_k$  stays below it, for a (sufficiently) big  $k$ . For example, we utilize parallel integration of  TangentDynamicalSystem  to compute  $GALI$  for many initial conditions and produce a color-coded map of regular and chaotic orbits of the standard map. The following is an example of advanced usage: using ChaosTools, CairoMakie\n# Initialize `TangentDynamicalSystem`\n@inbounds function standardmap_rule(x, par, n)\n    theta = x[1]; p = x[2]\n    p += par[1]*sin(theta)\n    theta += p\n    return mod2pi.(SVector(theta, p))\nend\n@inbounds standardmap_jacob(x, p, n) = SMatrix{2,2}(\n    1 + p[1]*cos(x[1]), p[1]*cos(x[1]), 1, 1\n)\nds = DeterministicIteratedMap(standardmap_rule, ones(2), [1.0])\ntands = TangentDynamicalSystem(ds; J = standardmap_jacob)\n# Collect initial conditions\ndens = 101\nθs = ps = range(0, stop = 2π, length = dens)\nics = vec(SVector{2, Float64}.(Iterators.product(θs, ps)))\n# Initialize as many systems as threads\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n# Perform threaded loop\nregularity = zeros(size(ics))\nThreads.@threads for i in eachindex(ics)\n    u0 = ics[i]\n    system = systems[Threads.threadid()]\n    reinit!(system, u0)\n    regularity[i] = gali(system, 500)[2][end]\nend\n# Visualize\nfig, ax, sc = scatter(ics; color = regularity)\nColorbar(fig[1,2], sc; label = \"regularity\")\nfig"},{"id":262,"pagetitle":"Detecting & Categorizing Chaos","title":"Predictability of a chaotic system","ref":"/chaostools/stable/chaos_detection/#Predictability-of-a-chaotic-system","content":" Predictability of a chaotic system Even if a system is \"formally\" chaotic, it can still be in phases where it is partially predictable, because the correlation coefficient between nearby trajectories vanishes very slowly with time.  Wernecke, Sándor & Gros  have developed an algorithm that allows one to classify a dynamical system to one of three categories: strongly chaotic, partially predictable chaos or regular (called  laminar  in their paper). We have implemented their algorithm in the function  predictability . Note that we set up the implementation to always return regular behavior for negative Lyapunov exponent. You may want to override this for research purposes."},{"id":263,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.predictability","ref":"/chaostools/stable/chaos_detection/#ChaosTools.predictability","content":" ChaosTools.predictability  —  Function predictability(ds::CoreDynamicalSystem; kwargs...) -> chaos_type, ν, C Determine whether  ds  displays strongly chaotic, partially-predictable chaotic or regular behaviour, using the method by Wernecke et al. described in [Wernecke2017] . Return the type of the behavior, the cross-distance scaling coefficient  ν  and the correlation coefficient  C . Typical values for  ν ,  C  and  chaos_type  are given in Table 2 of [Wernecke2017] : chaos_type ν C :SC 0 0 :PPC 0 1 :REG 1 1 If none of these conditions apply, the return value is  :IND  (for indeterminate). Keyword arguments Ttr = 200 : Extra transient time to evolve the system before sampling from  the trajectory. Should be  Int  for discrete systems. T_sample = 1e4 : Time to evolve the system for taking samples. Should be  Int  for discrete systems. n_samples = 500 : Number of samples to take for use in calculating statistics. λ_max = lyapunov(ds, 5000) : Value to use for largest Lyapunov exponent for finding the Lyapunov prediction time. If it is less than zero a regular result is returned immediately. d_tol = 1e-3 : tolerance distance to use for calculating Lyapunov prediction time. T_multiplier = 10 : Multiplier from the Lyapunov prediction time to the evaluation time. T_max = Inf : Maximum time at which to evaluate trajectory distance. If the internally  computed evaluation time is larger than  T_max , stop at  T_max  instead.   It is strongly recommended to manually set this! δ_range = 10.0 .^ (-9:-6) : Range of initial condition perturbation distances  to use to determine scaling  ν . ν_threshold = C_threshold = 0.5 : Thresholds for scaling coefficients (they become 0 or 1 if they are less or more than the threshold). Description The algorithm samples points from a trajectory of the system to be used as initial conditions. Each of these initial conditions is randomly perturbed by a distance  δ , and the trajectories for both the original and perturbed initial conditions are evolved up to the 'evaluation time'  T  (see below its definition). The average (over the samples) distance and cross-correlation coefficient of the state at time  T  is computed. This is repeated for a range of  δ  (defined by  δ_range ), and linear regression is used to determine how the distance and cross-correlation scale with  δ , allowing for identification of chaos type. The evaluation time  T  is calculated as  T = T_multiplier*Tλ , where the Lyapunov prediction time  Tλ = log(d_tol/δ)/λ_max . This may be very large if the  λ_max  is small, e.g. when the system is regular, so this internally computed time  T  can be overridden by a smaller  T_max  set by the user. Performance Notes For continuous systems, it is likely that the  maxiters  used by the integrators needs to be increased, e.g. to 1e9. This is part of the  diffeq  kwargs. In addition, be aware that this function does a  lot  of internal computations. It is operating in a different speed than e.g.  lyapunov . source"},{"id":264,"pagetitle":"Detecting & Categorizing Chaos","title":"Example Hénon Map","ref":"/chaostools/stable/chaos_detection/#Example-Hénon-Map","content":" Example Hénon Map We will create something similar to figure 2 of the paper, but for the Hénon map. fig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"x\")\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nas = 0.8:0.01:1.225\nod = orbitdiagram(he, 1, 1, as; n = 2000, Ttr = 2000)\ncolors = Dict(:REG => \"blue\", :PPC => \"green\", :SC => \"red\")\nfor (i, a) in enumerate(as)\n    set_parameter!(he, 1, a)\n    chaos_type, ν, C = predictability(he; T_max = 400000, Ttr = 2000)\n    scatter!(ax, a .* ones(length(od[i])), od[i];\n    color = (colors[chaos_type], 0.05), markersize = 2)\nend\nax.title = \"predictability of Hénon map\"\nfig"},{"id":265,"pagetitle":"Detecting & Categorizing Chaos","title":"The 0-1 test for chaos","ref":"/chaostools/stable/chaos_detection/#The-0-1-test-for-chaos","content":" The 0-1 test for chaos The methods mentioned in this page so far require a  DynamicalSystem  instance. But of course this is not always the case. The so-called \"0 to 1\" test for chaos, by Gottwald & Melbourne, takes as an input a timeseries and outputs a boolean  true  if the timeseries is chaotic or  false  if it is not. Notice that the method does have a lot of caveats, so you should read the review paper before using. Also, it doesn't work for noisy data."},{"id":266,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.testchaos01","ref":"/chaostools/stable/chaos_detection/#ChaosTools.testchaos01","content":" ChaosTools.testchaos01  —  Function testchaos01(x::Vector [, cs, N0]) -> chaotic? Perform the so called \"0-1\" test for chaos introduced by Gottwald and Melbourne [Gottwald2016]  on the timeseries  x . Return  true  if  x  is chaotic,  false  otherwise. Description This method tests if the given timeseries is chaotic or not by transforming it into a two-dimensional diffusive process like so: \\[p_n = \\sum_{j=1}^{n}\\phi_j \\cos(j c),\\quad q_n = \\sum_{j=1}^{n}\\phi_j \\sin(j c)\\] If the timeseries is chaotic, the mean square displacement of the process grows as  sqrt(length(x)) , while it stays constant if the timeseries is regular. The implementation here computes  K , a coefficient measuring the growth of the mean square displacement, and simply checks if  K > 0.5 .  K  is the median of  $K_c$  over given  c , see the reference. If you want to access the various  Kc  you should call the method  testchaos01(x, c::Real, N0)  which returns  Kc . In fact, the high level method is just  median(testchaos01(x, c, N0) for c in cs) > 0.5 . cs  defaults to  3π/5*rand(100) + π/4  and  N0 , the length of the two-dimensional process, is  N0 = length(x)/10 . For data sampled from continuous dynamical systems, some care must be taken regarding the values of  cs . Also note that this method performs rather poorly with even the slight amount of noise, returning  true  for even small amounts of noise noisy timeseries. Some possibilities to alleviate this exist, but are context specific on the application. See  [Gottwald2016]  for more info. source"},{"id":267,"pagetitle":"Detecting & Categorizing Chaos","title":"Expansion entropy","ref":"/chaostools/stable/chaos_detection/#Expansion-entropy","content":" Expansion entropy The expansion entropy is a quantity that is suggested by B. Hunt and E. Ott as a measure that can define chaos (so far no widely accepted definition of chaos exists). Positive expansion entropy means chaos."},{"id":268,"pagetitle":"Detecting & Categorizing Chaos","title":"ChaosTools.expansionentropy","ref":"/chaostools/stable/chaos_detection/#ChaosTools.expansionentropy","content":" ChaosTools.expansionentropy  —  Function expansionentropy(ds::DynamicalSystem, sampler, isinside; kwargs...) Calculate the expansion entropy [Hunt2015]  of  ds , in the restraining region  $S$  by estimating the slope (via linear regression) of the curve  $\\log E_{t0+T, t0}(f, S)$  versus  $T$  (using  linear_region ). This is an approximation of the expansion entropy  $H_0$ , according to [Hunt2015] . Return  $T$ ,   $\\log E$  and the calculated slope. sampler  is a 0-argument function that generates a random initial conditions of  ds  and  isinside  is a 1-argument function that given a state it returns true if the state is inside the restraining region. Typically  sampler, isinside  are the output of  statespace_sampler . Keyword arguments N = 1000 : Number of samples taken at each batch (same as  $N$  of  [Hunt2015] ). steps = 40 : The maximal steps for which the system will be run. batches = 100 : Number of batches to run the calculation, see below. Δt = 1 : Time evolution step size. J = nothing : Jacobian function given to  TangentDynamicalSystem . Description N  samples are initialized and propagated forwards in time (along with their tangent space). At every time  $t$  in  [t0+Δt, t0+2Δt, ..., t0+steps*Δt]  we calculate  $H$ : \\[H[t] = \\log E_{t0+T, t0}(f, S),\\] with \\[E_{t0+T, t0}(f, S) = \\frac 1 N \\sum_{i'} G(Df_{t0+t, t0}(x_i))\\] (using same notation as  [Hunt2015] ). In principle  $E$  is the average largest possible growth ratio within the restraining region (sampled by the initial conditions). The summation is only over  $x_i$  that stay inside the region  $S$  defined by the boolean function  isinside . This process is done by the  ChaosTools.expansionentropy_sample  function. Then, this is repeated for  batches  amount of times, as recommended in [Hunt2015] . From all these batches, the mean and std of  $H$  is computed at every time point. This is done by the  expansionentropy_batch  function. When plotted versus  $t$ , these create the curves and error bars of e.g. Figs 2, 3 of [1]. This function  expansionentropy  simply returns the slope of the biggest linear region of the curve  $H$  versus  $t$ , which approximates the expansion entropy  $H_0$ . It is therefore  recommended  to use  expansionentropy_batch  directly and evaluate the result yourself, as this step is known to be inaccurate for non-chaotic systems (where  $H$  fluctuates strongly around 0). source Skokos2007 Skokos, C. H.  et al. , Physica D  231 , pp 30–54 (2007) Skokos2016b Skokos, C. H.  et al. ,  Chaos Detection and Predictability  - Chapter 5 (section 5.3.1 and ref. [85] therein), Lecture Notes in Physics  915 , Springer (2016) Wernecke2017 Wernecke, H., Sándor, B. & Gros, C.  How to test for partially predictable chaos .  Scientific Reports  7 , (2017) . Gottwald2016 Gottwald & Melbourne, “The 0-1 test for chaos: A review”  Lect. Notes Phys., vol. 915, pp. 221–247, 2016. Hunt2015 Hunt & Ott, ‘Defining Chaos’,  Chaos 25.9 (2015)"},{"id":271,"pagetitle":"Dimensionality reduction","title":"Dimensionality reduction","ref":"/chaostools/stable/dimreduction/#Dimensionality-reduction","content":" Dimensionality reduction"},{"id":272,"pagetitle":"Dimensionality reduction","title":"Broomhead-King Coordinates","ref":"/chaostools/stable/dimreduction/#Broomhead-King-Coordinates","content":" Broomhead-King Coordinates"},{"id":273,"pagetitle":"Dimensionality reduction","title":"ChaosTools.broomhead_king","ref":"/chaostools/stable/dimreduction/#ChaosTools.broomhead_king","content":" ChaosTools.broomhead_king  —  Function broomhead_king(s::AbstractVector, d::Int) -> U, S, Vtr Return the Broomhead-King coordinates of a timeseries  s  by performing  svd  on high-dimensional delay embedding if  s  with dimension  d  with minimum delay. Description Broomhead and King coordinates is an approach proposed in  [Broomhead1987]  that applies the Karhunen–Loève theorem to delay coordinates embedding with smallest possible delay. The function performs singular value decomposition on the  d -dimensional matrix  $X$  of  $s$ , \\[X = \\frac{1}{\\sqrt{N}}\\left(\n\\begin{array}{cccc}\nx_1 & x_2 & \\ldots & x_d \\\\\nx_2 & x_3 & \\ldots & x_{d+1}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nx_{N-d+1} & x_{N-d+2} &\\ldots & x_N\n\\end{array}\n\\right) = U\\cdot S \\cdot V^{tr}.\\] where  $x := s - \\bar{s}$ . The columns of  $U$  can then be used as a new coordinate system, and by considering the values of the singular values  $S$  you can decide how many columns of  $U$  are \"important\". source This alternative/improvement of the traditional delay coordinates can be a very powerful tool. An example where it shines is noisy data where there is the effect of superficial dimensions due to noise. Take the following example where we produce noisy data from a system and then use Broomhead-King coordinates as an alternative to \"vanilla\" delay coordinates: using ChaosTools, CairoMakie\n\nfunction gissinger_rule(u, p, t)\n    μ, ν, Γ = p\n    du1 = μ*u[1] - u[2]*u[3]\n    du2 = -ν*u[2] + u[1]*u[3]\n    du3 = Γ - u[3] + u[1]*u[2]\n    return SVector{3}(du1, du2, du3)\nend\n\ngissinger = CoupledODEs(gissinger_rule, ones(3), [0.112, 0.1, 0.9])\nX, t = trajectory(gissinger, 1000.0; Ttr = 100, Δt = 0.1)\nx = X[:, 1]\n\nL = length(x)\ns = x .+ 0.5rand(L) #add noise\n\nU, S = broomhead_king(s, 20)\nsummary(U) \"9982×20 Matrix{Float64}\" Now let's simply compare the above result with the one you get from doing a standard delay coordinates embedding using DelayEmbeddings: embed, estimate_delay\n\nfig = Figure()\naxs = [Axis3(fig[1, i]) for i in 1:2]\nlines!(axs[1], U[:, 1], U[:, 2], U[:, 3])\naxs[1].title = \"Broomhead-King of s\"\n\nR = embed(s, 3, estimate_delay(x, \"mi_min\"))\nlines!(axs[2], columns(R)...)\naxs[2].title = \"2D embedding of s\"\nfig we have used the same system as in the  Delay Coordinates Embedding  example, and picked the optimal delay time of  τ = 30  (for same  Δt = 0.05 ). Regardless, the vanilla delay coordinates is much worse than the Broomhead-King coordinates."},{"id":274,"pagetitle":"Dimensionality reduction","title":"DyCA - Dynamical Component Analysis","ref":"/chaostools/stable/dimreduction/#DyCA-Dynamical-Component-Analysis","content":" DyCA - Dynamical Component Analysis"},{"id":275,"pagetitle":"Dimensionality reduction","title":"ChaosTools.dyca","ref":"/chaostools/stable/dimreduction/#ChaosTools.dyca","content":" ChaosTools.dyca  —  Function dyca(data, eig_threshold) -> eigenvalues, proj_mat, projected_data Compute the Dynamical Component analysis (DyCA) of the given  data [Uhl2018]  used for dimensionality reduction. Return the eigenvalues, projection matrix, and reduced-dimension data (which are just  data*proj_mat ). Keyword Arguments norm_eigenvectors=false : if true, normalize the eigenvectors Description Dynamical Component Analysis (DyCA) is a method to detect projection vectors to reduce the dimensionality of multi-variate, high-dimensional deterministic datasets. Unlike methods like PCA or ICA that make a stochasticity assumption, DyCA relies on a determinacy assumption on the time-series and is based on the solution of a generalized eigenvalue problem. After choosing an appropriate eigenvalue threshold and solving the eigenvalue problem, the obtained eigenvectors are used to project the high-dimensional dataset onto a lower dimension. The obtained eigenvalues measure the quality of the assumption of linear determinism for the investigated data. Furthermore, the number of the generalized eigenvalues with a value of approximately 1.0 are a measure of the number of linear equations contained in the dataset. This property is useful in detecting regions with highly deterministic parts in the time-series and also as a preprocessing step for reservoir computing of high-dimensional spatio-temporal data. The generalised eigenvalue problem we solve is: \\[C_1 C_0^{-1} C_1^{\\top} \\bar{u} = \\lambda C_2 \\bar{u}\n\\] where  $C_0$  is the correlation matrix of the data with itself,  $C_1$  the correlation matrix of the data with its derivative, and  $C_2$  the correlation matrix of the derivative of the data with itself. The eigenvectors  $\\bar{u}$  with eigenvalues approximately 1 and their  $C_1^{-1} C_2 u$  counterpart, form the space where the data is projected onto. source Broomhead1987 Broomhead, Jones, King, J. Phys. A  20 , 9, pp L563 (1987) Uhl2018 B Seifert, K Korn, S Hartmann, C Uhl,  Dynamical Component Analysis (DYCA): Dimensionality Reduction for High-Dimensional Deterministic Time-Series , 10.1109/mlsp.2018.8517024, 2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)"},{"id":278,"pagetitle":"Lyapunov Exponents","title":"Lyapunov Exponents","ref":"/chaostools/stable/lyapunovs/#Lyapunov-Exponents","content":" Lyapunov Exponents Lyapunov exponents measure exponential rates of separation of nearby trajectories in the flow of a dynamical system. The concept of these exponents is best explained in Chapter 3 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. The explanations of the chapter directly utilize the code of the functions in this page."},{"id":279,"pagetitle":"Lyapunov Exponents","title":"Lyapunov Spectrum","ref":"/chaostools/stable/lyapunovs/#Lyapunov-Spectrum","content":" Lyapunov Spectrum The function  lyapunovspectrum  calculates the entire spectrum of the Lyapunov exponents of a system:"},{"id":280,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunovspectrum","ref":"/chaostools/stable/lyapunovs/#ChaosTools.lyapunovspectrum","content":" ChaosTools.lyapunovspectrum  —  Function lyapunovspectrum(ds::DynamicalSystem, N, k = dimension(ds); kwargs...) -> λs Calculate the spectrum of Lyapunov exponents  [Lyapunov1992]  of  ds  by applying a QR-decomposition on the parallelepiped defined by the deviation vectors, in total for  N  evolution steps. Return the spectrum sorted from maximum to minimum. The third argument  k  is optional, and dictates how many lyapunov exponents to calculate (defaults to  dimension(ds) ). See also  lyapunov ,  local_growth_rates . Note:  This function simply initializes a  TangentDynamicalSystem  and calls the method below. This means that the automatic Jacobian is used by default. Initialize manually a  TangentDynamicalSystem  if you have a hand-coded Jacobian. Keyword arguments u0 = current_state(ds) : State to start from. Ttr = 0 : Extra transient time to evolve the system before application of the algorithm. Should be  Int  for discrete systems. Both the system and the deviation vectors are evolved for this time. Δt = 1 : Time of individual evolutions between successive orthonormalization steps. For continuous systems this is approximate. show_progress = false : Display a progress bar of the process. Description The method we employ is \"H2\" of  [Geist1990] , originally stated in  [Benettin1980] , and explained in educational form in  [DatserisParlitz2022] . The deviation vectors defining a  D -dimensional parallelepiped in tangent space are evolved using the tangent dynamics of the system (see  TangentDynamicalSystem ). A QR-decomposition at each step yields the local growth rate for each dimension of the parallelepiped. At each step the parallelepiped is re-normalized to be orthonormal. The growth rates are then averaged over  N  successive steps, yielding the lyapunov exponent spectrum. source lyapunovspectrum(tands::TangentDynamicalSystem, N::Int; Ttr, Δt, show_progress) The low-level method that is called by  lyapunovspectrum(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  tands . Also use this method if you have a hand-coded Jacobian to pass when creating  tands . source"},{"id":281,"pagetitle":"Lyapunov Exponents","title":"Example","ref":"/chaostools/stable/lyapunovs/#Example","content":" Example For example, the Lyapunov spectrum of the  folded towel map  is calculated as: using ChaosTools\nfunction towel_rule(x, p, n)\n    @inbounds x1, x2, x3 = x[1], x[2], x[3]\n    SVector( 3.8*x1*(1-x1) - 0.05*(x2+0.35)*(1-2*x3),\n    0.1*( (x2+0.35)*(1-2*x3) - 1 )*(1 - 1.9*x1),\n    3.78*x3*(1-x3)+0.2*x2 )\nend\nfunction towel_jacob(x, p, n)\n    row1 = SVector(3.8*(1 - 2x[1]), -0.05*(1-2x[3]), 0.1*(x[2] + 0.35))\n    row2 = SVector(-0.19((x[2] + 0.35)*(1-2x[3]) - 1),  0.1*(1-2x[3])*(1-1.9x[1]),  -0.2*(x[2] + 0.35)*(1-1.9x[1]))\n    row3 = SVector(0.0,  0.2,  3.78(1-2x[3]))\n    return vcat(row1', row2', row3')\nend\n\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\nλλ = lyapunovspectrum(tands, 10000) 3-element Vector{Float64}:\n  0.43224475747701546\n  0.3722615352085776\n -3.296655735650821 lyapunovspectrum  also works for continuous time systems and will auto-generate a Jacobian function if one is not give. For example, function lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nlor = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\nλλ = lyapunovspectrum(lor, 10000; Δt = 0.1) 3-element Vector{Float64}:\n   0.9888670182160189\n   0.003980822649737431\n -14.659449798405003 lyapunovspectrum  is also very fast: using BenchmarkTools\nds = DeterministicIteratedMap(towel_rule, [0.085, -0.121, 0.075], nothing)\ntands = TangentDynamicalSystem(ds; J = towel_jacob)\n\n@btime lyapunovspectrum($tands, 10000)   966.500 μs (10 allocations: 576 bytes) # on my laptop Here is an example of using  reinit!  to efficiently iterate over different parameter values, and parallelize via  Threads , to compute the exponents over a given parameter range. using ChaosTools, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\nds = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\ntands = TangentDynamicalSystem(ds; J = henon_jacob)\n\nas = 0.8:0.005:1.225;\nλs = zeros(length(as), 2)\n\n# Since `DynamicalSystem`s are mutable, we need to copy to parallelize\nsystems = [deepcopy(tands) for _ in 1:Threads.nthreads()-1]\npushfirst!(systems, tands)\n\nThreads.@threads for i in eachindex(as)\n    system = systems[Threads.threadid()]\n    set_parameter!(system, 1, as[i])\n    λs[i, :] .= lyapunovspectrum(system, 10000; Ttr = 500)\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"a\", ylabel = L\"\\lambda\")\nfor j in 1:2\n    lines!(ax, as, λs[:, j])\nend\nfig"},{"id":282,"pagetitle":"Lyapunov Exponents","title":"Maximum Lyapunov Exponent","ref":"/chaostools/stable/lyapunovs/#Maximum-Lyapunov-Exponent","content":" Maximum Lyapunov Exponent It is possible to get only the maximum Lyapunov exponent simply by giving  1  as the third argument of  lyapunovspectrum . However, there is a second algorithm that calculates the maximum exponent:"},{"id":283,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunov","ref":"/chaostools/stable/lyapunovs/#ChaosTools.lyapunov","content":" ChaosTools.lyapunov  —  Function lyapunov(ds::DynamicalSystem, Τ; kwargs...) -> λ Calculate the maximum Lyapunov exponent  λ  using a method due to Benettin  [Benettin1976] , which simply evolves two neighboring trajectories (one called \"given\" and one called \"test\") while constantly rescaling the test one. T   denotes the total time of evolution (should be  Int  for discrete time systems). See also  lyapunovspectrum ,  local_growth_rates . Keyword arguments show_progress = false : Display a progress bar of the process. u0 = initial_state(ds) : Initial condition. Ttr = 0 : Extra \"transient\" time to evolve the trajectories before starting to measure the exponent. Should be  Int  for discrete systems. d0 = 1e-9 : Initial & rescaling distance between the two neighboring trajectories. d0_lower = 1e-3*d0 : Lower distance threshold for rescaling. d0_upper = 1e+3*d0 : Upper distance threshold for rescaling. Δt = 1 : Time of evolution between each check rescaling of distance. For continuous time systems this is approximate. inittest = (u1, d0) -> u1 .+ d0/sqrt(length(u1)) : A function that given  (u1, d0)  initializes the test state with distance  d0  from the given state  u1   ( D  is the dimension of the system). This function can be used when you want to avoid the test state appearing in a region of the phase-space where it would have e.g. different energy or escape to infinity. Description Two neighboring trajectories with initial distance  d0  are evolved in time. At time  $t_i$  if their distance  $d(t_i)$  either exceeds the  d0_upper , or is lower than  d0_lower , the test trajectory is rescaled back to having distance  d0  from the reference one, while the rescaling keeps the difference vector along the maximal expansion/contraction direction:  $u_2 \\to u_1+(u_2−u_1)/(d(t_i)/d_0)$ . The maximum Lyapunov exponent is the average of the time-local Lyapunov exponents \\[\\lambda = \\frac{1}{t_{n} - t_0}\\sum_{i=1}^{n}\n\\ln\\left( a_i \\right),\\quad a_i = \\frac{d(t_{i})}{d_0}.\\] Performance notes This function simply initializes a  ParallelDynamicalSystem  and calls the method below. source lyapunov(pds::ParallelDynamicalSystem, T; Ttr, Δt, d0, d0_upper, d0_lower) The low-level method that is called by  lyapunov(ds::DynamicalSystem, ...) . Use this method for looping over different initial conditions or parameters by calling  reinit!  to  pds . source For example: using ChaosTools\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\nλ = lyapunov(henon, 10000; d0 = 1e-7, d0_upper = 1e-4, Ttr = 100) 0.42018736282059616"},{"id":284,"pagetitle":"Lyapunov Exponents","title":"Local Growth Rates","ref":"/chaostools/stable/lyapunovs/#Local-Growth-Rates","content":" Local Growth Rates"},{"id":285,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.local_growth_rates","ref":"/chaostools/stable/lyapunovs/#ChaosTools.local_growth_rates","content":" ChaosTools.local_growth_rates  —  Function local_growth_rates(ds::DynamicalSystem, points::Dataset; kwargs...) → λlocal Compute the local exponential growth rate(s) of perturbations of the dynamical system  ds  for initial conditions given in  points . For each initial condition  u ∈ points ,  S  total perturbations are created and evolved exactly for time  Δt . The exponential local growth rate is defined simply by  log(g/g0)/Δt  with  g0  the initial perturbation size and  g  the size after  Δt . Thus,  λlocal  is a matrix of size  (length(points), S) . This function is a modification of  lyapunov . It uses the full nonlinear dynamics and a  ParallelDynamicalSystem  to evolve the perturbations, but does not do any re-scaling, thus allowing probing state and time dependence of perturbation growth. The actual growth is given by  exp(λlocal * Δt) . The output of this function is sometimes called \"Nonlinear Local Lyapunov Exponent\". Keyword arguments S = 100 Δt = 5 perturbation : If given, it should be a function  perturbation(ds, u, j)  that outputs a perturbation vector (preferrably  SVector ) given the system, current initial condition  u  and the counter  j ∈ 1:S . If not given, a random perturbation is generated with norm given by the keyword  e = 1e-6 . source Here is a simple example using the Henon map using ChaosTools\nusing Statistics, CairoMakie\n\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhe = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\npoints = trajectory(he, 2000; Ttr = 100)[1]\n\nλlocal = local_growth_rates(he, points; Δt = 1)\n\nλmeans = mean(λlocal; dims = 2)\nλstds = std(λlocal; dims = 2)\nx, y = columns(points)\nfig, ax, obj = scatter(x, y; color = vec(λmeans))\nColorbar(fig[1,2], obj)\nfig"},{"id":286,"pagetitle":"Lyapunov Exponents","title":"Lyapunov exponent from data","ref":"/chaostools/stable/lyapunovs/#Lyapunov-exponent-from-data","content":" Lyapunov exponent from data"},{"id":287,"pagetitle":"Lyapunov Exponents","title":"ChaosTools.lyapunov_from_data","ref":"/chaostools/stable/lyapunovs/#ChaosTools.lyapunov_from_data","content":" ChaosTools.lyapunov_from_data  —  Function lyapunov_from_data(R::Dataset, ks; kwargs...) For the given dataset  R , which is expected to represent a trajectory of a dynamical system, calculate and return  E(k) , which is the average logarithmic distance between states of a neighborhood that are evolved in time for  k  steps ( k  must be integer). The slope of  E  vs  k  approximates the maximum Lyapunov exponent. Typically  R  is the result of delay coordinates embedding of a timeseries (see DelayEmbeddings.jl). Keyword arguments refstates = 1:(length(R) - ks[end]) : Vector of indices that notes which states of the dataset should be used as \"reference states\", which means that the algorithm is applied for all state indices contained in  refstates . w::Int = 1 : The  Theiler window . ntype = NeighborNumber(1) : The neighborhood type. Either  NeighborNumber  or  WithinRange . See  Neighborhoods  for more info. distance = FirstElement() : Specifies what kind of distance function is used in the logarithmic distance of nearby states. Allowed distances values are  FirstElement()  or  Euclidean() , see below for more info. The metric for finding neighbors is always the Euclidean one. Description If the dataset exhibits exponential divergence of nearby states, then it should hold \\[E(k) \\approx \\lambda\\cdot k \\cdot \\Delta t + E(0)\\] for a  well defined region  in the  $k$  axis, where  $\\lambda$  is the approximated maximum Lyapunov exponent.  $\\Delta t$  is the time between samples in the original timeseries. You can use  linear_region  with arguments  (ks .* Δt, E)  to identify the slope (=  $\\lambda$ ) immediately, assuming you have chosen sufficiently good  ks  such that the linear scaling region is bigger than the saturated region. The algorithm used in this function is due to Parlitz [Skokos2016] , which itself expands upon Kantz [Kantz1994] . In sort, for each reference state a neighborhood is evaluated. Then, for each point in this neighborhood, the logarithmic distance between reference state and neighborhood state(s) is calculated as the \"time\" index  k  increases. The average of the above over all neighborhood states over all reference states is the returned result. If the  distance  is  Euclidean()  then use the Euclidean distance of the full  D -dimensional points (distance  $d_E$  in ref. [Skokos2016] ). If however the  distance  is  FirstElement() , calculate the absolute distance of  only the first elements  of the points of  R  (distance  $d_F$  in ref. [Skokos2016] , useful when  R  comes from delay embedding). source"},{"id":288,"pagetitle":"Lyapunov Exponents","title":"Neighborhood.NeighborNumber","ref":"/chaostools/stable/lyapunovs/#Neighborhood.NeighborNumber","content":" Neighborhood.NeighborNumber  —  Type NeighborNumber(k::Int) <: SearchType Search type representing the  k  nearest neighbors of the query (or approximate neighbors, depending on the search structure)."},{"id":289,"pagetitle":"Lyapunov Exponents","title":"Neighborhood.WithinRange","ref":"/chaostools/stable/lyapunovs/#Neighborhood.WithinRange","content":" Neighborhood.WithinRange  —  Type WithinRange(r::Real) <: SearchType Search type representing all neighbors with distance  ≤ r  from the query (according to the search structure's metric). Let's apply the method to a timeseries from a continuous time system. In this case, one must be a bit more thoughtful when choosing parameters. The following example helps the users get familiar with the process: using ChaosTools, CairoMakie\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\n\nds = CoupledODEs(lorenz_rule, fill(10.0, 3), [10, 32, 8/3])\n# create a timeseries of 1 dimension\nΔt = 0.05\nx = trajectory(ds, 1000.0; Ttr = 10, Δt)[1][:, 1] 20001-element Vector{Float64}:\n   4.080373146944597\n   4.240648211827648\n   4.995736623578444\n   6.360963428912875\n   8.36866320098022\n  10.871010893289172\n  13.195165389449697\n  14.068943434493336\n  12.64585168056817\n   9.674355464223451\n   ⋮\n  -7.766435939250359\n -10.779066585434036\n -13.875176556262863\n -15.33559998218722\n -13.671249922644769\n  -9.817896236698331\n  -5.976667861675601\n  -3.289232024597773\n  -1.7613819625138338 From prior knowledge of the system, we know we need to use  k  up to about  150 . However, due to the dense time sampling, we don't have to compute for every  k  in the range  0:150 . Instead, we can use ks = 0:4:150 0:4:148 Now we plot some example computations using delay embeddings to \"reconstruct\" the chaotic attractor using DelayEmbeddings: embed\nfig = Figure()\nax = Axis(fig[1,1]; xlabel=\"k (0.05×t)\", ylabel=\"E - E(0)\")\nntype = NeighborNumber(5) #5 nearest neighbors of each state\n\nfor d in [4, 8], τ in [7, 15]\n    r = embed(x, d, τ)\n\n    # E1 = lyapunov_from_data(r, ks1; ntype)\n    # λ1 = ChaosTools.linreg(ks1 .* Δt, E1)[2]\n    # plot(ks1,E1.-E1[1], label = \"dense, d=$(d), τ=$(τ), λ=$(round(λ1, 3))\")\n\n    E2 = lyapunov_from_data(r, ks; ntype)\n    λ2 = ChaosTools.linreg(ks .* Δt, E2)[2]\n    lines!(ks, E2.-E2[1]; label = \"d=$(d), τ=$(τ), λ=$(round(λ2, digits = 3))\")\nend\naxislegend(ax; position = :lt)\nax.title = \"Continuous Reconstruction Lyapunov\"\nfig As you can see, using  τ = 15  is not a great choice! The estimates with  τ = 7  though are very good (the actual value is around  λ ≈ 0.89... ). Notice that above a linear regression was done over the whole curves, which doesn't make sense. One should identify a linear scaling region and extract the slope of that one. The function  linear_region  from  FractalDimensions.jl  does this! Lyapunov1992 A. M. Lyapunov,  The General Problem of the Stability of Motion , Taylor & Francis (1992) Geist1990 K. Geist  et al. , Progr. Theor. Phys.  83 , pp 875 (1990) Benettin1980 G. Benettin  et al. , Meccanica  15 , pp 9-20 & 21-30 (1980) DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics Benettin1976 G. Benettin  et al. , Phys. Rev. A  14 , pp 2338 (1976) Skokos2016 Skokos, C. H.  et al. ,  Chaos Detection and Predictability  - Chapter 1 (section 1.3.2), Lecture Notes in Physics  915 , Springer (2016) Kantz1994 Kantz, H., Phys. Lett. A  185 , pp 77–87 (1994)"},{"id":292,"pagetitle":"Orbit diagrams","title":"Orbit diagrams","ref":"/chaostools/stable/orbitdiagram/#Orbit-diagrams","content":" Orbit diagrams An orbit diagram is a way to visualize the asymptotic behaviour of a map, when a parameter of the system is changed. In practice an orbit diagram is a simple plot that plots the last  n  states of a dynamical system at a given parameter, repeated for all parameters in a range of interest. While this concept can apply to any kind of system, it makes most sense in discrete time dynamical systems. See Chapter 4 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022, for a more involved discussion on orbit diagrams for both discrete and continuous time systems."},{"id":293,"pagetitle":"Orbit diagrams","title":"ChaosTools.orbitdiagram","ref":"/chaostools/stable/orbitdiagram/#ChaosTools.orbitdiagram","content":" ChaosTools.orbitdiagram  —  Function orbitdiagram(ds::DynamicalSystem, i, p_index, pvalues; kwargs...) → od Compute the orbit diagram (sometimes wrongly called bifurcation diagram) of the given dynamical system, saving the  i  variable(s) for parameter values  pvalues . The  p_index  specifies which parameter to change via  set_parameter!(ds, p_index, pvalue) . Works for any kind of  DynamicalSystem , although it mostly makes sense with one of  DeterministicIteratedMap, StroboscopicMap, PoincareMap . An orbit diagram is simply a collection of the last  n  states of  ds  as  ds  is evolved. This is done for each parameter value. i  can be  Int  or  AbstractVector{Int} . If  i  is  Int ,  od  is a vector of vectors. Else  od  is a vector of vectors of vectors. Each entry od  od  are the points at each parameter value, so that  length(od) == length(pvalues)  and  length(od[j]) == n, ∀ j . Keyword arguments n::Int = 100 : Amount of points to save for each parameter value. Δt = 1 : Stepping time between saving points. u0 = nothing : Specify an initial state. If  nothing , the previous state after each parameter is used to seed the new initial condition at the new parameter (with the very first state being the system's state). This makes convergence to the attractor faster, necessitating smaller  Ttr . Otherwise  u0  can be a standard state, or a vector of states, so that a specific state is used for each parameter. Ttr::Int = 10 : Each orbit is evolved for  Ttr  first before saving output. ulims = (-Inf, Inf) : only record system states within  ulims  (only valid if  i isa Int ). Iteration continues until  n  states fall within  ulims . show_progress = false : Display a progress bar (counting the parameter values). periods = nothing : Only valid if  ds isa StroboscopicMap . If given, it must be a a container with same layout as  pvalues . Provides a value for the  period  for each parameter value. Useful in case the orbit diagram is produced versus a driving frequency. source"},{"id":294,"pagetitle":"Orbit diagrams","title":"Deterministic iterated map","ref":"/chaostools/stable/orbitdiagram/#Deterministic-iterated-map","content":" Deterministic iterated map For example, let's compute the famous orbit diagram of the logistic map: using ChaosTools, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nlogistic = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\n\ni = 1\nparameter = 1\npvalues = 2.5:0.004:4\nn = 2000\nTtr = 2000\noutput = orbitdiagram(logistic, i, parameter, pvalues; n, Ttr)\n\nL = length(pvalues)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= pvalues[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"r\", ylabel = L\"x\"),\n    markersize = 0.8, color = (\"black\", 0.05),\n)\nax.title = \"Logistic map orbit diagram\"\nxlims!(ax, pvalues[1], pvalues[end]); ylims!(ax,0,1)\nfig"},{"id":295,"pagetitle":"Orbit diagrams","title":"Stroboscopic map","ref":"/chaostools/stable/orbitdiagram/#Stroboscopic-map","content":" Stroboscopic map The beauty of  orbitdiagram  is that it can be directly applied to any kind of  DynamicalSystem . The most useful cases are the already seen  DeterministicIteratedMap , but also  PoincareMap  and  StroboscopicMap . Here is an example of the orbit diagram for the Duffing oscillator (making the same as Figure 9.2 of   Nonlinear Dynamics , Datseris & Parlitz, Springer 2022). using ChaosTools, CairoMakie\n\nfunction duffing_rule(u,p,t)\n    d, a, ω = p\n    du1 =  u[2]\n    du2 =  -u[1] - u[1]*u[1]*u[1] - d*u[2] + a*sin(ω*t)\n    return SVector(du1, du2)\nend\nT0 = 25.0\np0 = [0.1, 7, 2π/T0]\nu0 = [1.1, 1.1]\nds = CoupledODEs(duffing_rule, u0, p0)\nduffing = StroboscopicMap(ds, T0)\n\n# We want to change both the parameter `ω`, but also the\n# period of the stroboscopic map. `orbitdiagram` allows this!\nTrange = range(8, 26; length = 201)\nωrange = @. 2π / Trange\nn = 200\noutput = orbitdiagram(duffing, 1, 3, ωrange; n, u0, Ttr = 100, periods = Trange)\n\nL = length(Trange)\nx = Vector{Float64}(undef, n*L)\ny = copy(x)\nfor j in 1:L\n    x[(1 + (j-1)*n):j*n] .= Trange[j]\n    y[(1 + (j-1)*n):j*n] .= output[j]\nend\n\nfig, ax = scatter(x, y; axis = (xlabel = L\"T\", ylabel = L\"u_1\"),\n    markersize = 8, color = (\"blue\", 0.25),\n)\nylims!(ax, -1, 1)\nfig Pro tip: to actually make Fig. 9.2 you'd have to do two modifications: first, pass  periods = Trange ./ 2 , so that points are recorded every half period. Then, at the very end, do  y[2:2:end] .= -y[2:2:end]  so that the symmetric orbits are recorded as well"},{"id":298,"pagetitle":"Fixed points & Periodicity","title":"Fixed points & Periodicity","ref":"/chaostools/stable/periodicity/#Fixed-points-and-Periodicity","content":" Fixed points & Periodicity"},{"id":299,"pagetitle":"Fixed points & Periodicity","title":"Fixed points","ref":"/chaostools/stable/periodicity/#Fixed-points","content":" Fixed points"},{"id":300,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.fixedpoints","ref":"/chaostools/stable/periodicity/#ChaosTools.fixedpoints","content":" ChaosTools.fixedpoints  —  Function fixedpoints(ds::CoreDynamicalSystem, box, J = nothing; kwargs...) → fp, eigs, stable Return all fixed points  fp  of the given out-of-place  ds  (either  DeterministicIteratedMap  or  CoupledODEs ) that exist within the state space subset  box  for parameter configuration  p . Fixed points are returned as a  StateSpaceSet . For convenience, a vector of the Jacobian eigenvalues of each fixed point, and whether the fixed points are stable or not, are also returned. box  is an appropriate  IntervalBox  from IntervalRootFinding.jl. E.g. for a 3D system it would be something like v, z = -5..5, -2..2   # 1D intervals, can use `interval(-5, 5)` instead\nbox = v × v × z       # `\\times = ×`, or use `IntervalBox(v, v, z)` instead J  is the Jacobian of the dynamic rule of  ds . It is like in  TangentDynamicalSystem , however in this case automatic Jacobian estimation does not work, hence a hand-coded version must be given. Internally IntervalRootFinding.jl is used and as a result we are guaranteed to find all fixed points that exist in  box , regardless of stability. Since IntervalRootFinding.jl returns an interval containing a unique fixed point, we return the midpoint of the interval as the actual fixed point. Naturally, limitations inherent to IntervalRootFinding.jl apply here. The output of  fixedpoints  can be used in the  BifurcationKit.jl  as a start of a continuation process. See also  periodicorbits . Keyword arguments method = IntervalRootFinding.Krawczyk  configures the root finding method, see the docs of IntervalRootFinding.jl for all possibilities. tol = 1e-15  is the root-finding tolerance. warn = true  throw a warning if no fixed points are found. source A rather simple example of the fixed points can be demonstrated using E.g., the Lorenz-63 system, whose fixed points can be calculated analytically to be the following three \\[(0,0,0) \\\\\n\\left( \\sqrt{\\beta(\\rho-1)}, \\sqrt{\\beta(\\rho-1)}, \\rho-1 \\right) \\\\\n\\left( -\\sqrt{\\beta(\\rho-1)}, -\\sqrt{\\beta(\\rho-1)}, \\rho-1 \\right) \\\\\\] So, let's calculate using ChaosTools\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector{3}(du1, du2, du3)\nend\nfunction lorenz_jacob(u, p, t)\n    σ, ρ, β = p\n    return SMatrix{3,3}(-σ, ρ - u[3], u[2], σ, -1, u[1], 0, -u[1], -β)\nend\n\nρ, β = 30.0, 10/3\nlorenz = CoupledODEs(lorenz_rule, 10ones(3), [10.0, ρ, β])\n# Define the box within which to find fixed points:\nx = y = interval(-20, 20)\nz = interval(0, 40)\nbox = x × y × z\n\nfp, eigs, stable = fixedpoints(lorenz, box, lorenz_jacob)\nfp 3-dimensional StateSpaceSet{Float64} with 3 points\n  9.83192       9.83192      29.0\n -9.83192      -9.83192      29.0\n  3.23647e-17   3.23647e-17   3.43473e-16 and compare this with the analytic ones: lorenzfp(ρ, β) = [\n    SVector(0, 0, 0.0),\n    SVector(sqrt(β*(ρ-1)), sqrt(β*(ρ-1)), ρ-1),\n    SVector(-sqrt(β*(ρ-1)), -sqrt(β*(ρ-1)), ρ-1),\n]\n\nlorenzfp(ρ, β) 3-element Vector{SVector{3, Float64}}:\n [0.0, 0.0, 0.0]\n [9.83192080250175, 9.83192080250175, 29.0]\n [-9.83192080250175, -9.83192080250175, 29.0]"},{"id":301,"pagetitle":"Fixed points & Periodicity","title":"Stable and Unstable Periodic Orbits of Maps","ref":"/chaostools/stable/periodicity/#Stable-and-Unstable-Periodic-Orbits-of-Maps","content":" Stable and Unstable Periodic Orbits of Maps Chaotic behavior of low dimensional dynamical systems is affected by the position and the stability properties of the  periodic orbits  of a dynamical system. Finding unstable (or stable) periodic orbits of a discrete mapping analytically rapidly becomes impossible for higher orders of fixed points. Fortunately there is a numeric algorithm due to Schmelcher & Diakonos which allows such a computation. Notice that even though the algorithm can find stable fixed points, it is mainly aimed at  unstable  ones. The functions  periodicorbits  and  lambdamatrix  implement the algorithm:"},{"id":302,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.periodicorbits","ref":"/chaostools/stable/periodicity/#ChaosTools.periodicorbits","content":" ChaosTools.periodicorbits  —  Function periodicorbits(ds::DeterministicIteratedMap,\n               o, ics [, λs, indss, singss]; kwargs...) -> FP Find fixed points  FP  of order  o  for the map  ds  using the algorithm due to Schmelcher & Diakonos [Schmelcher1997] .  ics  is a collection of initial conditions (container of vectors) to be evolved. Optional arguments The optional arguments  λs, indss, singss must be containers  of appropriate values, besides  λs  which can also be a number. The elements of those containers are passed to:  lambdamatrix(λ, inds, sings) , which creates the appropriate  $\\mathbf{\\Lambda}_k$  matrix. If these arguments are not given, a random permutation will be chosen for them, with  λ=0.001 . Keyword arguments maxiters::Int = 100000 : Maximum amount of iterations an i.c. will be iterated  before claiming it has not converged. disttol = 1e-10 : Distance tolerance. If the 2-norm of a previous state with  the next one is  ≤ disttol  then it has converged to a fixed point. inftol = 10.0 : If a state reaches  norm(state) ≥ inftol  it is assumed that  it has escaped to infinity (and is thus abandoned). roundtol::Int = 4 : The found fixed points are rounded  to  roundtol  digits before pushed into the list of returned fixed points  FP ,   if  they are not already contained in  FP .  This is done so that  FP  doesn't contain duplicate fixed points (notice  that this has nothing to do with  disttol ). Description The algorithm used can detect periodic orbits by turning fixed points of the original map  ds  to stable ones, through the transformation \\[\\mathbf{x}_{n+1} = \\mathbf{x}_n +\n\\mathbf{\\Lambda}_k\\left(f^{(o)}(\\mathbf{x}_n) - \\mathbf{x}_n\\right)\\] The index  $k$  counts the various possible  $\\mathbf{\\Lambda}_k$ . Performance notes All  initial conditions are evolved for  all $\\mathbf{\\Lambda}_k$  which can very quickly lead to long computation times. source"},{"id":303,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.lambdamatrix","ref":"/chaostools/stable/periodicity/#ChaosTools.lambdamatrix","content":" ChaosTools.lambdamatrix  —  Function lambdamatrix(λ, inds::Vector{Int}, sings) -> Λk Return the matrix  $\\mathbf{\\Lambda}_k$  used to create a new dynamical system with some unstable fixed points turned to stable in the function  periodicorbits . Arguments λ<:Real  : the multiplier of the  $C_k$  matrix, with  0<λ<1 . inds::Vector{Int}  : The  i th entry of this vector gives the  row  of the nonzero element of the  i th column of  $C_k$ . sings::Vector{<:Real}  : The element of the  i th column of  $C_k$  is +1 if  signs[i] > 0  and -1 otherwise ( sings  can also be  Bool  vector). Calling  lambdamatrix(λ, D::Int)  creates a random  $\\mathbf{\\Lambda}_k$  by randomly generating an  inds  and a  signs  from all possible combinations. The  collections  of all these combinations can be obtained from the function  lambdaperms . Description Each element of  inds must be unique  such that the resulting matrix is orthogonal and represents the group of special reflections and permutations. Deciding the appropriate values for  λ, inds, sings  is not trivial. However, in ref. [Pingel2000]  there is a lot of information that can help with that decision. Also, by appropriately choosing various values for  λ , one can sort periodic orbits from e.g. least unstable to most unstable, see [Diakonos1998]  for details. source"},{"id":304,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.lambdaperms","ref":"/chaostools/stable/periodicity/#ChaosTools.lambdaperms","content":" ChaosTools.lambdaperms  —  Function lambdaperms(D) -> indperms, singperms Return two collections that each contain all possible combinations of indices (total of  $D!$ ) and signs (total of  $2^D$ ) for dimension  D  (see  lambdamatrix ). source"},{"id":305,"pagetitle":"Fixed points & Periodicity","title":"Standard Map example","ref":"/chaostools/stable/periodicity/#Standard-Map-example","content":" Standard Map example For example, let's find the fixed points of the  Systems.standardmap  of order 2, 3, 4, 5, 6 and 8. We will use all permutations for the  signs  but only one for the  inds . We will also only use one  λ  value, and a 21×21 density of initial conditions. First, initialize everything using ChaosTools\n\nfunction standardmap_rule(x, k, n)\n    theta = x[1]; p = x[2]\n    p += k[1]*sin(theta)\n    theta += p\n    return SVector(mod2pi(theta), mod2pi(p))\nend\n\nstandardmap = DeterministicIteratedMap(standardmap_rule, rand(2), [1.0])\nxs = range(0, stop = 2π, length = 11); ys = copy(xs)\nics = [SVector{2}(x,y) for x in xs for y in ys]\n\n# All permutations of [±1, ±1]:\nsingss = lambdaperms(2)[2] # second entry are the signs\n\n# I know from personal research I only need this `inds`:\nindss = [[1,2]] # <- must be container of vectors!\n\nλs = 0.005 # <- only this allowed to not be vector (could also be vector)\n\norders = [2, 3, 4, 5, 6, 8]\nALLFP = Dataset{2, Float64}[]\n\nstandardmap 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  standardmap_rule\n parameters:    [1.0]\n time:          0\n state:         [0.8905154044954036, 0.16337024368056163]\n Then, do the necessary computations for all orders for o in orders\n    FP = periodicorbits(standardmap, o, ics, λs, indss, singss)\n    push!(ALLFP, FP)\nend Plot the phase space of the standard map using CairoMakie\niters = 1000\ndataset = trajectory(standardmap, iters)[1]\nfor x in xs\n    for y in ys\n        append!(dataset, trajectory(standardmap, iters, [x, y])[1])\n    end\nend\n\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"\\theta\", ylabel = L\"p\",\n    limits = ((xs[1],xs[end]), (xs[1],xs[end]))\n)\nscatter!(ax, dataset[:, 1], dataset[:, 2]; markersize = 1, color = \"black\")\nfig and finally, plot the fixed points markers = [:diamond, :utriangle, :rect, :pentagon, :hexagon, :circle]\n\nfor i in 1:6\n    FP = ALLFP[i]\n    o = orders[i]\n    scatter!(ax, columns(FP)...; marker=markers[i], color = Cycled(i),\n        markersize = 30 - 2i, strokecolor = \"grey\", strokewidth = 1, label = \"order $o\"\n    )\nend\naxislegend(ax)\nfig Okay, this output is great, and we can tell that it is correct because: Fixed points of order  $n$  are also fixed points of order  $2n, 3n, 4n, ...$ Besides fixed points of previous orders,  original  fixed points of order  $n$  come in (possible multiples of)  $2n$ -sized pairs (see e.g. order 5). This is a direct consequence of the Poincaré–Birkhoff theorem."},{"id":306,"pagetitle":"Fixed points & Periodicity","title":"Estimating the Period","ref":"/chaostools/stable/periodicity/#Estimating-the-Period","content":" Estimating the Period The function  estimate_period  offers ways for estimating the period (either exact for periodic timeseries, or approximate for near-periodic ones) of a given timeseries. We offer five methods to estimate periods, some of which work on evenly sampled data only, and others which accept any data. The figure below summarizes this: "},{"id":307,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.estimate_period","ref":"/chaostools/stable/periodicity/#ChaosTools.estimate_period","content":" ChaosTools.estimate_period  —  Function estimate_period(v::Vector, method, t=0:length(v)-1; kwargs...) Estimate the period of the signal  v , with accompanying time vector  t , using the given  method . If  t  is an AbstractArray, then it is iterated through to ensure that it's evenly sampled (if necessary for the algorithm).  To avoid this, you can pass any  AbstractRange , like a  UnitRange  or a  LinRange , which are defined to be evenly sampled. Methods requiring evenly sampled data These methods are faster, but some are error-prone. :periodogram  or  :pg : Use the fast Fourier transform to compute a  periodogram (power-spectrum) of the given data.  Data must be evenly sampled. :multitaper  or  mt : The multitaper method reduces estimation bias by using multiple independent estimates from the same sample. Data tapers are then windowed and the power spectra are obtained.  Available keywords follow:  nw  is the time-bandwidth product, and  ntapers  is the number of tapers. If  window  is not specified, the signal is tapered with  ntapers  discrete prolate spheroidal sequences with time-bandwidth product  nw . Each sequence is equally weighted; adaptive multitaper is not (yet) supported. If  window  is specified, each column is applied as a taper. The sum of periodograms is normalized by the total sum of squares of  window . :autocorrelation  or  :ac : Use the autocorrelation function (AC). The value where the AC first comes back close to 1 is the period of the signal. The keyword  L = length(v)÷10  denotes the length of the AC (thus, given the default setting, this method will fail if there less than 10 periods in the signal). The keyword  ϵ = 0.2  ( \\epsilon ) means that  1-ϵ  counts as \"1\" for the AC. :yin : The YIN algorithm. An autocorrelation-based method to estimate the fundamental period of the signal. See the original paper  [CheveigneYIN2002]  or the implementation  yin . Sampling rate is taken as  sr = 1/mean(diff(t))  if not given. speech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930. Methods not requiring evenly sampled data These methods tend to be slow, but versatile and low-error. :lombscargle  or  :ls : Use the Lomb-Scargle algorithm to compute a periodogram.  The advantage of the Lomb-Scargle method is that it does not require an equally sampled dataset and performs well on undersampled datasets. Constraints have been set on the period, since Lomb-Scargle tends to have false peaks at very low frequencies.  That being said, it's a very flexible method.  It is extremely customizable, and the keyword arguments that can be passed to it are given  in the documentation . :zerocrossing  or  :zc : Find the zero crossings of the data, and use the average difference between zero crossings as the period.  This is a naïve implementation, with only linear interpolation; however, it's useful as a sanity check.  The keyword  line  controls where the \"crossing point\" is. It defaults to  mean(v) . For more information on the periodogram methods, see the documentation of DSP.jl and LombScargle.jl. source"},{"id":308,"pagetitle":"Fixed points & Periodicity","title":"ChaosTools.yin","ref":"/chaostools/stable/periodicity/#ChaosTools.yin","content":" ChaosTools.yin  —  Function yin(sig::Vector, sr::Int; kwargs...) -> F0s, frame_times Estimate the fundamental frequency (F0) of the signal  sig  using the YIN algorithm  [1] . The signal  sig  is a vector of points uniformly sampled at a rate  sr . Keyword arguments w_len : size of the analysis window [samples == number of points] f_step : size of the lag between two consecutive frames [samples == number of points] f0_min : Minimum fundamental frequency that can be detected [linear frequency] f0_max : Maximum fundamental frequency that can be detected [linear frequency] harmonic_threshold : Threshold of detection. The algorithm returns the first minimum of the CMNDF function below this threshold. diffference_function : The difference function to be used (by default  ChaosTools.difference_function_original ). Description The YIN algorithm  [CheveigneYIN2002]  estimates the signal's fundamental frequency  F0  by basically looking for the period  τ0   which minimizes the signal's autocorrelation. This autocorrelation is calculated for signal segments (frames), composed of two windows of length  w_len . Each window is separated by a distance  τ , and the idea is that the distance which minimizes the pairwise difference between each window is considered to be the fundamental period  τ0  of that frame. More precisely, the algorithm first computes the cumulative mean normalized difference function (MNDF) between two windows of a frame for several candidate periods  τ  ranging from  τ_min=sr/f0_max  to  τ_max=sr/f0_min . The MNDF is defined as \\[d_t^\\prime(\\tau) = \\begin{cases}\n        1 & \\text{if} ~ \\tau=0 \\\\\n        d_t(\\tau)/\\left[{(\\frac 1 \\tau) \\sum_{j=1}^{\\tau} d_{t}(j)}\\right] & \\text{otherwise}\n        \\end{cases}\\] where  d_t  is the difference function: \\[d_t(\\tau) = \\sum_{j=1}^W (x_j - x_{j+\\tau})^2\\] It then refines the local minima of the MNDF using parabolic (quadratic) interpolation. This is done by taking each minima, along with their first neighbor points, and finding the minimum of the corresponding interpolated parabola. The MNDF minima are substituted by the interpolation minima. Finally, the algorithm chooses the minimum with the smallest period and with a corresponding MNDF below the  harmonic threshold . If this doesn't exist, it chooses the period corresponding to the global minimum. It repeats this for frames starting at the first signal point, and separated by a distance  f_step  (frames can overlap), and returns the vector of frequencies  F0=sr/τ0  for each frame, along with the start times of each frame. As a note, the physical unit of the frequency is 1/[time], where [time] is decided by the sampling rate  sr . If, for instance, the sampling rate is over seconds, then the frequency is in Hertz. speech and music. The Journal of the Acoustical Society of America, 111(4), 1917-1930. source"},{"id":309,"pagetitle":"Fixed points & Periodicity","title":"Example","ref":"/chaostools/stable/periodicity/#Example","content":" Example Here we will use a modified FitzHugh-Nagumo system that results in periodic behavior, and then try to estimate its period. First, let's see the trajectory: using ChaosTools, CairoMakie\n\nfunction FHN(u, p, t)\n    e, b, g = p\n    v, w = u\n    dv = min(max(-2 - v, v), 2 - v) - w\n    dw = e*(v - g*w + b)\n    return SVector(dv, dw)\nend\n\ng, e, b  = 0.8, 0.04, 0.0\np0 = [e, b, g]\n\nfhn = CoupledODEs(FHN, SVector(-2, -0.6667), p0)\nT, Δt = 1000.0, 0.1\nX, t = trajectory(fhn, T; Δt)\nv = X[:, 1]\n\nlines(t, v) Examining the figure, one can see that the period of the system is around  91  time units. To estimate it numerically let's use some of the methods: estimate_period(v, :autocorrelation, t) 91.0 estimate_period(v, :periodogram, t) 91.62720091627202 estimate_period(v, :zerocrossing, t) 91.08000000000001 estimate_period(v, :yin, t; f0_min=0.01) 91.07348421171405 Schmelcher1997 P. Schmelcher & F. K. Diakonos, Phys. Rev. Lett.  78 , pp 4733 (1997) Pingel2000 D. Pingel  et al. , Phys. Rev. E  62 , pp 2119 (2000) Diakonos1998 F. K. Diakonos  et al. , Phys. Rev. Lett.  81 , pp 4349 (1998) CheveigneYIN2002 De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for CheveigneYIN2002 De Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency estimator for"},{"id":312,"pagetitle":"Rare events","title":"Rare events","ref":"/chaostools/stable/rareevents/#Rare-events","content":" Rare events"},{"id":313,"pagetitle":"Rare events","title":"Return time statistics","ref":"/chaostools/stable/rareevents/#Return-time-statistics","content":" Return time statistics"},{"id":314,"pagetitle":"Rare events","title":"ChaosTools.mean_return_times","ref":"/chaostools/stable/rareevents/#ChaosTools.mean_return_times","content":" ChaosTools.mean_return_times  —  Function mean_return_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → τ, c Return the mean return times  τ , as well as the amount of returns  c , for subsets of the state space of  ds  defined by  u₀, εs . The  ds  is evolved for a maximum of  T  time. This function is a convenience wrapper around calls to  exit_entry_times  and then to  transit_return  and then some averaging. Thus see  exit_entry_times  for the meaning of  u₀  and  εs  and further info. source"},{"id":315,"pagetitle":"Rare events","title":"ChaosTools.exit_entry_times","ref":"/chaostools/stable/rareevents/#ChaosTools.exit_entry_times","content":" ChaosTools.exit_entry_times  —  Function exit_entry_times(ds::DynamicalSystem, u₀, εs, T; kwargs...) → exits, entries Collect exit and entry times for balls or boxes centered at  u₀  with radii  εs , in the state space of the given dynamical system. Return the exit and (re-)entry return times to the set(s), where each of these is a vector containing all collected times for the respective  ε -radius set, for  ε ∈ εs . The dynamical system is evolved up to  T  total time. Use  transit_return_times(exits, entries)  to transform the output into transit and return times, and see also  mean_return_times . The keyword  show_progress  displays a progress bar. It is  false  for discrete and  true  for continuous systems by default. Description Transit and return time statistics are important for the transport properties of dynamical systems [Meiss1997]  and can be connected with fractal dimensions of chaotic sets [Boev2014] . The current algorithm collects exit and re-entry times to given sets in the state space, which are centered at the state  u₀ .  The system evolution always starts from  u₀  and the initial state of  ds  is irrelevant.  εs  is always a  Vector . Specification of sets to return to If each entry of  εs  is a real number, then sets around  u₀  are nested hyper-spheres of radius  ε ∈ εs . The sets can also be hyper-rectangles (boxes), if each entry of  εs  is a vector itself. Then, the  i -th box is defined by the space covered by  u0 .± εs[i]  (thus the actual box size is  2εs[i] !). In the future, state space sets will be specified more conveniently and a single argument  sets  will be given instead of  u₀, εs . The reason to input multiple  εs  at once is purely for performance optimization (much faster than doing each  ε  individually). Discrete time systems For discrete systems, exit time is recorded immediately after exiting of the set, and re-entry is recorded immediately on re-entry. This means that if an orbit needs 1 step to leave the set and then it re-enters immediately on the next step, the return time is 1. Continuous time systems For continuous systems, a steppable integrator supporting interpolation is used. The way to specify how to estimate exit and entry times is via the keyword  crossing_method  whose values can be: CrossingLinearIntersection() : Linear interpolation is used between integrator steps and the intersection between lines and spheres is used to find the crossing times. CrossingAccurateInterpolation(; abstol=1e-12, reltol=1e-6) : Extremely accurate high order interpolation is used between integrator steps. First, a minimization with Optim.jl finds the minimum distance of the trajectory to the set center. Then, Roots.jl is used to find the exact crossing point. The tolerances are given to both procedures. Clearly,  CrossingAccurateInterpolation  is much more accurate than  CrossingLinearIntersection , but also much slower. However, the smaller the steps the integrator takes (in case some very high accuracy solver is used), the closer the linear intersection gets to the accurate version. Benchmarks are advised for the individual specific case the algorithm is applied at, in order to choose the best method. The keyword  threshold_distance = Inf  provides a means to skip the interpolation check, if the current state of the integrator is too far from the set center. If the distance of the current state of the integrator is  threshold_distance  or more distance away from the set center, attempts to interpolate are skipped. By default  threshold_distance = Inf  and hence this never happens. Typically you'd want this to be 10-100 times the distance the trajectory covers at an average integrator step. source Meiss1997 Meiss, J. D.  Average exit time for volume-preserving maps ,  Chaos (1997) Boev2014 Boev, Vadivasova, & Anishchenko,  Poincaré recurrence statistics as an indicator of chaos synchronization ,  Chaos (2014)"},{"id":320,"pagetitle":"Attractors.jl","title":"Attractors.jl","ref":"/attractors/stable/#Attractors.jl","content":" Attractors.jl"},{"id":321,"pagetitle":"Attractors.jl","title":"Attractors","ref":"/attractors/stable/#Attractors","content":" Attractors  —  Module Attractors.jl A Julia module for finding attractors of arbitrary dynamical systems finding their basins of attraction or the state space fractions of the basins \"continuing\" the attractors and their basins over a parameter range finding the basin boundaries and analyzing their fractal properties tipping points related functionality for systems with known dynamic rule and more! It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"Attractors\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, Attractors.jl was part of ChaosTools.jl source"},{"id":322,"pagetitle":"Attractors.jl","title":"Latest news","ref":"/attractors/stable/#Latest-news","content":" Latest news Our paper on the global stability analysis framework offered by Attractors.jl ( continuation ) and the novel continuation offered by  RecurrencesFindAndMatch  is published as a  Featured Article  in Chaos (https://pubs.aip.org/aip/cha/article/33/7/073151/2904709/Framework-for-global-stability-analysis-of) and has been featured in the AIP publishing showcase (https://www.growkudos.com/publications/10.1063%25252F5.0159675/reader) New function  minimal_fatal_shock New function  match_continuation!  which improves the matching during a continuation process where attractors dissapear and re-appear."},{"id":323,"pagetitle":"Attractors.jl","title":"Outline of Attractors.jl","ref":"/attractors/stable/#Outline-of-Attractors.jl","content":" Outline of Attractors.jl First be sure that you are aware of what is a  DynamicalSystem . This is the input to the whole infrastructure of Attractors.jl. The bulk of the work in Attractors.jl is done by the  AttractorMapper  type, that instructs how to find attractors and maps initial conditions to them. It can be used in functions like  basins_fractions . For grouping features, there is a sub-infrastructure for instructing how to group features, which is governed by  GroupingConfig . The infrastructure of finding attractors and their basins fractions is then integrated into a brand new way of doing bifurcation analysis in the  continuation  function. See  Examples for Attractors.jl  for several applications in real world cases."},{"id":326,"pagetitle":"Finding Attractors","title":"Finding Attractors","ref":"/attractors/stable/attractors/#Finding-Attractors","content":" Finding Attractors Attractors.jl defines a generic interface for finding attractors of dynamical systems. One first decides the instance of  DynamicalSystem  they need. Then, an instance of  AttractorMapper  is created from this dynamical system. This  mapper  instance can be used to compute e.g.,  basins_of_attraction , and the output can be further analyzed to get e.g., the  basin_entropy ."},{"id":327,"pagetitle":"Finding Attractors","title":"Attractors.AttractorMapper","ref":"/attractors/stable/attractors/#Attractors.AttractorMapper","content":" Attractors.AttractorMapper  —  Type AttractorMapper(ds::DynamicalSystem, args...; kwargs...) → mapper Subtypes of  AttractorMapper  are structures that map initial conditions of  ds  to attractors. Currently available mapping methods: AttractorsViaProximity AttractorsViaRecurrences AttractorsViaFeaturizing All  AttractorMapper  subtypes can be used with  basins_fractions  or  basins_of_attraction . In addition, some mappers can be called as a function of an initial condition: label = mapper(u0) and this will on the fly compute and return the label of the attractor  u0  converges at. The mappers that can do this are: AttractorsViaProximity AttractorsViaRecurrences AttractorsViaFeaturizing  with the  GroupViaHistogram  configuration. source"},{"id":328,"pagetitle":"Finding Attractors","title":"Recurrences","ref":"/attractors/stable/attractors/#Recurrences","content":" Recurrences"},{"id":329,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaRecurrences","ref":"/attractors/stable/attractors/#Attractors.AttractorsViaRecurrences","content":" Attractors.AttractorsViaRecurrences  —  Type AttractorsViaRecurrences(ds::DynamicalSystem, grid::Tuple; kwargs...) Map initial conditions of  ds  to attractors by identifying attractors on the fly based on recurrences in the state space, as outlined by Datseris & Wagemakers  (Datseris and Wagemakers, 2022) . grid  is a tuple of ranges partitioning the state space so that a finite state machine can operate on top of it. For example  grid = (xg, yg)  where  xg = yg = range(-5, 5; length = 100)  for a two-dimensional system. The grid has to be the same dimensionality as the state space, use a  ProjectedDynamicalSystem  if you want to search for attractors in a lower dimensional subspace. Keyword arguments sparse = true : control the interval representation of the state space grid. If true,  uses a sparse array, whose memory usage is in general more efficient than a regular  array obtained with  sparse=false . In practice, the sparse representation should  always be preferred when searching for  basins_fractions . Only for very low  dimensional systems and for computing the full  basins_of_attraction  the  non-sparse version should be used. Time evolution configuration Ttr = 0 : Skip a transient before the recurrence routine begins. Δt : Approximate integration time step (second argument of the  step!  function). The keyword  Dt  can also be used instead if  Δ  ( \\Delta ) is not accessible. It is  1  for discrete time systems. For continuous systems, an automatic value is calculated using  automatic_Δt_basins . For very fine grids, this can become very small, much smaller than the typical integrator internal step size in case of adaptive integrators. In such cases, use  force_non_adaptive = true . force_non_adaptive = false : Only used if the input dynamical system is  CoupledODEs . If  true  the additional keywords  adaptive = false, dt = Δt  are given as  diffeq  to the  CoupledODEs . This means that adaptive integration is turned off and  Δt  is used as the ODE integrator timestep. This is useful in (1) very fine grids, and (2) if some of the attractors are limit cycles. We have noticed that in this case the integrator timestep becomes commensurate with the limit cycle period, leading to incorrectly counting the limit cycle as more than one attractor. Finite state machine configuration mx_chk_att = 2 : Μaximum checks of consecutives hits of an existing attractor cell before declaring convergence to that existing attractor. mx_chk_hit_bas = 10 : Maximum check of consecutive visits of the same basin of attraction before declaring convergence to an existing attractor. mx_chk_fnd_att = 100 : Maximum check of consecutive visits to a previously visited unlabeled cell before declaring we have found a new attractor. mx_chk_loc_att = 100 : Maximum check of consecutive visits to cells marked as a new attractor, during the attractor identification phase, before declaring we that we have identified the new attractor with sufficient cells. store_once_per_cell = true : Control if multiple points in state space that belong to the same cell are stored or not in the attractor, after an attractor is found. If  true , each visited cell will only store a point once, which is desirable for fixed points and limit cycles. If  false , at least  mx_chk_loc_att  points are stored per attractor, leading to more densely stored attractors, which may be desirable for instance in chaotic attractors. mx_chk_lost = 20 : Maximum check of iterations outside the defined grid before we declare the orbit lost outside and hence assign it label  -1 . horizon_limit = 1e6 : If the norm of the integrator state reaches this limit we declare that the orbit diverged to infinity. mx_chk_safety = Int(1e6) : A safety counter that is always increasing for each initial condition. Once exceeded, the algorithm assigns  -1  and throws a warning. This clause exists to stop the algorithm never haulting for innappropriate grids, where a found attractor may intersect in the same cell with a new attractor the orbit traces (which leads to infinite resetting of all counters). Description An initial condition given to an instance of  AttractorsViaRecurrences  is iterated based on the integrator corresponding to  ds . A recurrence in the state space means that the trajectory has converged to an attractor. This is the basis for finding attractors. A finite state machine (FSM) follows the trajectory in the state space, and constantly maps it to the given  grid . The FSM decides when an initial condition has successfully converged into an attractor. An array, internally called \"basins\", stores the state of the FSM on the grid, according to the indexing system described in  (Datseris and Wagemakers, 2022) . As the system is integrated more and more, the information of the \"basins\" becomes richer and richer with more identified attractors or with grid cells that belong to basins of already found attractors. Notice that only in the special method  basins_of_attraction(mapper::AttractorsViaRecurrences)  the information of the attraction or exit basins is utilized. In other functions like  basins_fractions  only the attractor locations are utilized, as the basins themselves are not stored. The iteration of a given initial condition continues until one of the following happens: The trajectory hits  mx_chk_fnd_att  times in a row grid cells previously visited:  it is considered that an attractor is found and is labelled with a new ID. Then,  iteration continues a bit more until we have identified the attractor with sufficient  accuracy, i.e., until  mx_chk_loc_att  cells with the new ID have been visited. The trajectory hits an already identified attractor  mx_chk_att  consecutive times:  the initial condition is numbered with the attractor's ID. The trajectory hits a known basin  mx_chk_hit_bas  times in a row: the initial condition  belongs to that basin and is numbered accordingly. Notice that basins are stored and  used only when  sparse = false . The trajectory spends  mx_chk_lost  steps outside the defined grid or the norm  of the integrator state becomes > than  horizon_limit : the initial  condition's label is set to  -1 . If none of the above happens, the initial condition is labelled  -1  after  and  mx_chk_safety  integrator steps. source"},{"id":330,"pagetitle":"Finding Attractors","title":"Attractors.automatic_Δt_basins","ref":"/attractors/stable/attractors/#Attractors.automatic_Δt_basins","content":" Attractors.automatic_Δt_basins  —  Function automatic_Δt_basins(ds::DynamicalSystem, grid; N = 5000) → Δt Calculate an optimal  Δt  value for  basins_of_attraction . This is done by evaluating the dynamic rule  f  (vector field) at  N  randomly chosen points of the grid. The average  f  is then compared with the diagonal length of a grid cell and their ratio provides  Δt . Notice that  Δt  should not be too small which happens typically if the grid resolution is high. It is okay for  basins_of_attraction  if the trajectory skips a few cells. But if  Δt  is too small the default values for all other keywords such as  mx_chk_hit_bas  need to be increased drastically. Also,  Δt  that is smaller than the internal step size of the integrator will cause a performance drop. source"},{"id":331,"pagetitle":"Finding Attractors","title":"Proximity","ref":"/attractors/stable/attractors/#Proximity","content":" Proximity"},{"id":332,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaProximity","ref":"/attractors/stable/attractors/#Attractors.AttractorsViaProximity","content":" Attractors.AttractorsViaProximity  —  Type AttractorsViaProximity(ds::DynamicalSystem, attractors::Dict [, ε]; kwargs...) Map initial conditions to attractors based on whether the trajectory reaches  ε -distance close to any of the user-provided  attractors . They have to be in a form of a dictionary mapping attractor labels to  StateSpaceSet s containing the attractors. The system gets stepped, and at each step the minimum distance to all attractors is computed. If any of these distances is  < ε , then the label of the nearest attractor is returned. If an  ε::Real  is  not  provided by the user, a value is computed automatically as half of the minimum distance between all attractors. This operation can be expensive for large attractor StateSpaceSets. If  length(attractors) == 1 , then  ε  becomes 1/10 of the diagonal of the box containing the attractor. If  length(attractors) == 1  and the attractor is a single point, an error is thrown. Because in this method the attractors are already known to the user, the method can also be called  supervised . Keywords Ttr = 100 : Transient time to first evolve the system for before checking for proximity. Δt = 1 : Step time given to  step! . horizon_limit = 1e3 : If the maximum distance of the trajectory from any of the given attractors exceeds this limit, it is assumed that the trajectory diverged (gets labelled as  -1 ). mx_chk_lost = 1000 : If the integrator has been stepped this many times without coming  ε -near to any attractor,  it is assumed that the trajectory diverged (gets labelled as  -1 ). source"},{"id":333,"pagetitle":"Finding Attractors","title":"Featurizing","ref":"/attractors/stable/attractors/#Featurizing","content":" Featurizing"},{"id":334,"pagetitle":"Finding Attractors","title":"Attractors.AttractorsViaFeaturizing","ref":"/attractors/stable/attractors/#Attractors.AttractorsViaFeaturizing","content":" Attractors.AttractorsViaFeaturizing  —  Type AttractorsViaFeaturizing(\n    ds::DynamicalSystem, featurizer::Function,\n    grouping_config = GroupViaClustering(); kwargs...\n) Initialize a  mapper  that maps initial conditions to attractors using a featurizing and grouping approach. This is a supercase of the featurizing and clustering approach that is utilized by bSTAB  (Stender and Hoffmann, 2021)  and MCBB  Gelbrecht2021 . See  AttractorMapper  for how to use the  mapper . This  mapper  also allows the syntax  mapper(u0)  but only if the  grouping_config  is  not GroupViaClustering . featurizer  is a function  f(A, t) that takes as an input an integrated trajectory A::StateSpaceSet and the corresponding time vector t and returns a vector v of features describing the trajectory. For better performance, it is strongly recommended that v  isa SVector{<:Real}`. grouping_config  is an instance of any subtype of  GroupingConfig  and decides how features will be grouped into attractors, see below. See also the intermediate functions  extract_features  and  group_features , which can be utilized when wanting to work directly with features. Keyword arguments T=100, Ttr=100, Δt=1 : Propagated to  trajectory . threaded = true : Whether to run the generation of features over threads by integrating trajectories in parallel. Description The trajectory  X  of an initial condition is transformed into features. Each feature is a number useful in  characterizing the attractor  the initial condition ends up at, and distinguishing it from other attractors. Example features are the mean or standard deviation of some the dimensions of the trajectory, the entropy of some of the dimensions, the fractal dimension of  X , or anything else you may fancy. All feature vectors (each initial condition = 1 vector) are then grouped using one of the sevaral available grouping configurations. Each group is assumed to be a unique attractor, and hence each initial condition is labelled according to the group it is part of. The method thus relies on the user having at least some basic idea about what attractors to expect in order to pick the right features, and the right way to group them, in contrast to  AttractorsViaRecurrences . source"},{"id":335,"pagetitle":"Finding Attractors","title":"Grouping configurations","ref":"/attractors/stable/attractors/#Grouping-configurations","content":" Grouping configurations"},{"id":336,"pagetitle":"Finding Attractors","title":"Grouping types","ref":"/attractors/stable/attractors/#Grouping-types","content":" Grouping types"},{"id":337,"pagetitle":"Finding Attractors","title":"Attractors.GroupingConfig","ref":"/attractors/stable/attractors/#Attractors.GroupingConfig","content":" Attractors.GroupingConfig  —  Type GroupingConfig Supertype for configuration structs on how to group features together. Used in several occasions such as  AttractorsViaFeaturizing  or  aggregate_attractor_fractions . Currently available grouping configurations are: GroupViaClustering GroupViaNearestFeature GroupViaHistogram GroupingConfig  defines an extendable interface. The only thing necessary for a new grouping configuration is to: Make a new type and subtype  GroupingConfig . If the grouping allows for mapping individual initial conditions to IDs, then instead extend the  internal function feature_to_group(feature, config) . This will allow doing  id = mapper(u0)  with  AttractorsViaFeaturizing . Else, extend the function  group_features(features, config) . You could still extend  group_features  even if (2.) is satisfied, if there are any performance benefits. Include the new grouping file in the  grouping/all_grouping_configs.jl  and list it in this documentation string. source"},{"id":338,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaClustering","ref":"/attractors/stable/attractors/#Attractors.GroupViaClustering","content":" Attractors.GroupViaClustering  —  Type GroupViaClustering(; kwargs...) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaClustering  clusters features into groups using DBSCAN, similar to the original work by bSTAB  (Stender and Hoffmann, 2021)  and MCBB  Gelbrecht2021 . Several options on clustering are available, see keywords below. The defaults are a significant improvement over existing literature, see Description. Keyword arguments clust_distance_metric = Euclidean() : A metric to be used in the clustering. It can be any function  f(a, b)  that returns the distance between real-valued vectors  a, b . All metrics from Distances.jl can be used here. rescale_features = true : if true, rescale each dimension of the extracted features separately into the range  [0,1] . This typically leads to more accurate clustering. min_neighbors = 10 : minimum number of neighbors (i.e. of similar features) each feature needs to have, including counting its own self, in order to be considered in a cluster (fewer than this, it is labeled as an outlier,  -1 ). use_mmap = false : whether to use an on-disk map for creating the distance matrix of the features. Useful when the features are so many where a matrix with side their length would not fit to memory. Keywords for optimal radius estimation optimal_radius_method::Union{Real, String} = \"silhouettes_optim\" : if a real number, it is the radius used to cluster features. Otherwise, it determines the method used to automatically determine that radius. Possible values are: \"silhouettes\" : Performs a linear (sequential) search for the radius that maximizes a   statistic of the silhouette values of clusters (typically the mean). This can be chosen   with  silhouette_statistic . The linear search may take some time to finish. To   increase speed, the number of radii iterated through can be reduced by decreasing    num_attempts_radius  (see its entry below). \"silhouettes_optim\" : Same as  \"silhouettes\"  but performs an optimized search via   Optim.jl. It's faster than  \"silhouettes\" , with typically the same accuracy (the   search here is not guaranteed to always find the global maximum, though it typically   gets close). \"knee\" : chooses the the radius according to the knee (a.k.a. elbow,   highest-derivative method) and is quicker, though generally leading to much worse   clustering. It requires that  min_neighbors  > 1. num_attempts_radius = 100 : number of radii that the  optimal_radius_method  will try out in its iterative procedure. Higher values increase the accuracy of clustering, though not necessarily much, while always reducing speed. silhouette_statistic::Function = mean : statistic (e.g. mean or minimum) of the silhouettes that is maximized in the \"optimal\" clustering. The original implementation in  (Stender and Hoffmann, 2021)  used the  minimum  of the silhouettes, and typically performs less accurately than the  mean . max_used_features = 0 : if not  0 , it should be an  Int  denoting the max amount of features to be used when finding the optimal radius. Useful when clustering a very large number of features (e.g., high accuracy estimation of fractions of basins of attraction). Description The DBSCAN clustering algorithm is used to automatically identify clusters of similar features. Each feature vector is a point in a feature space. Each cluster then basically groups points that are closely packed together. Closely packed means that the points have at least  min_neighbors  inside a ball of radius  optimal_radius  centered on them. This method typically works well if the radius is chosen well, which is not necessarily an easy task. Currently, three methods are implemented to automatically estimate an \"optimal\" radius. Estimating the optimal radius The default method is the  silhouettes method , which includes keywords  silhouette  and  silhouette_optim . Both of them search for the radius that optimizes the clustering, meaning the one that maximizes a statistic  silhouette_statistic  (e.g. mean value) of a quantifier for the quality of each cluster. This quantifier is the silhouette value of each identified cluster. A silhouette value measures how similar a point is to the cluster it currently belongs to, compared to the other clusters, and ranges from -1 (worst matching) to +1 (ideal matching). If only one cluster is found, the assigned silhouette is zero. So for each attempted radius in the search the clusters are computed, their silhouettes calculated, and the statistic of these silhouettes computed. The algorithm then finds the radius that leads to the maximum such statistic. For  optimal_radius_method = \"silhouettes\" , the search is done linearly, from a minimum to a maximum candidate radius for  optimal_radius_method = \"silhouettes\" ;  optimal_radius_method = silhouettes_optim , it is done via an optimized search performed by Optim.jl which is typically faster and with similar accuracy. A third alternative is the \"elbow\"  method, which works by calculating the distance of each point to its k-nearest-neighbors (with  k=min_neighbors ) and finding the distance corresponding to the highest derivative in the curve of the distances, sorted in ascending order. This distance is chosen as the optimal radius. It is described in  Ester1996  and  (Schubert  et al. , 2017) . It typically performs considerably worse than the  \"silhouette\"  methods. source"},{"id":339,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaHistogram","ref":"/attractors/stable/attractors/#Attractors.GroupViaHistogram","content":" Attractors.GroupViaHistogram  —  Type GroupViaHistogram(binning::FixedRectangularBinning) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaHistogram  performs a histogram in feature space. Then, all features that are in the same histogram bin get the same label. The  binning  is an instance of  FixedRectangularBinning  from ComplexityMeasures.jl. (the reason to not allow  RectangularBinning  is because during continuation we need to ensure that bins remain identical). source"},{"id":340,"pagetitle":"Finding Attractors","title":"Attractors.GroupViaNearestFeature","ref":"/attractors/stable/attractors/#Attractors.GroupViaNearestFeature","content":" Attractors.GroupViaNearestFeature  —  Type GroupViaNearestFeature(templates; kwargs...) Initialize a struct that contains instructions on how to group features in  AttractorsViaFeaturizing .  GroupViaNearestFeature  accepts a  template , which is a vector of features. Then, generated features from initial conditions in  AttractorsViaFeaturizing  are labelled according to the feature in  templates  that is closest (the label is the index of the closest template). templates  can be a vector or dictionary mapping keys to templates. Internally all templates are converted to  SVector  for performance. Hence, it is strongly recommended that both  templates  and the output of the  featurizer  function in  AttractorsViaFeaturizing  return  SVector  types. Keyword arguments metric = Euclidean() : metric to be used to quantify distances in the feature space. max_distance = Inf : Maximum allowed distance between a feature and its nearest template for it to be assigned to that template. By default,  Inf  guarantees that a feature is assigned to its nearest template regardless of the distance. Features that exceed  max_distance  to their nearest template get labelled  -1 . source"},{"id":341,"pagetitle":"Finding Attractors","title":"Grouping utility functions","ref":"/attractors/stable/attractors/#Grouping-utility-functions","content":" Grouping utility functions"},{"id":342,"pagetitle":"Finding Attractors","title":"Attractors.group_features","ref":"/attractors/stable/attractors/#Attractors.group_features","content":" Attractors.group_features  —  Function group_features(features, group_config::GroupingConfig) → labels Group the given vector of feature vectors according to the configuration and return the labels (vector of equal length as  features ). See  AttractorsViaFeaturizing  for possible configurations. source"},{"id":343,"pagetitle":"Finding Attractors","title":"Attractors.extract_features","ref":"/attractors/stable/attractors/#Attractors.extract_features","content":" Attractors.extract_features  —  Function extract_features(mapper, ics; N = 1000, show_progress = true) Return a vector of the features of each initial condition in  ics  (as in  basins_fractions ), using the configuration of  mapper::AttractorsViaFeaturizing . Keyword  N  is ignored if  ics isa StateSpaceSet . source"},{"id":346,"pagetitle":"Basins of Attraction","title":"Basins of Attraction","ref":"/attractors/stable/basins/#Basins-of-Attraction","content":" Basins of Attraction This page provides several functions related to the basins of attraction and their boundaries. It requires you to have first understood the  Finding Attractors  page."},{"id":347,"pagetitle":"Basins of Attraction","title":"Basins of attraction","ref":"/attractors/stable/basins/#Basins-of-attraction","content":" Basins of attraction Calculating basins of attraction, or their state space fractions, can be done with the functions: basins_fractions basins_of_attraction ."},{"id":348,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractions","ref":"/attractors/stable/basins/#Attractors.basins_fractions","content":" Attractors.basins_fractions  —  Function basins_fractions(\n    mapper::AttractorMapper,\n    ics::Union{StateSpaceSet, Function};\n    kwargs...\n) Approximate the state space fractions  fs  of the basins of attraction of a dynamical stystem by mapping initial conditions to attractors using  mapper  (which contains a reference to a  DynamicalSystem ). The fractions are simply the ratios of how many initial conditions ended up at each attractor. Initial conditions to use are defined by  ics . It can be: a  StateSpaceSet  of initial conditions, in which case all are used. a 0-argument function  ics()  that spits out random initial conditions. Then  N  random initial conditions are chosen. See  statespace_sampler  to generate such functions. Return The function will always return  fractions , which is a dictionary whose keys are the labels given to each attractor (always integers enumerating the different attractors), and whose values are the respective basins fractions. The label  -1  is given to any initial condition where  mapper  could not match to an attractor (this depends on the  mapper  type). If  ics  is a  StateSpaceSet  the function will also return  labels , which is  vector , of equal length to  ics , that contains the label each initial condition was mapped to. See  AttractorMapper  for all possible  mapper  types, and use  extract_attractors  (after calling  basins_fractions ) to extract the stored attractors from the  mapper . Keyword arguments N = 1000 : Number of random initial conditions to generate in case  ics  is a function. show_progress = true : Display a progress bar of the process. source basins_fractions(basins::AbstractArray) → fs::Dict Calculate the state space fraction of the basins of attraction encoded in  basins . The elements of  basins  are integers, enumerating the attractor that the entry of  basins  converges to (i.e., like the output of  basins_of_attraction ). Return a dictionary that maps attractor IDs to their relative fractions. In  (Menck  et al. , 2013)  the authors use these fractions to quantify the stability of a basin of attraction, and specifically how it changes when a parameter is changed. For this, see  continuation . source"},{"id":349,"pagetitle":"Basins of Attraction","title":"Attractors.extract_attractors","ref":"/attractors/stable/basins/#Attractors.extract_attractors","content":" Attractors.extract_attractors  —  Function extract_attractors(mapper::AttractorsMapper) → attractors Return a dictionary mapping label IDs to attractors found by the  mapper . This function should be called after calling  basins_fractions  with the given  mapper  so that the attractors have actually been found first. For  AttractorsViaFeaturizing , the attractors are only stored if the mapper was called with pre-defined initial conditions rather than a sampler (function returning initial conditions). source"},{"id":350,"pagetitle":"Basins of Attraction","title":"Attractors.basins_of_attraction","ref":"/attractors/stable/basins/#Attractors.basins_of_attraction","content":" Attractors.basins_of_attraction  —  Function basins_of_attraction(mapper::AttractorMapper, grid::Tuple) → basins, attractors Compute the full basins of attraction as identified by the given  mapper , which includes a reference to a  GeneralizedDynamicalSystem  and return them along with (perhaps approximated) found attractors. grid  is a tuple of ranges defining the grid of initial conditions that partition the state space into boxes with size the step size of each range. For example,  grid = (xg, yg)  where  xg = yg = range(-5, 5; length = 100) . The grid has to be the same dimensionality as the state space expected by the integrator/system used in  mapper . E.g., a  projected_integrator  could be used for lower dimensional projections, etc. A special case here is a  poincaremap  with  plane  being  Tuple{Int, <: Real} . In this special scenario the grid can be one dimension smaller than the state space, in which case the partitioning happens directly on the hyperplane the Poincaré map operates on. basins_of_attraction  function is a convenience 5-lines-of-code wrapper which uses the  labels  returned by  basins_fractions  and simply assings them to a full array corresponding to the state space partitioning indicated by  grid . source basins_of_attraction(mapper::AttractorsViaRecurrences; show_progress = true) This is a special method of  basins_of_attraction  that using recurrences does  exactly  what is described in the paper by Datseris & Wagemakers  Datseris2022 . By enforcing that the internal grid of  mapper  is the same as the grid of initial conditions to map to attractors, the method can further utilize found exit and attraction basins, making the computation faster as the grid is processed more and more. source"},{"id":351,"pagetitle":"Basins of Attraction","title":"StateSpaceSets.statespace_sampler","ref":"/attractors/stable/basins/#StateSpaceSets.statespace_sampler","content":" StateSpaceSets.statespace_sampler  —  Function statespace_sampler(region [, seed = 42]) → sampler, isinside A function that facilitates sampling points randomly and uniformly in a state space  region . It generates two functions: sampler  is a 0-argument function that when called generates a random point inside a state space  region . The point is always a  Vector  for type stability irrespectively of dimension. Generally, the generated point should be  copied  if it needs to be stored. (i.e., calling  sampler()  utilizes a shared vector)  sampler  is a thread-safe function. isinside  is a 1-argument function that returns  true  if the given state space point is inside the  region . The  region  can be an instance of any of the following types (input arguments if not specified are vectors of length  D , with  D  the state space dimension): HSphere(radius::Real, center) : points  inside  the hypersphere (boundary excluded). Convenience method  HSphere(radius::Real, D::Int)  makes the center a  D -long vector of zeros. HSphereSurface(radius, center) : points on the hypersphere surface. Same convenience method as above is possible. HRectangle(mins, maxs) : points in [min, max) for the bounds along each dimension. The random number generator is always  Xoshiro  with the given  seed . statespace_sampler(grid::NTuple{N, AbstractRange} [, seed]) If given a  grid  that is a tuple of  AbstractVector s, the minimum and maximum of the vectors are used to make an  HRectangle  region."},{"id":352,"pagetitle":"Basins of Attraction","title":"Final state sensitivity / fractal boundaries","ref":"/attractors/stable/basins/#Final-state-sensitivity-/-fractal-boundaries","content":" Final state sensitivity / fractal boundaries Several functions are provided related with analyzing the fractality of the boundaries of the basins of attraction: basins_fractal_dimension basin_entropy basins_fractal_test uncertainty_exponent"},{"id":353,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractal_dimension","ref":"/attractors/stable/basins/#Attractors.basins_fractal_dimension","content":" Attractors.basins_fractal_dimension  —  Function basins_fractal_dimension(basins; kwargs...) -> V_ε, N_ε, d Estimate the fractal dimension  d  of the boundary between basins of attraction using a box-counting algorithm for the boxes that contain at least two different basin IDs. Keyword arguments range_ε = 2:maximum(size(basins))÷20  is the range of sizes of the box to test (in pixels). Description The output  N_ε  is a vector with the number of the balls of radius  ε  (in pixels) that contain at least two initial conditions that lead to different attractors.  V_ε  is a vector with the corresponding size of the balls. The ouput  d  is the estimation of the box-counting dimension of the boundary by fitting a line in the  log.(N_ε)  vs  log.(1/V_ε)  curve. However it is recommended to analyze the curve directly for more accuracy. It is the implementation of the popular algorithm of the estimation of the box-counting dimension. The algorithm search for a covering the boundary with  N_ε  boxes of size  ε  in pixels. source"},{"id":354,"pagetitle":"Basins of Attraction","title":"Attractors.basin_entropy","ref":"/attractors/stable/basins/#Attractors.basin_entropy","content":" Attractors.basin_entropy  —  Function basin_entropy(basins::Array, ε = 20) -> Sb, Sbb Compute the basin entropy  (Daza  et al. , 2016) Sb  and basin boundary entropy  Sbb  of the given  basins  of attraction by considering  ε  boxes along each dimension. Description First, the input  basins  is divided regularly into n-dimensional boxes of side  ε  (along all dimensions). Then  Sb  is simply the average of the Gibbs entropy computed over these boxes. The function returns the basin entropy  Sb  as well as the boundary basin entropy  Sbb . The later is the average of the entropy only for boxes that contains at least two different basins, that is, for the boxes on the boundaries. The basin entropy is a measure of the uncertainty on the initial conditions of the basins. It is maximum at the value  log(n_att)  being  n_att  the number of attractors. In this case the boundary is intermingled: for a given initial condition we can find another initial condition that lead to another basin arbitriraly close. It provides also a simple criterion for fractality: if the boundary basin entropy  Sbb  is above  log(2)  then we have a fractal boundary. It doesn't mean that basins with values below cannot have a fractal boundary, for a more precise test see  basins_fractal_test . An important feature of the basin entropy is that it allows comparisons between different basins using the same box size  ε . source"},{"id":355,"pagetitle":"Basins of Attraction","title":"Attractors.basins_fractal_test","ref":"/attractors/stable/basins/#Attractors.basins_fractal_test","content":" Attractors.basins_fractal_test  —  Function basins_fractal_test(basins; ε = 20, Ntotal = 1000) -> test_res, Sbb Perform an automated test to decide if the boundary of the basins has fractal structures based on the method of Puy et al.  (Puy  et al. , 2021) . Return  test_res  ( :fractal  or  :smooth ) and the mean basin boundary entropy. Keyword arguments ε = 20 : size of the box to compute the basin boundary entropy. Ntotal = 1000 : number of balls to test in the boundary for the computation of  Sbb Description The test \"looks\" at the basins with a magnifier of size  ε  at random. If what we see in the magnifier looks like a smooth boundary (onn average) we decide that the boundary is smooth. If it is not smooth we can say that at the scale  ε  we have structures, i.e., it is fractal. In practice the algorithm computes the boundary basin entropy  Sbb basin_entropy  for  Ntotal  random boxes of radius  ε . If the computed value is equal to theoretical value of a smooth boundary (taking into account statistical errors and biases) then we decide that we have a smooth boundary. Notice that the response  test_res  may depend on the chosen ball radius  ε . For larger size, we may observe structures for smooth boundary and we obtain a  different  answer. The output  test_res  is a symbol describing the nature of the basin and the output  Sbb  is the estimated value of the boundary basin entropy with the sampling method. source"},{"id":356,"pagetitle":"Basins of Attraction","title":"Attractors.uncertainty_exponent","ref":"/attractors/stable/basins/#Attractors.uncertainty_exponent","content":" Attractors.uncertainty_exponent  —  Function uncertainty_exponent(basins; kwargs...) -> ε, N_ε, α Estimate the uncertainty exponent Grebogi1983  of the basins of attraction. This exponent is related to the final state sensitivity of the trajectories in the phase space. An exponent close to  1  means basins with smooth boundaries whereas an exponent close to  0  represent completely fractalized basins, also called riddled basins. The output  N_ε  is a vector with the number of the balls of radius  ε  (in pixels) that contain at least two initial conditions that lead to different attractors. The ouput  α  is the estimation of the uncertainty exponent using the box-counting dimension of the boundary by fitting a line in the  log.(N_ε)  vs  log.(1/ε)  curve. However it is recommended to analyze the curve directly for more accuracy. Keyword arguments range_ε = 2:maximum(size(basins))÷20  is the range of sizes of the ball to test (in pixels). Description A phase space with a fractal boundary may cause a uncertainty on the final state of the dynamical system for a given initial condition. A measure of this final state sensitivity is the uncertainty exponent. The algorithm probes the basin of attraction with balls of size  ε  at random. If there are a least two initial conditions that lead to different attractors, a ball is tagged \"uncertain\".  f_ε  is the fraction of \"uncertain balls\" to the total number of tries in the basin. In analogy to the fractal dimension, there is a scaling law between,  f_ε ~ ε^α . The number that characterizes this scaling is called the uncertainty exponent  α . Notice that the uncertainty exponent and the box counting dimension of the boundary are related. We have  Δ₀ = D - α  where  Δ₀  is the box counting dimension computed with  basins_fractal_dimension  and  D  is the dimension of the phase space. The algorithm first estimates the box counting dimension of the boundary and returns the uncertainty exponent. source"},{"id":357,"pagetitle":"Basins of Attraction","title":"Tipping points","ref":"/attractors/stable/basins/#Tipping-points","content":" Tipping points This page discusses functionality related with tipping points in dynamical systems with known rule. If instead you are interested in identifying tipping points in measured timeseries, have a look at  TransitionIndicators.jl ."},{"id":358,"pagetitle":"Basins of Attraction","title":"Attractors.tipping_probabilities","ref":"/attractors/stable/basins/#Attractors.tipping_probabilities","content":" Attractors.tipping_probabilities  —  Function tipping_probabilities(basins_before, basins_after) → P Return the tipping probabilities of the computed basins before and after a change in the system parameters (or time forcing), according to the definition of  Kaszás2019 . The input  basins  are integer-valued arrays, where the integers enumerate the attractor, e.g. the output of  basins_of_attraction . Description Let  $\\mathcal{B}_i(p)$  denote the basin of attraction of attractor  $A_i$  at parameter(s)  $p$ . Kaszás et al  Kaszás2019  define the tipping probability from  $A_i$  to  $A_j$ , given a parameter change in the system of  $p_- \\to p_+$ , as \\[P(A_i \\to A_j | p_- \\to p_+) =\n\\frac{|\\mathcal{B}_j(p_+) \\cap \\mathcal{B}_i(p_-)|}{|\\mathcal{B}_i(p_-)|}\\] where  $|\\cdot|$  is simply the volume of the enclosed set. The value of  $P(A_i \\to A_j | p_- \\to p_+)$  is  P[i, j] . The equation describes something quite simple: what is the overlap of the basin of attraction of  $A_i$  at  $p_-$  with that of the attractor  $A_j$  at  $p_+$ . If  basins_before, basins_after  contain values of  -1 , corresponding to trajectories that diverge, this is considered as the last attractor of the system in  P . source"},{"id":359,"pagetitle":"Basins of Attraction","title":"Mimimal Fatal Shock","ref":"/attractors/stable/basins/#Mimimal-Fatal-Shock","content":" Mimimal Fatal Shock The algorithm to find minimal perturbation for arbitrary initial condition  u0  which will kick the system into different from the current basin. "},{"id":360,"pagetitle":"Basins of Attraction","title":"Attractors.minimal_fatal_shock","ref":"/attractors/stable/basins/#Attractors.minimal_fatal_shock","content":" Attractors.minimal_fatal_shock  —  Function minimal_fatal_shock(mapper::AttractorMapper, u0, search_area, algorithm) → mfs Return the minimal fatal shock  mfs  for the initial point  u0  according to the specified  algorithm  given a  mapper  that satisfies the  id = mapper(u0)  interface (see  AttractorMapper  if you are not sure which mappers do that). The  mapper  contains a reference to a  DynamicalSystem . The options for  algorithm  are:  MFSBruteForce  or  MFSBlackBoxOptim . Forh high dimensional systems  MFSBlackBoxOptim  is likely more accurate. The  search_area  dictates the state space range for the search of the  mfs . It can be a 2-tuple of (min, max) values, in which case the same values are used for each dimension of the system in  mapper . Otherwise, it can be a vector of 2-tuples, each for each dimension of the system. The search area is defined w.r.t. to  u0  (i.e., it is the search area for perturbations of  u0 ). Description The minimal fatal shock is defined as the smallest (smallest norm) perturbation of the initial point  u0  that will lead it a different basin of attraction. It is inspired by the paper \"Minimal fatal shocks in multistable complex networks\"  (Halekotte and Feudel, 2020) , however the implementation here is generic: it works for  any  dynamical system. source Missing docstring. Missing docstring for  MFSBlackBox . Check Documenter's build log for details."},{"id":361,"pagetitle":"Basins of Attraction","title":"Attractors.MFSBruteForce","ref":"/attractors/stable/basins/#Attractors.MFSBruteForce","content":" Attractors.MFSBruteForce  —  Type MFSBruteForce(; kwargs...) The brute force randomized search algorithm used in  minimal_fatal_shock . It consists of two steps: random initialization and sphere radius reduction. On the first step, the algorithm generates random pertubations within the search area and records the perturbation that leads to a different basin but with the smallest magnitude. With this obtained pertubation it proceeds to the second step. On the second step, the algorithm generates random pertubations on the surface of the hypersphere with radius equal to the norm of the pertubation found in the first step. It reduces the radius of the hypersphere and continues searching for the better result with a smaller radius. Each time a better result is found, the radius is reduced further. The algorithm records the perturbation with smallest radius that leads to a different basin. Keyword arguments initial_iterations = 10000 : number of random pertubations to try in the first step of the algorithm. sphere_iterations = 10000 : number of steps while initializing random points on hypersphere and decreasing its radius. sphere_decrease_factor = 0.999  factor by which the radius of the hypersphere is decreased (at each step the radius is multiplied by this number). Number closer to 1 means more refined accuracy source"},{"id":364,"pagetitle":"Attractor & Basins Continuation","title":"Attractor & Basins Continuation","ref":"/attractors/stable/continuation/#Attractor-and-Basins-Continuation","content":" Attractor & Basins Continuation"},{"id":365,"pagetitle":"Attractor & Basins Continuation","title":"A new kind of continuation","ref":"/attractors/stable/continuation/#A-new-kind-of-continuation","content":" A new kind of continuation If you have heard before the word \"continuation\", then you are likely aware of the  traditional continuation-based bifurcation analysis (CBA)  offered by many software, such as AUTO, MatCont, and in Julia  BifurcationKit.jl . Here we offer a completely different kind of continuation called  attractors & basins continuation . A direct comparison of the two approaches is not truly possible, because they do different things. The traditional linearized continuation analysis continues the curves of individual fixed points across the joint state-parameter space. The attractor and basins continuation first finds all attractors at all parameter values and then  matches  appropriately similar attractors across different parameters, giving the illusion of continuing them individually. Additionally, the curves of stable fixed points in the joint parameter space is only a small by-product of the attractor basins continuation, and the main information is the basin fractions and how these change in the parameter space. A more detailed comparison between these two fundamentally different approaches in is given in high detail in our  paper ."},{"id":366,"pagetitle":"Attractor & Basins Continuation","title":"Continuation API","ref":"/attractors/stable/continuation/#Continuation-API","content":" Continuation API"},{"id":367,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.continuation","ref":"/attractors/stable/continuation/#Attractors.continuation","content":" Attractors.continuation  —  Function continuation(abc::AttractorsBasinsContinuation, prange, pidx, ics; kwargs...) Find and continue attractors (or feature-based representations of attractors) and the fractions of their basins of attraction across a parameter range.  continuation  is the central function of the framework for global stability analysis illustrated in  (Datseris  et al. , 2023) . The continuation type  abc  is a subtype of  AttractorsBasinsContinuation  and contains an  AttractorMapper . The mapper contains information on how to find the attractors and basins of a dynamical system. Additional arguments and keyword arguments given when creating  abc  further tune the continuation and how attractors are matched across different parameter values. The basin fractions and the attractors (or some representation of them) are continued across the parameter range  prange , for the parameter of the system with index  pidx  (any index valid in  set_parameter!  can be used). ics  is a 0-argument function generating initial conditions for the dynamical system (as in  basins_fractions ). Possible subtypes of  AttractorsBasinsContinuation  are: RecurrencesFindAndMatch FeaturizeGroupAcrossParameter Return fractions_curves::Vector{Dict{Int, Float64}} . The fractions of basins of attraction.  fractions_curves[i]  is a dictionary mapping attractor IDs to their basin fraction at the  i -th parameter. attractors_info::Vector{Dict{Int, <:Any}} . Information about the attractors.  attractors_info[i]  is a dictionary mapping attractor ID to information about the attractor at the  i -th parameter. The type of information stored depends on the chosen continuation type. Keyword arguments show_progress = true : display a progress bar of the computation. samples_per_parameter = 100 : amount of initial conditions sampled at each parameter from  ics . source"},{"id":368,"pagetitle":"Attractor & Basins Continuation","title":"Recurrences continuation (best)","ref":"/attractors/stable/continuation/#Recurrences-continuation-(best)","content":" Recurrences continuation (best)"},{"id":369,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.RecurrencesFindAndMatch","ref":"/attractors/stable/continuation/#Attractors.RecurrencesFindAndMatch","content":" Attractors.RecurrencesFindAndMatch  —  Type RecurrencesFindAndMatch <: AttractorsBasinsContinuation\nRecurrencesFindAndMatch(mapper::AttractorsViaRecurrences; kwargs...) A method for  continuation  as in  (Datseris  et al. , 2023)  that is based on the recurrences-based algorithm for finding attractors ( AttractorsViaRecurrences ) and the \"matching attractors\" functionality offered by  match_statespacesets! . You can use  RAFM  as an alias. Description At the first parameter slice of the continuation process, attractors and their fractions are found as described in the  AttractorsViaRecurrences  mapper using recurrences in state space. At each subsequent parameter slice, new attractors are found by seeding initial conditions from the previously found attractors and then running these initial conditions through the recurrences algorithm of the  mapper . Seeding initial conditions close to previous attractors accelerates the main bottleneck of  AttractorsViaRecurrences , which is finding the attractors. After the special initial conditions are mapped to attractors, attractor basin fractions are computed by sampling random initial conditions. (using the provided  sampler  in  continuation ) and mapping them to attractors using the  AttractorsViaRecurrences  mapper. I.e., exactly as in  basins_fractions . Naturally, during this step new attractors may be found, besides those found using the \"seeding from previous attractors\". Once the basins fractions are computed, the parameter is incremented again and we perform the steps as before. This process continues for all parameter values. After all parameters are exhausted, the newly found attractors (and their fractions) are \"matched\" to the previous ones. I.e., their  IDs are changed , so that attractors with closest distance to those at a previous parameter get assigned the same ID. Matching is rather sophisticated and is described in  match_statespacesets!  and  match_continuation! . Typically, the matching process matches attractor IDs that are closest in state space distance, but more options are possible, see  match_statespacesets! . Note that in this continuation the finding-attractors and matching-attractors steps are completely independent. This means, that if you don't like the initial outcome of the matching process, you may call  match_continuation!  again on the outcome with (possibly different) matching-related keywords. Keyword arguments distance, threshold, use_vanished : propagated to  match_continuation! . info_extraction = identity : A function that takes as an input an attractor ( StateSpaceSet ) and outputs whatever information should be stored. It is used to return the  attractors_info  in  continuation . Note that the same attractors that are stored in  attractors_info  are also used to perform the matching in  match_continuation! , hence this keyword should be altered with care. seeds_from_attractor : A function that takes as an input an attractor and returns an iterator of initial conditions to be seeded from the attractor for the next parameter slice. By default, we sample only the first stored point on the attractor. source"},{"id":370,"pagetitle":"Attractor & Basins Continuation","title":"Matching attractors","ref":"/attractors/stable/continuation/#Matching-attractors","content":" Matching attractors"},{"id":371,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_statespacesets!","ref":"/attractors/stable/continuation/#Attractors.match_statespacesets!","content":" Attractors.match_statespacesets!  —  Function match_statespacesets!(a₊::AbstractDict, a₋; distance = Centroid(), threshold = Inf) Given dictionaries  a₊, a₋  mapping IDs to  StateSpaceSet  instances, match the IDs in dictionary  a₊  so that its sets that are the closest to those in dictionary  a₋  get assigned the same key as in  a₋ . Typically the +,- mean after and before some change of parameter of a system. Return the replacement map, a dictionary mapping old keys of  a₊  to the new ones that they were mapped to. You can obtain this map, without modifying the dictionaries, by calling the  replacement_map  function directly. Keyword arguments distance = Centroid() : given to  setsofsets_distances . threshold = Inf : attractors with distance larger than the  threshold  are guaranteed to not be mapped to each other. Description When finding attractors and their fractions in Attractors.jl, different attractors get assigned different IDs. However which attractor gets which ID is somewhat arbitrary. Finding the attractors of the same system for slightly different parameters could label \"similar\" attractors (at the different parameters) with different IDs.  match_statespacesets!  tries to \"match\" them by modifying the IDs, i.e., the keys of the given dictionaries. Do note however that there is nothing in this function that is limited to attractors in the formal mathematical sense. Any dictionary with  StateSpaceSet  values is a valid input and these sets may represent attractors, trajectories, group of features, or anything else. The matching happens according to the output of the  setsofsets_distances  function with the keyword  distance .  distance  can be whatever that function accepts, i.e., one of  Centroid, Hausdorff, StrictlyMinimumDistance  or any arbitrary user- provided function that given two sets it returns a positive number (their distance). State space sets are then matched according to this distance. First, all possible pairs (old, new, distance) are sorted according to their distance. The pair with smallest distance is matched. Sets in matched pairs are removed from the matching pool to ensure a unique mapping. Then, the next pair with least remaining distance is matched, and the process repeats until all pairs are exhausted. Additionally, you can provide a  threshold  value. If the distance between two attractors is larger than this  threshold , then it is guaranteed that the attractors will get assigned different key in the dictionary  a₊  (which is the next available integer). source"},{"id":372,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.replacement_map","ref":"/attractors/stable/continuation/#Attractors.replacement_map","content":" Attractors.replacement_map  —  Function replacement_map(a₊, a₋; distance = Centroid(), threshold = Inf) → rmap Return a dictionary mapping keys in  a₊  to new keys in  a₋ , as explained in  match_statespacesets! . source"},{"id":373,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.set_distance","ref":"/attractors/stable/continuation/#StateSpaceSets.set_distance","content":" StateSpaceSets.set_distance  —  Function set_distance(ssset1, ssset2 [, distance]) Calculate a distance between two  StateSpaceSet s, i.e., a distance defined between sets of points, as dictated by  distance . Possible  distance  types are: Centroid , which is the default, and 100s of times faster than the rest Hausdorff StrictlyMinimumDistance"},{"id":374,"pagetitle":"Attractor & Basins Continuation","title":"StateSpaceSets.setsofsets_distances","ref":"/attractors/stable/continuation/#StateSpaceSets.setsofsets_distances","content":" StateSpaceSets.setsofsets_distances  —  Function setsofsets_distances(a₊, a₋ [, distance]) → distances Calculate distances between sets of  StateSpaceSet s. Here   a₊, a₋  are containers of  StateSpaceSet s, and the returned distances are dictionaries of distances. Specifically,  distances[i][j]  is the distance of the set in the  i  key of  a₊  to the  j  key of  a₋ . Notice that distances from  a₋  to  a₊  are not computed at all (assumming symmetry in the distance function). The  distance  can be as in  set_distance , or it can be an arbitrary function that takes as input two state space sets and returns any positive-definite number as their \"distance\"."},{"id":375,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_continuation!","ref":"/attractors/stable/continuation/#Attractors.match_continuation!","content":" Attractors.match_continuation!  —  Function match_continuation!(fractions_curves::Vector{<:Dict}, attractors_info::Vector{<:Dict}; kwargs...) Loop over all entries in the given arguments (which are typically the direct outputs of  continuation  with  RecurrencesFindAndMatch ), and match the attractor IDs in both the attractors container and the basins fractions container. This means that we loop over each entry of the vectors (skipping the first), and in each entry we attempt to match the dictionary keys to the keys of the previous dictionary using  match_statespacesets . The keywords  distance, threshold  are propagated to  match_statespacesets . However, there is a unique keyword for  match_continuation! :  use_vanished::Bool . If  true , then attractors that existed before but have vanished are kept in \"memory\" when it comes to matching: the new attractors are compared to the latest istance of all attractors that have ever existed, and get match to their closest ones as per  match_statespacesets! . If  false , vanished attractors are ignored. Note that in this case new attractors that cannot be matched to any previous attractors will get an appropriately incremented ID. E.g., if we started with three attractors, and attractor 3 vanished, and at some later parameter value we again have three attractors, the new third attractor will  not  have ID 3, but 4 (i.e., the next available ID). By default  use_vanished = !isinf(threshold)  and since the default value for  threshold  is  Inf ,  use_vanished  is  false . The last keyword is  retract_keys = true  which will \"retract\" keys (i.e., make the integers smaller integers) so that all unique IDs are the 1-incremented positive integers. E.g., if the IDs where 1, 6, 8, they will become 1, 2, 3. The special id -1 is unaffected by this. rematch_continuation!(attractors_info::Vector{<:Dict}; kwargs...) This is a convenience method that only uses and modifies the state space set dictionary container without the need for a basins fractions container. source"},{"id":376,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.match_basins_ids!","ref":"/attractors/stable/continuation/#Attractors.match_basins_ids!","content":" Attractors.match_basins_ids!  —  Function match_basins_ids!(b₊::AbstractArray, b₋; threshold = Inf) Similar to  match_statespacesets!  but operate on basin arrays instead (the arrays typically returned by  basins_of_attraction ). This method matches IDs of attractors whose basins of attraction before and after  b₋,b₊  have the most overlap (in pixels). This overlap is normalized in 0-1 (with 1 meaning 100% overlap of pixels). The  threshold  in this case is compared to the inverse of the overlap (so, for  threshold = 2  attractors that have less than 50% overlap get different IDs guaranteed). source"},{"id":377,"pagetitle":"Attractor & Basins Continuation","title":"Aggregating attractors and fractions","ref":"/attractors/stable/continuation/#Aggregating-attractors-and-fractions","content":" Aggregating attractors and fractions"},{"id":378,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.aggregate_attractor_fractions","ref":"/attractors/stable/continuation/#Attractors.aggregate_attractor_fractions","content":" Attractors.aggregate_attractor_fractions  —  Function aggregate_attractor_fractions(\n    fractions_curves, attractors_info, featurizer, group_config [, info_extraction]\n) Aggregate the already-estimated curves of fractions of basins of attraction of similar attractors using the same pipeline used by  GroupingConfig . The most typical application of this function is to transform the output of  RecurrencesFindAndMatch  so that similar attractors, even across parameter space, are grouped into one \"attractor\". Thus, the fractions of their basins are aggregated. You could also use this function to aggregate attractors and their fractions even in a single parameter configuration, i.e., using the output of  basins_fractions . This function is useful in cases where you want the accuracy and performance of  AttractorsViaRecurrences , but you also want the convenience of \"grouping\" similar attractrors like in  AttractorsViaFeaturizing  for presentation or analysis purposes. For example, a high dimesional model of competition dynamics across multispecies may have extreme multistability. After finding this multistability however, one may care about aggregating all attractors into two groups: where a given species is extinct or not. This is the example highlighted in our documentation, in  Extinction of a species in a multistable competition model . Input fractions_curves : a vector of dictionaries mapping labels to basin fractions. attractors_info : a vector of dictionaries mapping labels to attractors. 1st and 2nd argument are exactly like the return values of  continuation  with  RecurrencesFindAndMatch  (or, they can be the return of  basins_fractions ). featurizer : a 1-argument function to map an attractor into an appropriate feature to be grouped later. Features expected by  GroupingConfig  are  SVector . group_config : a subtype of  GroupingConfig . info_extraction : a function accepting a vector of features and returning a description of the features. I.e., exactly as in  FeaturizeGroupAcrossParameter . The 5th argument is optional and defaults to the centroid of the features. Return aggregated_fractions : same as  fractions_curves  but now contains the fractions of the aggregated attractors. aggregated_info : dictionary mapping the new labels of  aggregated_fractions  to the extracted information using  info_extraction . Clustering attractors directly (this is rather advanced) You may also use the DBSCAN clustering approach here to group attractors based on their state space distance (the  set_distance ) by making a distance matrix as expected by the DBSCAN implementation. For this, use  identity  as  featurizer , and choose  GroupViaClustering  as the  group_config  with  clust_distance_metric = set_distance  and provide a numerical value for  optimal_radius_method  when initializing the  GroupViaClustering , and also, for the  info_extraction  argument, you now need to provide a function that expects a  vector of  StateSpaceSet s  and outputs a descriptor. E.g.,  info_extraction = vector -> mean(mean(x) for x in vector) . source"},{"id":379,"pagetitle":"Attractor & Basins Continuation","title":"Grouping continuation","ref":"/attractors/stable/continuation/#Grouping-continuation","content":" Grouping continuation"},{"id":380,"pagetitle":"Attractor & Basins Continuation","title":"Attractors.FeaturizeGroupAcrossParameter","ref":"/attractors/stable/continuation/#Attractors.FeaturizeGroupAcrossParameter","content":" Attractors.FeaturizeGroupAcrossParameter  —  Type FeaturizeGroupAcrossParameter <: AttractorsBasinsContinuation\nFeaturizeGroupAcrossParameter(mapper::AttractorsViaFeaturizing; kwargs...) A method for  continuation . It uses the featurizing approach discussed in  AttractorsViaFeaturizing  and hence requires an instance of that mapper as an input. When used in  continuation , features are extracted and then grouped across a parameter range. Said differently, all features of all initial conditions across all parameter values are put into the same \"pool\" and then grouped as dictated by the  group_config  of the mapper. After the grouping is finished the feature label fractions are distributed to each parameter value they came from. Keyword arguments info_extraction::Function  a function that takes as an input a vector of feature-vectors (corresponding to a cluster) and returns a description of the cluster. By default, the centroid of the cluster is used. par_weight = 0 : See below the section on MCBB. MCBB special version If the chosen grouping method is  GroupViaClustering , the additional keyword  par_weight::Real  can be used. If it is ≠ 0, the distance matrix between features obtains an extra weight that is proportional to the distance  par_weight*|p[i] - p[j]|  between the parameters used when extracting features. The range of parameters is normalized to 0-1 such that the largest distance in the parameter space is 1. The normalization is done because the feature space is also (by default) normalized to 0-1. This version of the algorithm is the original \"MCBB\" continuation method described in  (Gelbrecht  et al. , 2020) , besides the improvements of clustering accuracy and performance done by the developer team of Attractors.jl. source"},{"id":383,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystem  reference","ref":"/attractors/stable/dynsysref/#DynamicalSystem-reference","content":" DynamicalSystem  reference This page is a convenience reference to some of the contents of the DynamicalSystemsBase.jl package (one of the core modules of DynamicalSystems.jl)."},{"id":384,"pagetitle":"DynamicalSystem reference","title":"Dynamical systems","ref":"/attractors/stable/dynsysref/#Dynamical-systems","content":" Dynamical systems The kinds of dynamical systems that can be used in Attractors.jl are listed below for reference DynamicalSystem DeterministicIteratedMap CoupledODEs StroboscopicMap PoincareMap ProjectedDynamicalSystem ArbitrarySteppable"},{"id":385,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.DynamicalSystem","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.DynamicalSystem","content":" DynamicalSystemsBase.DynamicalSystem  —  Type DynamicalSystem DynamicalSystem  is an abstract supertype encompassing all concrete implementations of what counts as a \"dynamical system\" in the DynamicalSystems.jl library. All concrete implementations of  DynamicalSystem  can be iteratively evolved in time via the  step!  function.  Hence, most library functions that evolve the system will mutate its current state and/or parameters. See the documentation online for implications this has on for parallelization. DynamicalSystem  is further separated into two abstract types:  ContinuousTimeDynamicalSystem, DiscreteTimeDynamicalSystem . The simplest and most common concrete implementations of a  DynamicalSystem  are  DeterministicIteratedMap  or  CoupledODEs . Description Note The documentation of  DynamicalSystem  follows chapter 1 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. A  ds::DynamicalSystem representes a flow Φ in a state space . It mainly encapsulates three things: A state, typically referred to as  u , with initial value  u0 . The space that  u  occupies is the state space of  ds  and the length of  u  is the dimension of  ds  (and of the state space). A dynamic rule, typically referred to as  f , that dictates how the state evolves/changes with time when calling the  step!  function.  f  is a standard Julia function, see below. A parameter container  p  that parameterizes  f .  p  can be anything, but in general it is recommended to be a type-stable mutable container. In sort, any set of quantities that change in time can be considered a dynamical system, however the concrete subtypes of  DynamicalSystem  are much more specific in their scope. Concrete subtypes typically also contain more information than the above 3 items. In this scope dynamical systems have a known dynamic rule  f  defined as a standard Julia function.  Observed  or  measured  data from a dynamical system are represented using  StateSpaceSet  and are finite. Such data are obtained from the  trajectory  function or from an experimental measurement of a dynamical system with an unknown dynamic rule. Construction instructions on  f  and  u Most of the concrete implementations of  DynamicalSystem , with the exception of  ArbitrarySteppable , have two ways of implementing the dynamic rule  f , and as a consequence the type of the state  u . The distinction is done on whether  f  is defined as an in-place (iip) function or out-of-place (oop) function. oop  :  f must  be in the form  f(u, p, t) -> out    which means that given a state  u::SVector{<:Real}  and some parameter container    p  it returns the output of  f  as an  SVector{<:Real}  (static vector). iip  :  f must  be in the form  f!(out, u, p, t)    which means that given a state  u::AbstractArray{<:Real}  and some parameter container  p ,   it writes in-place the output of  f  in  out::AbstractArray{<:Real} .   The function  must  return  nothing  as a final statement. t  stands for current time in both cases.  iip  is suggested for systems with high dimension and  oop  for small. The break-even point is between 10 to 100 dimensions but should be benchmarked on a case-by-case basis as it depends on the complexity of  f . Autonomous vs non-autonomous systems Whether the dynamical system is autonomous ( f  doesn't depend on time) or not, it is still necessary to include  t  as an argument to  f . Some algorithms utilize this information, some do not, but we prefer to keep a consistent interface either way. You can also convert any system to autonomous by making time an additional variable. If the system is non-autonomous, its  effective dimensionality  is  dimension(ds)+1 . API The API that the interface of  DynamicalSystem  employs is the functions listed below. Once a concrete instance of a subtype of  DynamicalSystem  is obtained, it can quieried or altered with the following functions. The main use of a concrete dynamical system instance is to provide it to downstream functions such as  lyapunovspectrum  from ChaosTools.jl or  basins_of_attraction  from Attractors.jl. A typical user will likely not utilize directly the following API, unless when developing new algorithm implementations that use dynamical systems. API - information ds(t)  with  ds  an instance of  DynamicalSystem : return the state of  ds  at time  t . For continuous time systems this interpolates and extrapolates, while for discrete time systems it only works if  t  is the current time. current_state initial_state current_parameters initial_parameters isdeterministic isdiscretetime dynamic_rule current_time initial_time isinplace succesful_step API - alter status reinit! set_state! set_parameter! set_parameters!"},{"id":386,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.DeterministicIteratedMap","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.DeterministicIteratedMap","content":" DynamicalSystemsBase.DeterministicIteratedMap  —  Type DeterministicIteratedMap <: DynamicalSystem\nDeterministicIteratedMap(f, u0, p = nothing; t0 = 0) A deterministic discrete time dynamical system defined by an iterated map as follows: \\[\\vec{u}_{n+1} = \\vec{f}(\\vec{u}_n, p, n)\\] An alias for  DeterministicIteratedMap  is  DiscreteDynamicalSystem . Optionally configure the parameter container  p  and initial time  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem ."},{"id":387,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.CoupledODEs","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.CoupledODEs","content":" DynamicalSystemsBase.CoupledODEs  —  Type CoupledODEs <: ContinuousTimeDynamicalSystem\nCoupledODEs(f, u0 [, p]; diffeq, t0 = 0.0) A deterministic continuous time dynamical system defined by a set of coupled ordinary differential equations as follows: \\[\\frac{d\\vec{u}}{dt} = \\vec{f}(\\vec{u}, p, t)\\] An alias for  CoupledODE  is  ContinuousDynamicalSystem . Optionally provide the parameter container  p  and initial time as keyword  t0 . For construction instructions regarding  f, u0  see  DynamicalSystem . DifferentialEquations.jl keyword arguments and interfacing The ODEs are evolved via the solvers of DifferentialEquations.jl. When initializing a  CoupledODEs , you can specify the solver that will integrate  f  in time, along with any other integration options, using the  diffeq  keyword. For example you could use  diffeq = (abstol = 1e-9, reltol = 1e-9) . If you want to specify a solver, do so by using the keyword  alg , e.g.:  diffeq = (alg = Tsit5(), reltol = 1e-6) . This requires you to have been first  using OrdinaryDiffEq  to access the solvers. The default  diffeq  is: (alg = Tsit5(stage limiter! = trivial limiter!, step limiter! = trivial limiter!, thread = static(false)), abstol = 1.0e-6, reltol = 1.0e-6) diffeq  keywords can also include  callback  for  event handling  , however the majority of downstream functions in DynamicalSystems.jl assume that  f  is differentiable. The convenience constructor  CoupledODEs(prob::ODEProblem, diffeq)  and  CoupledODEs(ds::CoupledODEs, diffeq)  are also available. Dev note:  CoupledODEs  is a light wrapper of  ODEIntegrator  from DifferentialEquations.jl. The integrator is available as the field  integ , and the  ODEProblem  is  integ.sol.prob . The convenience syntax  ODEProblem(ds::CoupledODEs, tspan = (t0, Inf))  is available."},{"id":388,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.StroboscopicMap","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.StroboscopicMap","content":" DynamicalSystemsBase.StroboscopicMap  —  Type StroboscopicMap <: DiscreteTimeDynamicalSystem\nStroboscopicMap(ds::CoupledODEs, period::Real) → smap\nStroboscopicMap(period::Real, f, u0, p = nothing; kwargs...) A discrete time dynamical system that produces iterations of a time-dependent (non-autonomous)  CoupledODEs  system exactly over a given  period . The second signature first creates a  CoupledODEs  and then calls the first. StroboscopicMap  follows the  DynamicalSystem  interface. In addition, the function  set_period!(smap, period)  is provided, that sets the period of the system to a new value (as if it was a parameter). As this system is in discrete time,  current_time  and  initial_time  are integers. The initial time is always 0, because  current_time  counts elapsed periods. Call these functions on the  parent  of  StroboscopicMap  to obtain the corresponding continuous time. In contrast,  reinit!  expects  t0  in continuous time. The convenience constructor StroboscopicMap(T::Real, f, u0, p = nothing; diffeq, t0 = 0) → smap is also provided. See also  PoincareMap ."},{"id":389,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.PoincareMap","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.PoincareMap","content":" DynamicalSystemsBase.PoincareMap  —  Type PoincareMap <: DiscreteTimeDynamicalSystem\nPoincareMap(ds::CoupledODEs, plane; kwargs...) → pmap A discrete time dynamical system that produces iterations over the Poincaré map [DatserisParlitz2022]  of the given continuous time  ds . This map is defined as the sequence of points on the Poincaré surface of section, which is defined by the  plane  argument. See also  StroboscopicMap ,  poincaresos . Keyword arguments direction = -1 : Only crossings with  sign(direction)  are considered to belong to the surface of section. Negative direction means going from less than  $b$  to greater than  $b$ . u0 = nothing : Specify an initial state. rootkw = (xrtol = 1e-6, atol = 1e-8) : A  NamedTuple  of keyword arguments passed to  find_zero  from  Roots.jl . Tmax = 1e3 : The argument  Tmax  exists so that the integrator can terminate instead of being evolved for infinite time, to avoid cases where iteration would continue forever for ill-defined hyperplanes or for convergence to fixed points, where the trajectory would never cross again the hyperplane. If during one  step!  the system has been evolved for more than  Tmax , then  step!(pmap)  will terminate and error. Description The Poincaré surface of section is defined as sequential transversal crossings a trajectory has with any arbitrary manifold, but here the manifold must be a hyperplane.  PoincareMap  iterates over the crossings of the section. If the state of  ds  is  $\\mathbf{u} = (u_1, \\ldots, u_D)$  then the equation defining a hyperplane is \\[a_1u_1 + \\dots + a_Du_D = \\mathbf{a}\\cdot\\mathbf{u}=b\\] where  $\\mathbf{a}, b$  are the parameters of the hyperplane. In code,  plane  can be either: A  Tuple{Int, <: Real} , like  (j, r) : the plane is defined as when the  j th variable of the system equals the value  r . A vector of length  D+1 . The first  D  elements of the vector correspond to  $\\mathbf{a}$  while the last element is  $b$ . PoincareMap  uses  ds , higher order interpolation from DifferentialEquations.jl, and root finding from Roots.jl, to create a high accuracy estimate of the section. PoincareMap  follows the  DynamicalSystem  interface with the following adjustments: dimension(pmap) == dimension(ds) , even though the Poincaré map is effectively 1 dimension less. Like  StroboscopicMap  time is discrete and counts the iterations on the surface of section.  initial_time  is always  0  and  current_time  is current iteration number. A new function  current_crossing_time  returns the real time corresponding to the latest crossing of the hyperplane, which is what the  current_state(ds)  corresponds to as well. For the special case of  plane  being a  Tuple{Int, <:Real} , a special  reinit!  method is allowed with input state of length  D-1  instead of  D , i.e., a reduced state already on the hyperplane that is then converted into the  D  dimensional state. Example using DynamicalSystemsBase\nds = Systems.rikitake(zeros(3); μ = 0.47, α = 1.0)\npmap = poincaremap(ds, (3, 0.0))\nstep!(pmap)\nnext_state_on_psos = current_state(pmap)"},{"id":390,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.ProjectedDynamicalSystem","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.ProjectedDynamicalSystem","content":" DynamicalSystemsBase.ProjectedDynamicalSystem  —  Type ProjectedDynamicalSystem <: DynamicalSystem\nProjectedDynamicalSystem(ds::DynamicalSystem, projection, complete_state) A dynamical system that represents a projection of an existing  ds  on a (projected) space. The  projection  defines the projected space. If  projection isa AbstractVector{Int} , then the projected space is simply the variable indices that  projection  contains. Otherwise,  projection  can be an arbitrary function that given the state of the original system  ds , returns the state in the projected space. In this case the projected space can be equal, or even higher-dimensional, than the original. complete_state  produces the state for the original system from the projected state.  complete_state  can always be a function that given the projected state returns a state in the original space. However, if  projection isa AbstractVector{Int} , then  complete_state  can also be a vector that contains the values of the  remaining  variables of the system, i.e., those  not  contained in the projected space. In this case the projected space needs to be lower-dimensional than the original. Notice that  ProjectedDynamicalSystem  does not require an invertible projection,  complete_state  is only used during  reinit! .  ProjectedDynamicalSystem  is in fact a rather trivial wrapper of  ds  which steps it as normal in the original state space and only projects as a last step, e.g., during  current_state . Examples Case 1: project 5-dimensional system to its last two dimensions. ds = Systems.lorenz96(5)\nprojection = [4, 5]\ncomplete_state = [0.0, 0.0, 0.0] # completed state just in the plane of last two dimensions\npds = ProjectedDynamicalSystem(ds, projection, complete_state)\nreinit!(pds, [0.2, 0.4])\nstep!(pds)\nget_state(pds) Case 2: custom projection to general functions of state. ds = Systems.lorenz96(5)\nprojection(u) = [sum(u), sqrt(u[1]^2 + u[2]^2)]\ncomplete_state(y) = repeat([y[1]/5], 5)\npds = # same as in above example..."},{"id":391,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.ArbitrarySteppable","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.ArbitrarySteppable","content":" DynamicalSystemsBase.ArbitrarySteppable  —  Type ArbitrarySteppable <: DiscreteTimeDynamicalSystem\nArbitrarySteppable(\n    model, step!, extract_state, extract_parameters, reset_model!;\n    isdeterministic = true, set_state = reinit!,\n) A dynamical system generated by an arbitrary \"model\" that can be stepped  in-place  with some function  step!(model)  for 1 step. The state of the model is extracted by the  extract_state(model) -> u  function The parameters of the model are extracted by the  extract_parameters(model) -> p  function. The system may be re-initialized, via  reinit! , with the  reset_model!  user-provided function that must have the call signature reset_model!(model, u, p) given a (potentially new) state  u  and parameter container  p , both of which will default to the initial ones in the  reinit!  call. ArbitrarySteppable  exists to provide the DynamicalSystems.jl interface to models from other packages that could be used within the DynamicalSystems.jl library.  ArbitrarySteppable  follows the  DynamicalSystem  interface with the following adjustments: initial_time  is always 0, as time counts the steps the model has taken since creation or last  reinit!  call. set_state!  is the same as  reinit!  by default. If not, the keyword argument  set_state  is a function  set_state(model, u)  that sets the state of the model to  u . The keyword  isdeterministic  should be set properly, as it decides whether downstream algorithms should error or not."},{"id":392,"pagetitle":"DynamicalSystem reference","title":"Relevant dynamical systems API","ref":"/attractors/stable/dynsysref/#Relevant-dynamical-systems-API","content":" Relevant dynamical systems API Here we state only the relevant API functions; others can be found in the DynamicalSystemsBase.jl documentation."},{"id":393,"pagetitle":"DynamicalSystem reference","title":"CommonSolve.step!","ref":"/attractors/stable/dynsysref/#CommonSolve.step!-Tuple{DynamicalSystem, Vararg{Any}}","content":" CommonSolve.step!  —  Method step!(ds::DiscreteTimeDynamicalSystem [, n::Integer]) → ds Evolve the discrete time dynamical system for 1 or  n  steps. step!(ds::ContinuousTimeDynamicalSystem, [, dt::Real [, stop_at_tdt]]) → ds Evolve the continuous time dynamical system for one integration step. Alternatively, if a  dt  is given, then progress the integration until there is a temporal difference  ≥ dt  (so, step  at least  for  dt  time). When  true  is passed to the optional third argument, the integration advances for exactly  dt  time."},{"id":394,"pagetitle":"DynamicalSystem reference","title":"SciMLBase.reinit!","ref":"/attractors/stable/dynsysref/#SciMLBase.reinit!-Tuple{DynamicalSystem, Vararg{Any}}","content":" SciMLBase.reinit!  —  Method reinit!(ds::DynamicalSystem, u = initial_state(ds); kwargs...) → ds Reset the status of  ds , so that it is as if it has be just initialized with initial state  u . Practically every function of the ecosystem that evolves  ds  first calls this function on it. Besides the new initial state  u , you can also configure the keywords  t0 = initial_time(ds)  and  p = current_parameters(ds) . Note the default settings: the state and time are the initial, but the parameters are the current. The special method  reinit!(ds, ::Nothing; kwargs...)  is also available, which does nothing and leaves the system as is. This is so that downstream functions that call  reinit!  can still be used without resetting the system but rather continuing from its exact current state."},{"id":395,"pagetitle":"DynamicalSystem reference","title":"DynamicalSystemsBase.set_parameter!","ref":"/attractors/stable/dynsysref/#DynamicalSystemsBase.set_parameter!","content":" DynamicalSystemsBase.set_parameter!  —  Function set_parameter!(ds::DynamicalSystem, index, value) Change a parameter of  ds  given the  index  it has in the parameter container and the  value  to set it to. This function works for both array/dictionary containers as well as composite types. In the latter case  index  needs to be a  Symbol ."},{"id":396,"pagetitle":"DynamicalSystem reference","title":"StateSpaceSet  reference","ref":"/attractors/stable/dynsysref/#StateSpaceSet-reference","content":" StateSpaceSet  reference"},{"id":397,"pagetitle":"DynamicalSystem reference","title":"StateSpaceSets.StateSpaceSet","ref":"/attractors/stable/dynsysref/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. DatserisParlitz2022 Datseris & Parlitz 2022,  Nonlinear Dynamics: A Concise Introduction Interlaced with Code ,  Springer Nature, Undergrad. Lect. Notes In Physics"},{"id":400,"pagetitle":"Examples for Attractors.jl","title":"Examples for Attractors.jl","ref":"/attractors/stable/examples/#Examples-for-Attractors.jl","content":" Examples for Attractors.jl Note that the examples utilize some convenience plotting functions offered by Attractors.jl which come into scope when using  Makie  (or any of its backends such as  CairoMakie ), see the  visualization utilities  for more."},{"id":401,"pagetitle":"Examples for Attractors.jl","title":"Newton's fractal (basins of 2D map)","ref":"/attractors/stable/examples/#Newton's-fractal-(basins-of-2D-map)","content":" Newton's fractal (basins of 2D map) using Attractors\nfunction newton_map(z, p, n)\n    z1 = z[1] + im*z[2]\n    dz1 = newton_f(z1, p[1])/newton_df(z1, p[1])\n    z1 = z1 - dz1\n    return SVector(real(z1), imag(z1))\nend\nnewton_f(x, p) = x^p - 1\nnewton_df(x, p)= p*x^(p-1)\n\nds = DiscreteDynamicalSystem(newton_map, [0.1, 0.2], [3.0])\nxg = yg = range(-1.5, 1.5; length = 400)\n# Use non-sparse for using `basins_of_attraction`\nmapper = AttractorsViaRecurrences(ds, (xg, yg);\n    sparse = false, mx_chk_lost = 1000\n)\nbasins, attractors = basins_of_attraction(mapper; show_progress = false)\nbasins 400×400 Matrix{Int32}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n 1  1  1  1  1  1  1  1  1  1  1  1  1     2  2  2  2  2  2  2  2  2  2  2  2\n ⋮              ⋮              ⋮        ⋱        ⋮              ⋮           \n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3  …  3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3\n 3  3  3  3  3  3  3  3  3  3  3  3  3     3  3  3  3  3  3  3  3  3  3  3  3 attractors Dict{Int32, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points Now let's plot this as a heatmap, and on top of the heatmap, let's scatter plot the attractors. We do this in one step by utilizing one of the pre-defined plotting functions offered by Attractors.jl using CairoMakie\ngrid = (xg, yg)\nfig = heatmap_basins_attractors(grid, basins, attractors) Instead of computing the full basins, we could get only the fractions of the basins of attractions using  basins_fractions , which is typically the more useful thing to do in a high dimensional system. In such cases it is also typically more useful to define a sampler that generates initial conditions on the fly instead of pre-defining some initial conditions (as is done in  basins_of_attraction . This is simple to do: grid = (xg, yg)\nmapper = AttractorsViaRecurrences(ds, grid;\n    sparse = false, mx_chk_lost = 1000\n)\n\nsampler, = statespace_sampler(grid)\n\nbasins = basins_fractions(mapper, sampler) Dict{Int64, Float64} with 3 entries:\n  2 => 0.335\n  3 => 0.346\n  1 => 0.319 in this case, to also get the attractors we simply extract them from the underlying storage of the mapper: attractors = extract_attractors(mapper) Dict{Int32, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points"},{"id":402,"pagetitle":"Examples for Attractors.jl","title":"Minimal Fatal Shock","ref":"/attractors/stable/examples/#Minimal-Fatal-Shock","content":" Minimal Fatal Shock Finding Minimal Fatal Shock for some point  u0  on example of Newton's fractal attractors attractors = extract_attractors(mapper)\nshocks = Dict()\nalgo_bb = Attractors.MFSBlackBoxOptim(max_steps = 20000)\nfor atr in values(attractors)\n    u0 = vec(atr)[1]\n    shocks[u0] = minimal_fatal_shock(mapper, u0, (-1.5,1.5), algo_bb)\n\nend\nshocks Dict{Any, Any} with 3 entries:\n  [-0.5, -0.866025] => [0.592005, 0.190974]\n  [1.0, 0.0]        => [-0.461391, -0.417204]\n  [-0.5, 0.866025]  => [0.592005, -0.190974] To visualize results we can make use of previously defined heatmap ax =  content(fig[1,1])\nfor (atr, shock) in shocks\n    lines!(ax, [atr, atr + shock[1]])\nend\nfig"},{"id":403,"pagetitle":"Examples for Attractors.jl","title":"Fractality of 2D basins of the (4D) magnetic pendulum","ref":"/attractors/stable/examples/#Fractality-of-2D-basins-of-the-(4D)-magnetic-pendulum","content":" Fractality of 2D basins of the (4D) magnetic pendulum In this section we will calculate the basins of attraction of the four-dimensional magnetic pendulum. We know that the attractors of this system are all individual fixed points on the (x, y) plane so we will only compute the basins there. We can also use this opportunity to highlight a different method, the  AttractorsViaProximity  which works when we already know where the attractors are. Furthermore we will also use a  ProjectedDynamicalSystem  to project the 4D system onto a 2D plane, saving a lot of computational time!"},{"id":404,"pagetitle":"Examples for Attractors.jl","title":"Computing the basins","ref":"/attractors/stable/examples/#Computing-the-basins","content":" Computing the basins First we need to load in the magnetic pendulum from the predefined dynamical systems library using Attractors, CairoMakie\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.magnetic_pendulum(d=0.2, α=0.2, ω=0.8, N=3) 4-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  MagneticPendulum\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    PredefinedDynamicalSystems.MagneticPendulumParams([1.0, 1.0, 1.0], 0.2, 0.2, 0.8)\n time:          0.0\n state:         [0.7094575840693688, 0.704748136859158, 0.0, 0.0]\n Then, we create a projected system on the x-y plane psys = ProjectedDynamicalSystem(ds, [1, 2], [0.0, 0.0]) 2-dimensional ProjectedDynamicalSystem\n deterministic:  true\n discrete time:  false\n in-place:       false\n dynamic rule:   MagneticPendulum\n projection:     [1, 2]\n complete state: [0.0, 0.0]\n parameters:     PredefinedDynamicalSystems.MagneticPendulumParams([1.0, 1.0, 1.0], 0.2, 0.2, 0.8)\n time:           0.0\n state:          [0.7094575840693688, 0.704748136859158]\n For this systems we know the attractors are close to the magnet positions. The positions can be obtained from the equations of the system, provided that one has seen the source code (not displayed here), like so: attractors = Dict(i => StateSpaceSet([dynamic_rule(ds).magnets[i]]) for i in 1:3) Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n  2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n  1 => 2-dimensional StateSpaceSet{Float64} with 1 points and then create a mapper = AttractorsViaProximity(psys, attractors) AttractorsViaProximity\n rule f:      AttractorsViaProximity\n type:        ProjectedDynamicalSystem\n ε:           0.8660254037844386\n Δt:          1\n Ttr:         100\n attractors:  Dict{Int64, StateSpaceSet{2, Float64}} with 3 entries:\n                2 => 2-dimensional StateSpaceSet{Float64} with 1 points\n                3 => 2-dimensional StateSpaceSet{Float64} with 1 points\n                1 => 2-dimensional StateSpaceSet{Float64} with 1 points\n and as before, get the basins of attraction xg = yg = range(-4, 4; length = 201)\ngrid = (xg, yg)\nbasins, = basins_of_attraction(mapper, grid; show_progress = false)\n\nheatmap_basins_attractors(grid, basins, attractors)"},{"id":405,"pagetitle":"Examples for Attractors.jl","title":"Computing the uncertainty exponent","ref":"/attractors/stable/examples/#Computing-the-uncertainty-exponent","content":" Computing the uncertainty exponent Let's now calculate the  uncertainty_exponent  for this system as well. The calculation is straightforward: using CairoMakie\nε, f_ε, α = uncertainty_exponent(basins)\nfig, ax = lines(log.(ε), log.(f_ε))\nax.title = \"α = $(round(α; digits=3))\"\nfig The actual uncertainty exponent is the slope of the curve (α) and indeed we get an exponent near 0 as we know a-priory the basins have fractal boundaries for the magnetic pendulum."},{"id":406,"pagetitle":"Examples for Attractors.jl","title":"Computing the tipping probabilities","ref":"/attractors/stable/examples/#Computing-the-tipping-probabilities","content":" Computing the tipping probabilities We will compute the tipping probabilities using the magnetic pendulum's example as the \"before\" state. For the \"after\" state we will change the  γ  parameter of the third magnet to be so small, its basin of attraction will virtually disappear. As we don't know  when  the basin of the third magnet will disappear, we switch the attractor finding algorithm back to  AttractorsViaRecurrences . set_parameter!(psys, :γs, [1.0, 1.0, 0.1])\nmapper = AttractorsViaRecurrences(psys, (xg, yg); Δt = 1)\nbasins_after, attractors_after = basins_of_attraction(\n    mapper, (xg, yg); show_progress = false\n)\n# matching attractors is important!\nrmap = match_statespacesets!(attractors_after, attractors)\n# Don't forget to update the labels of the basins as well!\nreplace!(basins_after, rmap...)\n\n# now plot\nheatmap_basins_attractors(grid, basins_after, attractors_after) And let's compute the tipping \"probabilities\": P = tipping_probabilities(basins, basins_after) 3×2 Matrix{Float64}:\n 0.503144  0.496856\n 0.448887  0.551113\n 0.551102  0.448898 As you can see  P  has size 3×2, as after the change only 2 attractors have been identified in the system (3 still exist but our state space discretization isn't fine enough to find the 3rd because it has such a small basin). Also, the first row of  P  is 50% probability to each other magnet, as it should be due to the system's symmetry."},{"id":407,"pagetitle":"Examples for Attractors.jl","title":"3D basins via recurrences","ref":"/attractors/stable/examples/#D-basins-via-recurrences","content":" 3D basins via recurrences To showcase the true power of  AttractorsViaRecurrences  we need to use a system whose attractors span higher-dimensional space. An example is using Attractors\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.thomas_cyclical(b = 0.1665) 3-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  thomas_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [0.1665]\n time:          0.0\n state:         [1.0, 0.0, 0.0]\n which, for this parameter, contains 3 coexisting attractors which are entangled periodic orbits that span across all three dimensions. To compute the basins we define a three-dimensional grid and call on it  basins_of_attraction . # This computation takes about an hour\nxg = yg = zg = range(-6.0, 6.0; length = 251)\nmapper = AttractorsViaRecurrences(ds, (xg, yg, zg); sparse = false)\nbasins, attractors = basins_of_attraction(mapper)\nattractors Dict{Int16, StateSpaceSet{3, Float64}} with 5 entries:\n  5 => 3-dimensional StateSpaceSet{Float64} with 1 points\n  4 => 3-dimensional StateSpaceSet{Float64} with 379 points\n  6 => 3-dimensional StateSpaceSet{Float64} with 1 points\n  2 => 3-dimensional StateSpaceSet{Float64} with 538 points\n  3 => 3-dimensional StateSpaceSet{Float64} with 537 points\n  1 => 3-dimensional StateSpaceSet{Float64} with 1 points Note: the reason we have 6 attractors here is because the algorithm also finds 3 unstable fixed points and labels them as attractors. This happens because we have provided initial conditions on the grid  xg, yg, zg  that start exactly on the unstable fixed points, and hence stay there forever, and hence are perceived as attractors by the recurrence algorithm. As you will see in the video below, they don't have any basin fractions The basins of attraction are very complicated. We can try to visualize them by animating the 2D slices at each z value, to obtain: Then, we visualize the attractors to obtain: In the animation above, the scattered points are the attractor values the function  AttractorsViaRecurrences  found by itself. Of course, for the periodic orbits these points are incomplete. Once the function's logic understood we are on an attractor, it stops computing. However, we also simulated lines, by evolving initial conditions colored appropriately with the basins output. The animation was produced with the code: using GLMakie\nfig = Figure()\ndisplay(fig)\nax = fig[1,1] = Axis3(fig; title = \"found attractors\")\ncmap = cgrad(:dense, 6; categorical = true)\n\nfor i in keys(attractors)\n    tr = attractors[i]\n    markersize = length(attractors[i]) > 10 ? 2000 : 6000\n    marker = length(attractors[i]) > 10 ? :circle : :rect\n    scatter!(ax, columns(tr)...; markersize, marker, transparency = true, color = cmap[i])\n    j = findfirst(isequal(i), bsn)\n    x = xg[j[1]]\n    y = yg[j[2]]\n    z = zg[j[3]]\n    tr = trajectory(ds, 100, SVector(x,y,z); Ttr = 100)\n    lines!(ax, columns(tr)...; linewidth = 1.0, color = cmap[i])\nend\n\na = range(0, 2π; length = 200) .+ π/4\n\nrecord(fig, \"cyclical_attractors.mp4\", 1:length(a)) do i\n    ax.azimuth = a[i]\nend"},{"id":408,"pagetitle":"Examples for Attractors.jl","title":"Basins of attraction of a Poincaré map","ref":"/attractors/stable/examples/#Basins-of-attraction-of-a-Poincaré-map","content":" Basins of attraction of a Poincaré map PoincareMap  is just another discrete time dynamical system within the DynamicalSystems.jl ecosystem. With respect to Attractors.jl functionality, there is nothing special about Poincaré maps. You simply initialize one use it like any other type of system. Let's continue from the above example  of the Thomas cyclical system using Attractors\nusing PredefinedDynamicalSystems\nds = PredefinedDynamicalSystems.thomas_cyclical(b = 0.1665); 3-dimensional CoupledODEs\n deterministic: true\n discrete time: false\n in-place:      false\n dynamic rule:  thomas_rule\n ODE solver:    Tsit5\n ODE kwargs:    (abstol = 1.0e-6, reltol = 1.0e-6)\n parameters:    [0.1665]\n time:          0.0\n state:         [1.0, 0.0, 0.0]\n The three limit cycles attractors we have above become fixed points in the Poincaré map (for appropriately chosen hyperplanes). Since we already know the 3D structure of the basins, we can see that an appropriately chosen hyperplane is just the plane  z = 0 . Hence, we define a Poincaré map on this plane: plane = (3, 0.0)\npmap = PoincareMap(ds, plane) 3-dimensional PoincareMap\n deterministic: true\n discrete time: true\n in-place:      false\n dynamic rule:  thomas_rule\n hyperplane:    (3, 0.0)\n crossing time: 0.0\n parameters:    [0.1665]\n time:          0\n state:         [1.0, 0.0, 0.0]\n We define the same grid as before, but now only we only use the x-y coordinates. This is because we can utilize the special  reinit!  method of the  PoincareMap , that allows us to initialize a new state directly on the hyperplane (and then the remaining variable of the dynamical system takes its value from the hyperplane itself). xg = yg = range(-6.0, 6.0; length = 251)\ngrid = (xg, yg)\nmapper = AttractorsViaRecurrences(pmap, grid; sparse = false) AttractorsViaRecurrences\n rule f:      AttractorsViaRecurrences\n system:      PoincareMap\n grid:        (-6.0:0.048:6.0, -6.0:0.048:6.0)\n attractors:  Dict{Int32, StateSpaceSet{2, Float64}}()\n All that is left to do is to call  basins_of_attraction : basins, attractors = basins_of_attraction(mapper; show_progress = false); (Int32[1 1 … 2 2; 1 1 … 2 2; … ; 2 2 … 1 1; 2 2 … 1 1], Dict{Int32, StateSpaceSet{2, Float64}}(4 => 2-dimensional StateSpaceSet{Float64} with 1 points, 2 => 2-dimensional StateSpaceSet{Float64} with 1 points, 3 => 2-dimensional StateSpaceSet{Float64} with 3 points, 1 => 2-dimensional StateSpaceSet{Float64} with 1 points)) heatmap_basins_attractors(grid, basins, attractors) just like in the example above, there is a fourth attractor with 0 basin fraction. This is an unstable fixed point, and exists exactly because we provided a grid with the unstable fixed point exactly on this grid"},{"id":409,"pagetitle":"Examples for Attractors.jl","title":"Basin fractions continuation in the magnetic pendulum","ref":"/attractors/stable/examples/#Basin-fractions-continuation-in-the-magnetic-pendulum","content":" Basin fractions continuation in the magnetic pendulum Perhaps the simplest application of  continuation  is to produce a plot of how the fractions of attractors change as we continuously change the parameter we changed above to calculate tipping probabilities."},{"id":410,"pagetitle":"Examples for Attractors.jl","title":"Computing the fractions","ref":"/attractors/stable/examples/#Computing-the-fractions","content":" Computing the fractions This is what the following code does: # initialize projected magnetic pendulum\nusing Attractors, PredefinedDynamicalSystems\nusing Random: Xoshiro\nds = Systems.magnetic_pendulum(; d = 0.3, α = 0.2, ω = 0.5)\nxg = yg = range(-3, 3; length = 101)\nds = ProjectedDynamicalSystem(ds, 1:2, [0.0, 0.0])\n# Choose a mapper via recurrences\nmapper = AttractorsViaRecurrences(ds, (xg, yg); Δt = 1.0)\n# What parameter to change, over what range\nγγ = range(1, 0; length = 101)\nprange = [[1, 1, γ] for γ in γγ]\npidx = :γs\n# important to make a sampler that respects the symmetry of the system\nregion = HSphere(3.0, 2)\nsampler, = statespace_sampler(region, 1234)\n# continue attractors and basins:\n# `Inf` threshold fits here, as attractors move smoothly in parameter space\nrsc = RecurrencesFindAndMatch(mapper; threshold = Inf)\nfractions_curves, attractors_info = continuation(\n    rsc, prange, pidx, sampler;\n    show_progress = false, samples_per_parameter = 100\n)\n# Show some characteristic fractions:\nfractions_curves[[1, 50, 101]] 3-element Vector{Dict{Int64, Float64}}:\n Dict(2 => 0.32, 3 => 0.3, 1 => 0.38)\n Dict(2 => 0.47572815533980584, 3 => 0.4174757281553398, 1 => 0.10679611650485436)\n Dict(2 => 0.39215686274509803, 3 => 0.6078431372549019)"},{"id":411,"pagetitle":"Examples for Attractors.jl","title":"Plotting the fractions","ref":"/attractors/stable/examples/#Plotting-the-fractions","content":" Plotting the fractions We visualize them using a predefined function that you can find in  docs/basins_plotting.jl # careful; `prange` isn't a vector of reals!\nplot_basins_curves(fractions_curves, γγ)"},{"id":412,"pagetitle":"Examples for Attractors.jl","title":"Fixed point curves","ref":"/attractors/stable/examples/#Fixed-point-curves","content":" Fixed point curves A by-product of the analysis is that we can obtain the curves of the position of fixed points for free. However, only the stable branches can be obtained! using CairoMakie\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = L\"\\gamma_3\", ylabel = \"fixed point\")\n# choose how to go from attractor to real number representation\nfunction real_number_repr(attractor)\n    p = attractor[1]\n    return (p[1] + p[2])/2\nend\n\nfor (i, γ) in enumerate(γγ)\n    for (k, attractor) in attractors_info[i]\n        scatter!(ax, γ, real_number_repr(attractor); color = Cycled(k))\n    end\nend\nfig as you can see, two of the three fixed points, and their stability, do not depend at all on the parameter value, since this parameter value tunes the magnetic strength of only the third magnet. Nevertheless, the  fractions of basin of attraction  of all attractors depend strongly on the parameter. This is a simple example that highlights excellently how this new approach we propose here should be used even if one has already done a standard linearized bifurcation analysis."},{"id":413,"pagetitle":"Examples for Attractors.jl","title":"Extinction of a species in a multistable competition model","ref":"/attractors/stable/examples/#Extinction-of-a-species-in-a-multistable-competition-model","content":" Extinction of a species in a multistable competition model In this advanced example we utilize both  RecurrencesFindAndMatch  and  aggregate_attractor_fractions  in analyzing species extinction in a dynamical model of competition between multiple species. The final goal is to show the percentage of how much of the state space leads to the extinction or not of a pre-determined species, as we vary a parameter. The model however displays extreme multistability, a feature we want to measure and preserve before aggregating information into \"extinct or not\". To measure and preserve this we will apply  RecurrencesFindAndMatch  as-is first. Then we can aggregate information. First we have using Attractors, OrdinaryDiffEq\nusing PredefinedDynamicalSystems\nusing Random: Xoshiro\n# arguments to algorithms\nsamples_per_parameter = 1000\ntotal_parameter_values = 101\ndiffeq = (alg = Vern9(), reltol = 1e-9, abstol = 1e-9, maxiters = Inf)\nrecurrences_kwargs = (; Δt= 1.0, mx_chk_fnd_att=9, diffeq);\n# initialize dynamical syste and sampler\nds = PredefinedDynamicalSystems.multispecies_competition() # 8-dimensional\nds = CoupledODEs(ODEProblem(ds), diffeq)\n# define grid in state space\nxg = range(0, 60; length = 300)\ngrid = ntuple(x -> xg, 8)\nprange = range(0.2, 0.3; length = total_parameter_values)\npidx = :D\nsampler, = statespace_sampler(grid, 1234)\n# initialize mapper\nmapper = AttractorsViaRecurrences(ds, grid; recurrences_kwargs...)\n# perform continuation of attractors and their basins\ncontinuation = RecurrencesFindAndMatch(mapper; threshold = Inf)\nfractions_curves, attractors_info = continuation(\n    continuation, prange, pidx, sampler;\n    show_progress = true, samples_per_parameter\n);\nplot_basins_curves(fractions_curves, prange; separatorwidth = 1) this example is not actually run when building the docs, because it takes about 60 minutes to complete depending on the computer; we load precomputed results instead As you can see, the system has extreme multistability with 64 unique attractors (according to the default matching behavior in  RecurrencesFindAndMatch ; a stricter matching with less than  Inf  threshold would generate more \"distinct\" attractors). One could also isolate a specific parameter slice, and do the same as what we do in the  Fractality of 2D basins of the (4D) magnetic pendulum  example, to prove that the basin boundaries are fractal, thereby indeed confirming the paper title \"Fundamental Unpredictability\". Regardless, we now want to continue our analysis to provide a figure similar to the above but only with two colors: fractions of attractors where a species is extinct or not. Here's how: species = 3 # species we care about its existence\n\nfeaturizer = (A) -> begin\n    i = isextinct(A, species)\n    return SVector(Int32(i))\nend\nisextinct(A, idx = unitidxs) = all(a -> a <= 1e-2, A[:, idx])\n\n# `minneighbors = 1` is crucial for grouping single attractors\ngroupingconfig = GroupViaClustering(; min_neighbors=1, optimal_radius_method=0.5)\n\naggregated_fractions, aggregated_info = aggregate_attractor_fractions(\n    fractions_curves, attractors_info, featurizer, groupingconfig\n)\n\nplot_basins_curves(aggregated_fractions, prange;\n    separatorwidth = 1, colors = [\"green\", \"black\"],\n    labels = Dict(1 => \"extinct\", 2 => \"alive\"),\n) (in hindsight, the labels are reversed; attractor 1 is the alive one, but oh well)"},{"id":414,"pagetitle":"Examples for Attractors.jl","title":"Trivial featurizing and grouping for basins fractions","ref":"/attractors/stable/examples/#Trivial-featurizing-and-grouping-for-basins-fractions","content":" Trivial featurizing and grouping for basins fractions This is a rather trivial example showcasing the usage of  AttractorsViaFeaturizing . Let us use once again the magnetic pendulum example. For it, we have a really good idea of what features will uniquely describe each attractor: the last points of a trajectory (which should be very close to the magnetic the trajectory converged to). To provide this information to the  AttractorsviaFeaturizing  we just create a julia function that returns this last point using Attractors\nusing PredefinedDynamicalSystems\n\nds = Systems.magnetic_pendulum(d=0.2, α=0.2, ω=0.8, N=3)\npsys = ProjectedDynamicalSystem(ds, [1, 2], [0.0, 0.0])\n\nfunction featurizer(X, t)\n    return X[end]\nend\n\nmapper = AttractorsViaFeaturizing(psys, featurizer; Ttr = 200, T = 1)\n\nxg = yg = range(-4, 4; length = 101)\n\nregion = HRectangle([-4, 4], [4, 4])\nsampler, = statespace_sampler(region)\n\nfs = basins_fractions(mapper, sampler; show_progress = false) Dict{Int64, Float64} with 3 entries:\n  2 => 0.403\n  3 => 0.243\n  1 => 0.354 As expected, the fractions are each about 1/3 due to the system symmetry."},{"id":415,"pagetitle":"Examples for Attractors.jl","title":"Featurizing and grouping across parameters (MCBB)","ref":"/attractors/stable/examples/#Featurizing-and-grouping-across-parameters-(MCBB)","content":" Featurizing and grouping across parameters (MCBB) Here we showcase the example of the Monte Carlo Basin Bifurcation publication. For this, we will use  FeaturizeGroupAcrossParameter  while also providing a  par_weight = 1  keyword. However, we will not use a network of 2nd order Kuramoto oscillators (as done in the paper by Gelbrecht et al.) because it is too costly to run on CI. Instead, we will use \"dummy\" system which we know analytically the attractors and how they behave versus a parameter. the Henon map and try to group attractors into period 1 (fixed point), period 3, and divergence to infinity. We will also use a pre-determined optimal radius for clustering, as we know a-priory the expected distances of features in feature space (due to the contrived form of the  featurizer  function below). using Attractors, Random\n\nfunction dumb_map(dz, z, p, n)\n    x, y = z\n    r = p[1]\n    if r < 0.5\n        dz[1] = dz[2] = 0.0\n    else\n        if x > 0\n            dz[1] = r\n            dz[2] = r\n        else\n            dz[1] = -r\n            dz[2] = -r\n        end\n    end\n    return\nend\n\nr = 3.833\nds = DiscreteDynamicalSystem(dumb_map, [0., 0.], [r]) 2-dimensional DeterministicIteratedMap\n deterministic: true\n discrete time: true\n in-place:      true\n dynamic rule:  dumb_map\n parameters:    [3.833]\n time:          0\n state:         [0.0, 0.0]\n sampler, = statespace_sampler(HRectangle([-3.0, -3.0], [3.0, 3.0]), 1234)\n\nrrange = range(0, 2; length = 21)\nridx = 1\n\nfeaturizer(a, t) = a[end]\nclusterspecs = GroupViaClustering(optimal_radius_method = \"silhouettes\", max_used_features = 200)\nmapper = AttractorsViaFeaturizing(ds, featurizer, clusterspecs; T = 20, threaded = true)\ngap = FeaturizeGroupAcrossParameter(mapper; par_weight = 1.0)\nfractions_curves, clusters_info = continuation(\n    gap, rrange, ridx, sampler; show_progress = false\n)\nfractions_curves 21-element Vector{Dict{Int64, Float64}}:\n Dict(1 => 1.0)\n Dict(1 => 1.0)\n Dict(1 => 1.0)\n Dict(1 => 1.0)\n Dict(1 => 1.0)\n Dict(2 => 0.47, 3 => 0.53)\n Dict(5 => 0.52, 4 => 0.48)\n Dict(6 => 0.56, 7 => 0.44)\n Dict(9 => 0.56, 8 => 0.44)\n Dict(11 => 0.46, 10 => 0.54)\n ⋮\n Dict(16 => 0.48, 17 => 0.52)\n Dict(18 => 0.54, 19 => 0.46)\n Dict(20 => 0.53, 21 => 0.47)\n Dict(22 => 0.4, 23 => 0.6)\n Dict(25 => 0.51, 24 => 0.49)\n Dict(27 => 0.49, 26 => 0.51)\n Dict(29 => 0.54, 28 => 0.46)\n Dict(31 => 0.55, 30 => 0.45)\n Dict(32 => 0.47, 33 => 0.53) Looking at the information of the \"attractors\" (here the clusters of the grouping procedure) does not make it clear which label corresponds to which kind of attractor, but we can look at the: clusters_info 21-element Vector{Dict{Int64, Vector{Float64}}}:\n Dict(1 => [0.0, 0.0])\n Dict(1 => [0.0, 0.0])\n Dict(1 => [0.0, 0.0])\n Dict(1 => [0.0, 0.0])\n Dict(1 => [0.0, 0.0])\n Dict(2 => [0.5, 0.5], 3 => [-0.5, -0.5])\n Dict(5 => [-0.6000000000000006, -0.6000000000000006], 4 => [0.6000000000000005, 0.6000000000000005])\n Dict(6 => [-0.7000000000000002, -0.7000000000000002], 7 => [0.6999999999999995, 0.6999999999999995])\n Dict(9 => [0.7999999999999995, 0.7999999999999995], 8 => [-0.8, -0.8])\n Dict(11 => [0.8999999999999992, 0.8999999999999992], 10 => [-0.8999999999999991, -0.8999999999999991])\n ⋮\n Dict(16 => [-1.200000000000001, -1.200000000000001], 17 => [1.2000000000000013, 1.2000000000000013])\n Dict(18 => [-1.2999999999999987, -1.2999999999999987], 19 => [1.299999999999999, 1.299999999999999])\n Dict(20 => [-1.4000000000000001, -1.4000000000000001], 21 => [1.3999999999999992, 1.3999999999999992])\n Dict(22 => [1.5, 1.5], 23 => [-1.5, -1.5])\n Dict(25 => [1.5999999999999994, 1.5999999999999994], 24 => [-1.5999999999999996, -1.5999999999999996])\n Dict(27 => [-1.7000000000000013, -1.7000000000000013], 26 => [1.7000000000000015, 1.7000000000000015])\n Dict(29 => [1.7999999999999983, 1.7999999999999983], 28 => [-1.7999999999999985, -1.7999999999999985])\n Dict(31 => [1.9000000000000015, 1.9000000000000015], 30 => [-1.9000000000000006, -1.9000000000000006])\n Dict(32 => [-2.0, -2.0], 33 => [2.0, 2.0])"},{"id":416,"pagetitle":"Examples for Attractors.jl","title":"Using histograms and histogram distances as features","ref":"/attractors/stable/examples/#Using-histograms-and-histogram-distances-as-features","content":" Using histograms and histogram distances as features One of the aspects discussed in the original MCBB paper and implementation was the usage of histograms of the means of the variables of a dynamical system as the feature vector. This is useful in very high dimensional systems, such as oscillator networks, where the histogram of the means is significantly different in synchronized or unsychronized states. This is possible to do with current interface without any modifications, by using two more packages: ComplexityMeasures.jl to compute histograms, and Distances.jl for the Kullback-Leibler divergence (or any other measure of distance in the space of probability distributions you fancy). The only code we need to write to achieve this feature is a custom featurizer and providing an alternative distance to  GroupViaClustering . The code would look like this: using Distances: KLDivergence\nusing ComplexityMeasures: ValueHistogram, FixedRectangularBinning, probabilities\n\n# you decide the binning for the histogram, but for a valid estimation of\n# distances, all histograms must have exactly the same bins, and hence be\n# computed with fixed ranges, i.e., using the `FixedRectangularBinning`\nconst binning = FixedRectangularBinning(range(-5, 5; length = 11))\n\nfunction histogram_featurizer(A, t)\n    ms = mean.(columns(A)) # vector of mean of each variable\n    p = probabilities(ValueHistogram(binning), ms) # this is the histogram\n    return vec(p) # because Distances.jl doesn't know `Probabilities`\nend\n\ngconfig = GroupViaClustering(;\n    clust_distance_metric = KLDivergence(), # or any other PDF distance\n) You can then pass the  histogram_featurizer  and  gconfig  to an  AttractorsViaFeaturizing  and use the rest of the library as usual."},{"id":419,"pagetitle":"References","title":"References","ref":"/attractors/stable/references/#References","content":" References Datseris, G.; Rossi, K. L. and Wagemakers, A. (2023).  Framework for global stability analysis of dynamical systems .  Chaos: An Interdisciplinary Journal of Nonlinear Science  33 . Datseris, G. and Wagemakers, A. (2022).  Effortless estimation of basins of attraction .  Chaos: An Interdisciplinary Journal of Nonlinear Science  32 , 023104 . Daza, A.; Wagemakers, A.; Georgeot, B.; Gu{é}ry-Odelin, D. and Sanju{á}n, M. A. (2016).  Basin entropy: a new tool to analyze uncertainty in dynamical systems .  Scientific Reports  6 . Gelbrecht, M.; Kurths, J. and Hellmann, F. (2020).  Monte Carlo basin bifurcation analysis .  New Journal of Physics  22 , 033032 . Halekotte, L. and Feudel, U. (2020).  Minimal fatal shocks in multistable complex networks .  Scientific Reports  10 . Menck, P. J.; Heitzig, J.; Marwan, N. and Kurths, J. (2013).  How basin stability complements the linear-stability paradigm .  Nature Physics  9 , 89–92 . Puy, A.; Daza, A.; Wagemakers, A. and Sanju{á}n, M. A. (2021).  A test for fractal boundaries based on the basin entropy .  Communications in Nonlinear Science and Numerical Simulation  95 , 105588 . Schubert, E.; Sander, J.; Ester, M.; Kriegel, H. P. and Xu, X. (2017).  DBSCAN Revisited,  Revisited .  ACM Transactions on Database Systems  42 , 1–21 . Stender, M. and Hoffmann, N. (2021).  bSTAB: an open-source software for computing the basin stability of multi-stable dynamical systems .  Nonlinear Dynamics  107 , 1451–1468 ."},{"id":424,"pagetitle":"Visualization utilities","title":"Visualization utilities","ref":"/attractors/stable/visualization/#Visualization-utilities","content":" Visualization utilities In this page we document several plotting utility functions that have been created to make the visualization of the output of Attractors.jl seamless. See the examples page for usage of all these plotting functions. Note that most functions have an out-of-place and an in-place form, the in-place form always taking as a first input a pre-initialized  Axis  to plot in while the out-of-place creates and returns a new figure object. E.g., heatmap_basins_attractors(grid, basins, attractors; kwargs...)\nheatmap_basins_attractors!(ax, grid, basins, attractors; kwargs...)"},{"id":425,"pagetitle":"Visualization utilities","title":"Common plotting keywords","ref":"/attractors/stable/visualization/#Common-plotting-keywords","content":" Common plotting keywords Common keywords for plotting functions in Attractors.jl are: ukeys : the basin ids (unique keys, vector of integers) to use. By default all existing keys are used. access = [1, 2] : indices of which dimensions of an attractor to select and visualize in a two-dimensional plot. Only these ids will be visualized. By default all are used. colors : a dictionary mapping basin ids (i.e., including the  -1  key) to a color. By default the JuliaDynamics colorscheme is used if less than 7 ids are present, otherwise random colors from the  :darktest  colormap. markers : dictionary mapping attractor ids to markers they should be plotted as labels = Dict(ukeys .=> ukeys) : how to label each attractor. add_legend = length(ukeys) < 7 : whether to add a legend mapping colors to labels."},{"id":426,"pagetitle":"Visualization utilities","title":"Basins related","ref":"/attractors/stable/visualization/#Basins-related","content":" Basins related"},{"id":427,"pagetitle":"Visualization utilities","title":"Attractors.heatmap_basins_attractors","ref":"/attractors/stable/visualization/#Attractors.heatmap_basins_attractors","content":" Attractors.heatmap_basins_attractors  —  Function heatmap_basins_attractors(grid, basins, attractors; kwargs...) Plot a heatmap of found (2-dimensional)  basins  of attraction and corresponding  attractors , i.e., the output of  basins_of_attraction . Keyword arguments All the  common plotting keywords . source"},{"id":428,"pagetitle":"Visualization utilities","title":"Continuation related","ref":"/attractors/stable/visualization/#Continuation-related","content":" Continuation related"},{"id":429,"pagetitle":"Visualization utilities","title":"Attractors.plot_basins_curves","ref":"/attractors/stable/visualization/#Attractors.plot_basins_curves","content":" Attractors.plot_basins_curves  —  Function plot_basins_curves(fractions_curves, prange = 1:length(); kwargs...) Plot the fractions of basins of attraction versus a parameter range, i.e., visualize the output of  continuation . Keyword arguments style = :band : how to visualize the basin fractions. Choices are  :band  for a band plot with cumulative sum = 1 or  :lines  for a lines plot of each basin fraction separatorwidth = 1, separatorcolor = \"white\" : adds a line separating the fractions if the style is  :band axislegend_kwargs = (position = :lt,) : propagated to  axislegend  if a legend is added series_kwargs = NamedTuple() : propagated to the band or scatterline plot Also all  common plotting keywords . source"},{"id":430,"pagetitle":"Visualization utilities","title":"Attractors.plot_attractors_curves","ref":"/attractors/stable/visualization/#Attractors.plot_attractors_curves","content":" Attractors.plot_attractors_curves  —  Function plot_attractors_curves(attractors_info, attractor_to_real, prange = 1:length(); kwargs...) Same as in  plot_basins_curves  but visualizes the attractor dependence on the parameter instead of their fraction. The function  attractor_to_real  takes as input a  StateSpaceSet  (attractor) and returns a real number so that it can be plotted versus the parameter axis. Same keywords as  plot_basins_curves . source"},{"id":431,"pagetitle":"Visualization utilities","title":"Attractors.plot_basins_attractors_curves","ref":"/attractors/stable/visualization/#Attractors.plot_basins_attractors_curves","content":" Attractors.plot_basins_attractors_curves  —  Function plot_basins_attractors_curves(\n    fractions_curves, attractors_info, attractor_to_real [, prange]\n    kwargs...\n) Convinience combination of  plot_basins_curves  and  plot_attractors_curves  in a two-panel plot that shares legend, colors, markers, etc. source"},{"id":432,"pagetitle":"Visualization utilities","title":"Video output","ref":"/attractors/stable/visualization/#Video-output","content":" Video output"},{"id":433,"pagetitle":"Visualization utilities","title":"Attractors.animate_attractors_continuation","ref":"/attractors/stable/visualization/#Attractors.animate_attractors_continuation","content":" Attractors.animate_attractors_continuation  —  Function animate_attractors_continuation(\n    ds::DynamicalSystem, attractors_info, fractions_curves, prange, pidx;\n    kwargs...\n) Animate how the found system attractors and their corresponding basin fractions change as the system parameter is increased. This function combines the input and output of the  continuation  function into a video output. The input dynamical system  ds  is used to evolve initial conditions sampled from the found attractors, so that the attractors are better visualized.  attractors_info, fractions_curves  are the output of  continuation  while  ds, prange, pidx  are the input to  continuation . Keyword arguments savename = \"test.mp4\" : name of video output file framerate = 4 : framerate of video output markersize = 10 Δt, T : propagated to  trajectory  for evolving an initial condition sampled from an attractor Also all  common plotting keywords . source"},{"id":436,"pagetitle":"DelayEmbeddings.jl","title":"DelayEmbeddings.jl","ref":"/delayembeddings/stable/#DelayEmbeddings.jl","content":" DelayEmbeddings.jl"},{"id":437,"pagetitle":"DelayEmbeddings.jl","title":"DelayEmbeddings","ref":"/delayembeddings/stable/#DelayEmbeddings","content":" DelayEmbeddings  —  Module DelayEmbeddings.jl A Julia package that provides a generic interface for performing delay coordinate embeddings, as well as cutting edge algorithms for creating optimal embeddings given some data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"DelayEmbeddings\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source"},{"id":438,"pagetitle":"DelayEmbeddings.jl","title":"Overview","ref":"/delayembeddings/stable/#Overview","content":" Overview Note The documentation and the code of this package is parallelizing Chapter 6 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. The package provides an interface to perform delay coordinates embeddings, as explained in  homonymous page . There are two approaches for estimating optimal parameters to do delay embeddings: Separated , where one tries to find the best value for a delay time  τ  and then an optimal embedding dimension  d . Unified , where at the same time an optimal combination of  τ, d  is found. The separated approach is something \"old school\", while recent scientific research has shifted almost exclusively to unified approaches. This page describes algorithms belonging to the separated approach, which is mainly done by the function  optimal_separated_de . The unified approach is discussed in the  Unified optimal embedding  page."},{"id":441,"pagetitle":"Delay coordinates embedding","title":"Delay coordinates embedding","ref":"/delayembeddings/stable/embed/#embedding","content":" Delay coordinates embedding A timeseries recorded in some manner from a dynamical system can be used to gain information about the dynamics of the entire state space of the system. This can be done by constructing a new state space from the timeseries. One method that can do this is what is known as  delay coordinates embedding  or delay coordinates reconstruction. The main functions to use for embedding some input data are  embed  or  genembed . Both functions return a  StateSpaceSet ."},{"id":442,"pagetitle":"Delay coordinates embedding","title":"Timeseries embedding","ref":"/delayembeddings/stable/embed/#Timeseries-embedding","content":" Timeseries embedding"},{"id":443,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.embed","ref":"/delayembeddings/stable/embed/#DelayEmbeddings.embed","content":" DelayEmbeddings.embed  —  Function embed(s, d, τ [, h]) Embed  s  using delay coordinates with embedding dimension  d  and delay time  τ  and return the result as a  StateSpaceSet . Optionally use weight  h , see below. Here  τ > 0 , use  genembed  for a generalized version. Description If  τ  is an integer, then the  $n$ -th entry of the embedded space is \\[(s(n), s(n+\\tau), s(n+2\\tau), \\dots, s(n+(d-1)\\tau))\\] If instead  τ  is a vector of integers, so that  length(τ) == d-1 , then the  $n$ -th entry is \\[(s(n), s(n+\\tau[1]), s(n+\\tau[2]), \\dots, s(n+\\tau[d-1]))\\] The resulting set can have same invariant quantities (like e.g. Lyapunov exponents) with the original system that the timeseries were recorded from, for proper  d  and  τ . This is known as the Takens embedding theorem  [Takens1981] [Sauer1991] . The case of different delay times allows embedding systems with many time scales, see [Judd1998] . If provided,  h  can be weights to multiply the entries of the embedded space. If  h isa Real  then the embedding is \\[(s(n), h \\cdot s(n+\\tau), h^2 \\cdot s(n+2\\tau), \\dots,h^{d-1} \\cdot s(n+γ\\tau))\\] Otherwise  h  can be a vector of length  d-1 , which the decides the weights of each entry directly. References [Takens1981]  : F. Takens,  Detecting Strange Attractors in Turbulence — Dynamical Systems and Turbulence , Lecture Notes in Mathematics  366 , Springer (1981) [Sauer1991]  : T. Sauer  et al. , J. Stat. Phys.  65 , pp 579 (1991) source Embedding discretized data values If the data values are very strongly discretized (e.g., integers or floating-point numbers with very small bits), this can result to distances between points in the embedded space being 0. This is problematic for several library functions. Best practice here is to add noise to your original timeseries  before  embedding, e.g.,  s = s .+ 1e-15randn(length(s)) . Here are some examples of embedding a 3D continuous chaotic system: using DelayEmbeddings\n\nx = cos.(0:0.1:1) 11-element Vector{Float64}:\n 1.0\n 0.9950041652780258\n 0.9800665778412416\n 0.955336489125606\n 0.9210609940028851\n 0.8775825618903728\n 0.8253356149096783\n 0.7648421872844885\n 0.6967067093471654\n 0.6216099682706644\n 0.5403023058681398 embed(x, 3, 1) 3-dimensional StateSpaceSet{Float64} with 9 points\n 1.0       0.995004  0.980067\n 0.995004  0.980067  0.955336\n 0.980067  0.955336  0.921061\n 0.955336  0.921061  0.877583\n 0.921061  0.877583  0.825336\n 0.877583  0.825336  0.764842\n 0.825336  0.764842  0.696707\n 0.764842  0.696707  0.62161\n 0.696707  0.62161   0.540302 `τ` and `Δt` Keep in mind that whether a value of  τ  is \"reasonable\" for continuous time systems depends on the sampling time  Δt ."},{"id":444,"pagetitle":"Delay coordinates embedding","title":"Embedding Structs","ref":"/delayembeddings/stable/embed/#Embedding-Structs","content":" Embedding Structs The high level function  embed  utilizes a low-level interface for creating embedded vectors on-the-fly. The high level interface simply loops over the low level interface."},{"id":445,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.DelayEmbedding","ref":"/delayembeddings/stable/embed/#DelayEmbeddings.DelayEmbedding","content":" DelayEmbeddings.DelayEmbedding  —  Type DelayEmbedding(γ, τ, h = nothing) → `embedding` Return a delay coordinates embedding structure to be used as a function-like-object, given a timeseries and some index. Calling embedding(s, n) will create the  n -th delay vector of the embedded space, which has  γ  temporal neighbors with delay(s)  τ .  γ  is the embedding dimension minus 1,  τ  is the delay time(s) while  h  are extra weights, as in  embed  for more. Be very careful when choosing  n , because  @inbounds  is used internally.  Use  τrange ! source"},{"id":446,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.τrange","ref":"/delayembeddings/stable/embed/#DelayEmbeddings.τrange","content":" DelayEmbeddings.τrange  —  Function τrange(s, de::AbstractEmbedding) Return the range  r  of valid indices  n  to create delay vectors out of  s  using  de . source"},{"id":447,"pagetitle":"Delay coordinates embedding","title":"Generalized embeddings","ref":"/delayembeddings/stable/embed/#Generalized-embeddings","content":" Generalized embeddings"},{"id":448,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.genembed","ref":"/delayembeddings/stable/embed/#DelayEmbeddings.genembed","content":" DelayEmbeddings.genembed  —  Function genembed(s, τs, js = ones(...); ws = nothing) → ssset Create a generalized embedding of  s  which can be a timeseries or arbitrary  StateSpaceSet , and return the result as a new  StateSpaceSet . The generalized embedding works as follows: τs  denotes what delay times will be used for each of the entries of the delay vector. It is recommended that  τs[1] = 0 .  τs  is allowed to have  negative entries  as well. js  denotes which of the timeseries contained in  s  will be used for the entries of the delay vector.  js  can contain duplicate indices. ws  are optional weights that weight each embedded entry (the i-th entry of the   delay vector is weighted by  ws[i] ). If provided, it is recommended that  ws[1] == 1 . τs, js, ws  are tuples (or vectors) of length  D , which also coincides with the embedding dimension. For example, imagine input trajectory  $s = [x, y, z]$  where  $x, y, z$  are timeseries (the columns of the  StateSpaceSet ). If  js = (1, 3, 2)  and  τs = (0, 2, -7)  the created delay vector at each step  $n$  will be \\[(x(n), z(n+2), y(n-7))\\] Using  ws = (1, 0.5, 0.25)  as well would create \\[(x(n), \\frac{1}{2} z(n+2), \\frac{1}{4} y(n-7))\\] js  can be skipped, defaulting to index 1 (first timeseries) for all delay entries, while it has no effect if  s  is a timeseries instead of a  StateSpaceSet . See also  embed . Internally uses  GeneralizedEmbedding . source"},{"id":449,"pagetitle":"Delay coordinates embedding","title":"DelayEmbeddings.GeneralizedEmbedding","ref":"/delayembeddings/stable/embed/#DelayEmbeddings.GeneralizedEmbedding","content":" DelayEmbeddings.GeneralizedEmbedding  —  Type GeneralizedEmbedding(τs, js = ones(length(τs)), ws = nothing) -> `embedding` Return a delay coordinates embedding structure to be used as a function. Given a timeseries  or  trajectory (i.e.  StateSpaceSet )  s  and calling embedding(s, n) will create the delay vector of the  n -th point of  s  in the embedded space using generalized embedding (see  genembed ). js  is ignored for timeseries input  s  (since all entries of  js  must be  1  in this case) and in addition  js  defaults to  (1, ..., 1)  for all  τ . Be very careful when choosing  n , because  @inbounds  is used internally.  Use  τrange ! source"},{"id":450,"pagetitle":"Delay coordinates embedding","title":"StateSpaceSet reference","ref":"/delayembeddings/stable/embed/#StateSpaceSet-reference","content":" StateSpaceSet reference"},{"id":451,"pagetitle":"Delay coordinates embedding","title":"StateSpaceSets.StateSpaceSet","ref":"/delayembeddings/stable/embed/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. Judd1998 K. Judd & A. Mees,  Physica D  120 , pp 273 (1998) Farmer1988 Farmer & Sidorowich,  Exploiting Chaos to Predict the Future and Reduce Noise\""},{"id":456,"pagetitle":"Separated optimal embedding","title":"Separated optimal embedding","ref":"/delayembeddings/stable/separated/#Separated-optimal-embedding","content":" Separated optimal embedding This page discusses and provides algorithms for estimating optimal parameters to do Delay Coordinates Embedding (DCE) with using the separated approach."},{"id":457,"pagetitle":"Separated optimal embedding","title":"Automated function","ref":"/delayembeddings/stable/separated/#Automated-function","content":" Automated function"},{"id":458,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.optimal_separated_de","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.optimal_separated_de","content":" DelayEmbeddings.optimal_separated_de  —  Function optimal_separated_de(s, method = \"afnn\", dmethod = \"mi_min\"; kwargs...) → 𝒟, τ, E Produce an optimal delay embedding  𝒟  of the given timeseries  s  by using the  separated  approach of first finding an optimal (and constant) delay time using  estimate_delay  with the given  dmethod , and then an optimal embedding dimension, by calculating an appropriate statistic for each dimension  d ∈ 1:dmax . Return the embedding  𝒟 , the optimal delay time  τ  (the optimal embedding dimension  d  is just  size(𝒟, 2) ) and the actual statistic  E  used to estimate optimal  d . Notice that  E  is a function of the embedding dimension, which ranges from 1 to  dmax . For calculating  E  to estimate the dimension we use the given  method  which can be: \"afnn\"  (default) is Cao's \"Averaged False Nearest Neighbors\" method [Cao1997] ,   which gives a ratio of distances between nearest neighbors. \"ifnn\"  is the \"Improved False Nearest Neighbors\" from Hegger & Kantz [Hegger1999] ,   which gives the fraction of false nearest neighbors. \"fnn\"  is Kennel's \"False Nearest Neighbors\" method [Kennel1992] , which gives   the number of points that cease to be \"nearest neighbors\" when the dimension   increases. \"f1nn\"  is Krakovská's \"False First Nearest Neighbors\" method [Krakovská2015] ,   which gives the ratio of pairs of points that cease to be \"nearest neighbors\"   when the dimension increases. For more details, see individual methods:  delay_afnn ,  delay_ifnn ,  delay_fnn ,  delay_f1nn . Careful in automated methods While this method is automated if you want to be  really sure  of the results, you should directly calculate the statistic and plot its values versus the dimensions. Keyword arguments The keywords τs = 1:100, dmax = 10 denote which delay times and embedding dimensions  ds ∈ 1:dmax  to consider when calculating optimal embedding. The keywords slope_thres = 0.05, stoch_thres = 0.1, fnn_thres = 0.05 are specific to this function, see Description below. All remaining keywords are propagated to the low level functions: w, rtol, atol, τs, metric, r Description We estimate the optimal embedding dimension based on the given delay time gained from  dmethod  as follows: For Cao's method the optimal dimension is reached, when the slope of the  E₁ -statistic (output from  \"afnn\" ) falls below the threshold  slope_thres  and the according stochastic test turns out to be false, i.e. if the  E₂ -statistic's first value is  < 1 - stoch_thres . For all the other methods we return the optimal embedding dimension when the corresponding FNN-statistic (output from  \"ifnn\" ,  \"fnn\"  or  \"f1nn\" ) falls below the fnn-threshold  fnn_thres  AND the slope of the statistic falls below the threshold  slope_thres . Note that with noise contaminated time series, one might need to adjust  fnn_thres  according to the noise level. See also the file  test/compare_different_dimension_estimations.jl  for a comparison. source"},{"id":459,"pagetitle":"Separated optimal embedding","title":"Optimal delay time","ref":"/delayembeddings/stable/separated/#Optimal-delay-time","content":" Optimal delay time"},{"id":460,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.estimate_delay","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.estimate_delay","content":" DelayEmbeddings.estimate_delay  —  Function estimate_delay(s, method::String [, τs = 1:100]; kwargs...) -> τ Estimate an optimal delay to be used in  embed . The  method  can be one of the following: \"ac_zero\"  : first delay at which the auto-correlation function becomes <0. \"ac_min\"  : delay of first minimum of the auto-correlation function. \"mi_min\"  : delay of first minimum of mutual information of  s  with itself (shifted for various  τs ). Keywords  nbins, binwidth  are propagated into  selfmutualinfo . \"exp_decay\"  :  exponential_decay_fit  of the correlation function rounded  to an integer (uses least squares on  c(t) = exp(-t/τ)  to find  τ ). \"exp_extrema\"  : same as above but the exponential fit is done to the absolute value of the local extrema of the correlation function. Both the mutual information and correlation function ( autocor ) are computed  only  for delays  τs . This means that the  min  methods can never return the first value of  τs ! The method  mi_min  is significantly more accurate than the others and also returns good results for most timeseries. It is however the slowest method (but still quite fast!). source"},{"id":461,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.exponential_decay_fit","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.exponential_decay_fit","content":" DelayEmbeddings.exponential_decay_fit  —  Function exponential_decay_fit(x, y, weight = :equal) -> τ Perform a least square fit of the form  y = exp(-x/τ)  and return  τ . Taken from:  http://mathworld.wolfram.com/LeastSquaresFittingExponential.html. Assumes equal lengths of  x, y  and that  y ≥ 0 . To use the method that gives more weight to small values of  y , use  weight = :small . source"},{"id":462,"pagetitle":"Separated optimal embedding","title":"Self Mutual Information","ref":"/delayembeddings/stable/separated/#Self-Mutual-Information","content":" Self Mutual Information"},{"id":463,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.selfmutualinfo","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.selfmutualinfo","content":" DelayEmbeddings.selfmutualinfo  —  Function selfmutualinfo(s, τs; kwargs...) → m Calculate the mutual information between the time series  s  and itself delayed by  τ  points for  τ  ∈  τs , using an  improvement  of the method outlined by Fraser & Swinney in [Fraser1986] . Description The joint space of  s  and its  τ -delayed image ( sτ ) is partitioned as a rectangular grid, and the mutual information is computed from the joint and marginal frequencies of  s  and  sτ  in the grid as defined in [1]. The mutual information values are returned in a vector  m  of the same length as  τs . If any of the optional keyword parameters is given, the grid will be a homogeneous partition of the space where  s  and  sτ  are defined. The margins of that partition will be divided in a number of bins equal to  nbins , such that the width of each bin will be  binwidth , and the range of nonzero values of  s  will be in the centre. If only of those two parameters is given, the other will be automatically calculated to adjust the size of the grid to the area where  s  and  sτ  are nonzero. If no parameter is given, the space will be partitioned by a recursive bisection algorithm based on the method given in [1]. Notice that the recursive method of [1] evaluates the joint frequencies of  s  and  sτ  in each cell resulting from a partition, and stops when the data points are uniformly distributed across the sub-partitions of the following levels. For performance and stability reasons, the automatic partition method implemented in this function is only used to divide the axes of the grid, using the marginal frequencies of  s . source Notice that mutual information between two  different  timeseries x, y exists in JuliaDynamics as well, but in the package  CausalityTools.jl . It is also trivial to define it yourself using  entropy  from  ComplexityMeasures ."},{"id":464,"pagetitle":"Separated optimal embedding","title":"Optimal embedding dimension","ref":"/delayembeddings/stable/separated/#Optimal-embedding-dimension","content":" Optimal embedding dimension"},{"id":465,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_afnn","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.delay_afnn","content":" DelayEmbeddings.delay_afnn  —  Function delay_afnn(s::AbstractVector, τ:Int, ds = 2:6; metric=Euclidean(), w = 0) → E₁ Compute the parameter E₁ of Cao's \"averaged false nearest neighbors\" method for determining the minimum embedding dimension of the time series  s , with a sequence of  τ -delayed temporal neighbors. Description Given the scalar timeseries  s  and the embedding delay  τ  compute the values of  E₁  for each embedding dimension  d ∈ ds , according to Cao's Method (eq. 3 of [Cao1997] ). This quantity is a ratio of the averaged distances between the nearest neighbors of the reconstructed time series, which quantifies the increment of those distances when the embedding dimension changes from  d  to  d+1 . Return the vector of all computed  E₁ s. To estimate a good value for  d  from this, find  d  for which the value  E₁  saturates at some value around 1. Note: This method does not work for datasets with perfectly periodic signals. w  is the  Theiler window . See also:  optimal_separated_de  and  stochastic_indicator . source"},{"id":466,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_ifnn","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.delay_ifnn","content":" DelayEmbeddings.delay_ifnn  —  Function delay_ifnn(s::Vector, τ::Int, ds = 1:10; kwargs...) → `FNNs` Compute and return the  FNNs -statistic for the time series  s  and a uniform time delay  τ  and embedding dimensions  ds  after  [Hegger1999] . In this notation  γ ∈ γs = d-1 , if  d  is the embedding dimension. This fraction tends to 0 when the optimal embedding dimension with an appropriate lag is reached. Keywords * r = 2 : Obligatory threshold, which determines the maximum tolerable spreading     of trajectories in the reconstruction space. * metric = Euclidean : The norm used for distance computations. * w = 1  = The  Theiler window . See also:  optimal_separated_de . source"},{"id":467,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_fnn","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.delay_fnn","content":" DelayEmbeddings.delay_fnn  —  Function delay_fnn(s::AbstractVector, τ:Int, ds = 2:6; rtol=10.0, atol=2.0) → FNNs Calculate the number of \"false nearest neighbors\" (FNNs) of the datasets created from  s  with  embed(s, d, τ) for d ∈ ds . Description Given a dataset made by  embed(s, d, τ)  the \"false nearest neighbors\" (FNN) are the pairs of points that are nearest to each other at dimension  d , but are separated at dimension  d+1 . Kennel's criteria for detecting FNN are based on a threshold for the relative increment of the distance between the nearest neighbors ( rtol , eq. 4 in [Kennel1992] ), and another threshold for the ratio between the increased distance and the \"size of the attractor\" ( atol , eq. 5 in [Kennel1992] ). These thresholds are given as keyword arguments. The returned value is a vector with the number of FNN for each  γ ∈ γs . The optimal value for  γ  is found at the point where the number of FNN approaches zero. See also:  optimal_separated_de . source"},{"id":468,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.delay_f1nn","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.delay_f1nn","content":" DelayEmbeddings.delay_f1nn  —  Function delay_f1nn(s::AbstractVector, τ::Int, ds = 2:6; metric = Euclidean()) Calculate the ratio of \"false first nearest neighbors\" (FFNN) of the datasets created from  s  with  embed(s, d, τ) for d ∈ ds . Description Given a dataset made by  embed(s, d, τ)  the \"false first nearest neighbors\" (FFNN) are the pairs of points that are nearest to each other at dimension  d  that cease to be nearest neighbors at dimension  d+1 . The returned value is a vector with the ratio between the number of FFNN and the number of points in the dataset for each  d ∈ ds . The optimal value for  d  is found at the point where this ratio approaches zero. See also:  optimal_separated_de . source"},{"id":469,"pagetitle":"Separated optimal embedding","title":"DelayEmbeddings.stochastic_indicator","ref":"/delayembeddings/stable/separated/#DelayEmbeddings.stochastic_indicator","content":" DelayEmbeddings.stochastic_indicator  —  Function stochastic_indicator(s::AbstractVector, τ:Int, ds = 2:5) -> E₂s Compute an estimator for apparent randomness in a delay embedding with  ds  dimensions. Description Given the scalar timeseries  s  and the embedding delay  τ  compute the values of  E₂  for each  d ∈ ds , according to Cao's Method (eq. 5 of  [Cao1997] ). Use this function to confirm that the input signal is not random and validate the results of  delay_afnn . In the case of random signals, it should be  E₂ ≈ 1 ∀ d . source"},{"id":470,"pagetitle":"Separated optimal embedding","title":"Example","ref":"/delayembeddings/stable/separated/#Example","content":" Example using DelayEmbeddings, CairoMakie\nusing DynamicalSystemsBase\n\nfunction roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\nds = CoupledODEs(roessler_rule, [1, -2, 0.1], [0.2, 0.2, 5.7])\n\n# This trajectory is a chaotic attractor with fractal dim ≈ 2\n# therefore the set needs at least embedding dimension of 3\nX, tvec = trajectory(ds, 1000.0; Δt = 0.05)\nx = X[:, 1]\n\ndmax = 7\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"embedding dimension\", ylabel = \"estimator\")\nfor (i, method) in enumerate([\"afnn\", \"fnn\", \"f1nn\", \"ifnn\"])\n    # Plot statistic used to estimate optimal embedding\n    # as well as the automated output embedding\n    𝒟, τ, E = optimal_separated_de(x, method; dmax)\n    lines!(ax, 1:dmax, E; label = method, marker = :circle, color = Cycled(i))\n    optimal_d = size(𝒟, 2)\n    ## Scatter the optimal embedding dimension as a lager marker\n    scatter!(ax, [optimal_d], [E[optimal_d]];\n        color = Cycled(i), markersize = 30\n    )\nend\naxislegend(ax)\nfig Cao1997 Liangyue Cao,  Physica D, pp. 43-50 (1997) Kennel1992 M. Kennel  et al. ,  Phys. Review A  45 (6), (1992) . Krakovská2015 Anna Krakovská  et al. ,  J. Complex Sys. 932750 (2015) Hegger1999 Hegger & Kantz,  Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970 . Fraser1986 Fraser A.M. & Swinney H.L. \"Independent coordinates for strange attractors from mutual information\"  Phys. Rev. A 33 (2), 1986, 1134:1140."},{"id":473,"pagetitle":"Unified optimal embedding","title":"Unified optimal embedding","ref":"/delayembeddings/stable/unified/#Unified-optimal-embedding","content":" Unified optimal embedding Unified approaches try to create an optimal embedding by in parallel optimizing what combination of delay times and embedding dimensions suits best. In addition, the unified approaches are the only ones that can accommodate multi-variate inputs. This means that if you have multiple measured input timeseries, you should be able to take advantage of all of them for the best possible embedding of the dynamical system's set."},{"id":474,"pagetitle":"Unified optimal embedding","title":"An example","ref":"/delayembeddings/stable/unified/#An-example","content":" An example"},{"id":475,"pagetitle":"Unified optimal embedding","title":"Univariate input","ref":"/delayembeddings/stable/unified/#Univariate-input","content":" Univariate input In following we illustrate the most recent unified optimal embedding method, called PECUZAL, on three examples (see  pecuzal_embedding ). We start with a univariate case, i.e. we only feed in one time series, here the x-component of the Lorenz system. using DynamicalSystemsBase # to simulate Lorenz63\n\nfunction lorenz_rule(u, p, t)\n    σ = p[1]; ρ = p[2]; β = p[3]\n    du1 = σ*(u[2]-u[1])\n    du2 = u[1]*(ρ-u[3]) - u[2]\n    du3 = u[1]*u[2] - β*u[3]\n    return SVector(du1, du2, du3)\nend\n\nlo = CoupledODEs(lorenz_rule, [1.0, 1.0, 50.0], [10, 28, 8/3])\ntr, tvec = trajectory(lo, 100; Δt = 0.01, Ttr = 10) (3-dimensional StateSpaceSet{Float64} with 10001 points, 10.0:0.01:110.0) using DelayEmbeddings\ns = vec(tr[:, 1]) # input timeseries = x component of Lorenz\ntheiler = estimate_delay(s, \"mi_min\") # estimate a Theiler window\nTmax = 100 # maximum possible delay\n\nY, τ_vals, ts_vals, Ls, εs = pecuzal_embedding(s; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(\"τ_vals = \", τ_vals)\nprintln(\"Ls = \", Ls)\nprintln(\"L_total_uni: $(sum(Ls))\") Initializing PECUZAL algorithm for univariate input...\nStarting 1-th embedding cycle...\nStarting 2-th embedding cycle...\nStarting 3-th embedding cycle...\nAlgorithm stopped due to increasing L-values. VALID embedding achieved ✓.\nτ_vals = [0, 18, 9]\nLs = [-0.813361817423154, -0.410653828820728]\nL_total_uni: -1.224015646243882 The output reveals that PECUZAL suggests a 3-dimensional embedding out of the un-lagged time series as the 1st component of the reconstruction, the time series lagged by 18 samples as the 2nd component and the time series lagged by 9 samples as the 3rd component. In the third embedding cycle there is no  ΔL<0  and the algorithm terminates. The result after two successful embedding cycles is the 3-dimensional embedding  Y  which is also returned. The total obtained decrease of  ΔL  throughout all encountered embedding cycles has been ~ -1.24. We can also look at  continuity statistic using CairoMakie\n\nfig = Figure()\nax = Axis(fig[1,1])\nlines!(εs[:,1], label=\"1st emb. cycle\")\nscatter!([τ_vals[2]], [εs[τ_vals[2],1]])\nlines!(εs[:,2], label=\"2nd emb. cycle\")\nscatter!([τ_vals[3]], [εs[τ_vals[3],2]])\nlines!(εs[:,3], label=\"3rd emb. cycle\")\nax.title = \"Continuity statistics PECUZAL Lorenz\"\nax.xlabel = \"delay τ\"\nax.ylabel = \"⟨ε⋆⟩\"\naxislegend(ax)\nfig The picked delay values are marked with filled circles. As already mentioned, the third embedding cycle did not contribute to the embedding, i.e. there has been no delay value chosen."},{"id":476,"pagetitle":"Unified optimal embedding","title":"Multivariate input","ref":"/delayembeddings/stable/unified/#Multivariate-input","content":" Multivariate input Similar to the approach in the preceding example, we now highlight the capability of the PECUZAL embedding method for a multivariate input. The idea is now to feed in all three time series to the algorithm, even though this is a very far-from-reality example. We already have an adequate representation of the system we want to reconstruct, namely the three time series from the numerical integration. But let us see what PECUZAL suggests for a reconstruction. # compute Theiler window\nw1 = estimate_delay(tr[:,1], \"mi_min\")\nw2 = estimate_delay(tr[:,2], \"mi_min\")\nw3 = estimate_delay(tr[:,3], \"mi_min\")\nw = max(w1,w2,w3)\nY_m, τ_vals_m, ts_vals_m, = pecuzal_embedding(tr; τs = 0:Tmax , w = theiler, econ = true)\n\nprintln(τ_vals_m)\nprintln(ts_vals_m) [0, 12, 0, 84, 69, 56]\n[3, 1, 1, 1, 1, 1] PECUZAL returns a 6-dimensional embedding using the un-lagged  z - and  x -component as 1st and 3rd component of the reconstruction vectors, as well as the  x -component lagged by 12, 79, 64, and 53 samples. The total decrease of  ΔL  is ~-1.64, and thus, way smaller compared to the univariate case, as we would expect it. Nevertheless, the main contribution to this increase is made by the first two embedding cycles. For suppressing embedding cycles, which yield negligible - but negative -  ΔL -values one can use the keyword argument  L_threshold Y_mt, τ_vals_mt, ts_vals_mt, Ls_mt, εs_mt = pecuzal_embedding(tr;\n    τs = 0:Tmax, L_threshold = 0.2, w = theiler, econ = true\n)\n\nprintln(τ_vals_mt)\nprintln(ts_vals_mt) Initializing PECUZAL algorithm for multivariate input...\nStarting 1-th embedding cycle...\nStarting 2-th embedding cycle...\nStarting 3-th embedding cycle...\nAlgorithm stopped due to increasing L-values. VALID embedding achieved ✓.\n[0, 12, 0]\n[3, 1, 1] As you can see here the algorithm stopped already at 3-dimensional embedding. Let's plot these three components: ts_str = [\"x\", \"y\", \"z\"]\n\nfig = Figure(resolution = (1000,500) )\nax1 = Axis3(fig[1,1], title = \"PECUZAL reconstructed\")\nlines!(ax1, Y_mt[:,1], Y_mt[:,2], Y_mt[:,3]; linewidth = 1.0)\nax1.xlabel = \"$(ts_str[ts_vals_mt[1]])(t+$(τ_vals_mt[1]))\"\nax1.ylabel = \"$(ts_str[ts_vals_mt[2]])(t+$(τ_vals_mt[2]))\"\nax1.zlabel = \"$(ts_str[ts_vals_mt[3]])(t+$(τ_vals_mt[3]))\"\nax1.azimuth = 3π/2 + π/4\n\nax2 = Axis3(fig[1,2], title = \"original\")\nlines!(ax2, tr[:,1], tr[:,2], tr[:,3]; linewidth = 1.0, color = Cycled(2))\nax2.xlabel = \"x(t)\"\nax2.ylabel = \"y(t)\"\nax2.zlabel = \"z(t)\"\nax2.azimuth = π/2 + π/4\nfig Finally we show what PECUZAL does with a non-deterministic source: using Random\n\n# Dummy input\nRandom.seed!(1234)\nd1 = randn(1000)\nd2 = rand(1000)\nTmax = 100\ndummy_set = Dataset(d1,d2)\n\nw1 = estimate_delay(d1, \"mi_min\")\nw2 = estimate_delay(d2, \"mi_min\")\ntheiler = min(w1, w2)\n\nY_d, τ_vals_d, ts_vals_d, Ls_d , ε★_d = pecuzal_embedding(dummy_set; τs = 0:Tmax , w = theiler, econ = true)\n\nsize(Y_d) (1000,) So, no (proper) embedding is done."},{"id":477,"pagetitle":"Unified optimal embedding","title":"All unified algorithms","ref":"/delayembeddings/stable/unified/#All-unified-algorithms","content":" All unified algorithms Several algorithms have been created to implement a unified approach to delay coordinates embedding. You can find some implementations below:"},{"id":478,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.pecora","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.pecora","content":" DelayEmbeddings.pecora  —  Function pecora(s, τs, js; kwargs...) → ⟨ε★⟩, ⟨Γ⟩ Compute the (average) continuity statistic  ⟨ε★⟩  and undersampling statistic  ⟨Γ⟩  according to Pecora et al. [Pecoral2007]  (A unified approach to attractor reconstruction), for a given input  s  (timeseries or  StateSpaceSet ) and input generalized embedding defined by  (τs, js) , according to  genembed . The continuity statistic represents functional independence between the components of the existing embedding and one additional timeseries. The returned results are  matrices  with size  T x J . Keyword arguments delays = 0:50 : Possible time delay values  delays  (in sampling time units). For each of the  τ 's in  delays  the continuity-statistic  ⟨ε★⟩  gets computed. If  undersampling = true  (see further down), also the undersampling statistic  ⟨Γ⟩  gets returned for all considered delay values. J = 1:dimension(s) : calculate for all timeseries indices in  J . If input  s  is a timeseries, this is always just 1. samplesize::Real = 0.1 : determine the fraction of all phase space points (= length(s) ) to be considered (fiducial points v) to average ε★ to produce  ⟨ε★⟩, ⟨Γ⟩ K::Int = 13 : the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic).  ⟨ε★⟩  is computed taking the minimum result over all  k ∈ K . metric = Chebyshev() : metrix with which to find nearest neigbhors in the input embedding (ℝᵈ space,  d = length(τs) ). w = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. undersampling = false  : whether to calculate the undersampling statistic or not (if not, zeros are returned for  ⟨Γ⟩ ). Calculating  ⟨Γ⟩  is thousands of times slower than  ⟨ε★⟩ . db::Int = 100 : Amount of bins used into calculating the histograms of each timeseries (for the undersampling statistic). α::Real = 0.05 : The significance level for obtaining the continuity statistic p::Real = 0.5 : The p-parameter for the binomial distribution used for the computation of the continuity statistic. Description Notice that the full algorithm is too large to discuss here, and is written in detail (several pages!) in the source code of  pecora . source"},{"id":479,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.uzal_cost","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.uzal_cost","content":" DelayEmbeddings.uzal_cost  —  Function uzal_cost(Y::StateSpaceSet; kwargs...) → L Compute the L-statistic  L  for input dataset  Y  according to Uzal et al. [Uzal2011] , based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. It serves as a cost function of a state space trajectory/embedding and therefore allows to estimate a \"goodness of a embedding\" and also to choose proper embedding parameters, while minimizing  L  over the parameter space. For receiving the local cost function  L_local  (for each point in state space - not averaged), use  uzal_cost_local(...) . Keyword arguments samplesize = 0.5 : Number of considered fiducial points v as a fraction of input state space trajectory  Y 's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce  L . K = 3 : the amount of nearest neighbors considered, in order to compute σ_k^2 (read algorithm description). If given a vector, minimum result over all  k ∈ K  is returned. metric = Euclidean() : metric used for finding nearest neigbhors in the input state space trajectory `Y. w = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. Tw = 40 : The time horizon (in sampling units) up to which E_k^2 gets computed and averaged over (read algorithm description). Description The  L -statistic is based on theoretical arguments on noise amplification, the complexity of the reconstructed attractor and a direct measure of local stretch which constitutes an irrelevance measure. Technically, it is the logarithm of the product of  σ -statistic and a normalization statistic  α : L = log10(σ*α) The  σ -statistic is computed as follows.  σ = √σ² = √(E²/ϵ²) .  E²  approximates the conditional variance at each point in state space and for a time horizon  T ∈ Tw , using  K  nearest neighbors. For each reference point of the state space trajectory, the neighborhood consists of the reference point itself and its  K+1  nearest neighbors.  E²  measures how strong a neighborhood expands during  T  time steps.  E²  is averaged over many time horizons  T = 1:Tw . Consequently,  ϵ²  is the size of the neighborhood at the reference point itself and is defined as the mean pairwise distance of the neighborhood. Finally,  σ²  gets averaged over a range of reference points on the attractor, which is controlled by  samplesize . This is just for performance reasons and the most accurate result will obviously be gained when setting  samplesize=1.0 The  α -statistic is a normalization factor, such that  σ 's from different embeddings can be compared.  α²  is defined as the inverse of the sum of the inverse of all  ϵ² 's for all considered reference points. source"},{"id":480,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.garcia_almeida_embedding","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.garcia_almeida_embedding","content":" DelayEmbeddings.garcia_almeida_embedding  —  Function garcia_almeida_embedding(s; kwargs...) → Y, τ_vals, ts_vals, FNNs ,NS A unified approach to properly embed a time series ( Vector  type) or a set of time series ( StateSpaceSet  type) based on the papers of Garcia & Almeida  [Garcia2005a] , [Garcia2005b] . Keyword arguments τs= 0:50 : Possible delay values  τs  (in sampling time units). For each of the  τs 's the N-statistic gets computed. w::Int = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. r1 = 10 : The threshold, which defines the factor of tolerable stretching for the d_E1-statistic. r2 = 2 : The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension. fnn_thres= 0.05 : A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1). T::Int = 1 : The forward time step (in sampling units) in order to compute the  d_E2 -statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to  T=1 . metric = Euclidean() : metric used for finding nearest neigbhors in the input phase space trajectory  Y . max_num_of_cycles = 50 : The algorithm will stop after that many cycles no matter what. Description The method works iteratively and gradually builds the final embedding vectors  Y . Based on the  N -statistic the algorithm picks an optimal delay value  τ  for each embedding cycle as the first local minimum of  N . In case of multivariate embedding, i.e. when embedding a set of time series ( s::StateSpaceSet ), the optimal delay value  τ  is chosen as the first minimum from all minimum's of all considered  N -statistics for each embedding cycle. The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . After each embedding cycle the FNN-statistic  FNNs [Hegger1999] [Kennel1992]  is being checked and as soon as this statistic drops below the threshold  fnn_thres , the algorithm breaks. In order to increase the  practability of the method the algorithm also breaks, when the FNN-statistic  FNNs  increases . The final embedding vector is stored in  Y  ( StateSpaceSet ). The chosen delay values for each embedding cycle are stored in the  τ_vals  and the according time series number chosen for the according delay value in  τ_vals  is stored in  ts_vals . For univariate embedding ( s::Vector )  ts_vals  is a vector of ones of length  τ_vals , because there is simply just one time series to choose from. The function also returns the  N -statistic  NS  for each embedding cycle as an  Array  of  Vector s. Notice that we were  not  able to reproduce the figures from the papers with our implementation (which nevertheless we believe is the correct one). source"},{"id":481,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.mdop_embedding","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.mdop_embedding","content":" DelayEmbeddings.mdop_embedding  —  Function mdop_embedding(s::Vector; kwargs...) → Y, τ_vals, ts_vals, FNNs, βS MDOP (for \"maximizing derivatives on projection\") is a unified approach to properly embed a timeseries or a set of timeseries ( StateSpaceSet ) based on the paper of Chetan Nichkawde  [Nichkawde2013] . Keyword arguments τs= 0:50 : Possible delay values  τs . For each of the  τs 's the β-statistic gets computed. w::Int = 1 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. fnn_thres::Real= 0.05 : A threshold value defining a sufficiently small fraction of false nearest neighbors, in order to the let algorithm terminate and stop the embedding procedure (`0 ≤ fnn_thres < 1). r::Real = 2 : The threshold for the tolerable relative increase of the distance between the nearest neighbors, when increasing the embedding dimension. max_num_of_cycles = 50 : The algorithm will stop after that many cycles no matter what. Description The method works iteratively and gradually builds the final embedding  Y . Based on the  beta_statistic  the algorithm picks an optimal delay value  τ  for each embedding cycle as the global maximum of  β . In case of multivariate embedding, i.e. when embedding a set of time series ( s::StateSpaceSet ), the optimal delay value  τ  is chosen as the maximum from all maxima's of all considered  β -statistics for each possible timeseries. The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . After each embedding cycle the FNN-statistic  FNNs [Hegger1999] [Kennel1992]  is being checked and as soon as this statistic drops below the threshold  fnn_thres , the algorithm terminates. In order to increase the practability of the method the algorithm also terminates when the FNN-statistic  FNNs  increases. The final embedding is returned as  Y . The chosen delay values for each embedding cycle are stored in the  τ_vals  and the according timeseries index chosen for the the respective according delay value in  τ_vals  is stored in  ts_vals .  βS, FNNs  are returned for clarity and double-checking, since they are computed anyway. In case of multivariate embedding,  βS  will store all  β -statistics for all available time series in each embedding cycle. To double-check the actual used  β -statistics in an embedding cycle 'k', simply  βS[k][:,ts_vals[k+1]] . source"},{"id":482,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.pecuzal_embedding","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.pecuzal_embedding","content":" DelayEmbeddings.pecuzal_embedding  —  Function pecuzal_embedding(s; kwargs...) → Y, τ_vals, ts_vals, ΔLs, ⟨ε★⟩ A unified approach to properly embed a timeseries or a set of timeseries ( StateSpaceSet ) based on the recent PECUZAL algorithm due to Kraemer et al. [Kraemer2021] . For more details, see the description below. Keyword arguments τs = 0:50 : Possible delay values  τs  (in sampling time units). For each of the  τs 's the continuity statistic ⟨ε★⟩ gets computed and further processed in order to find optimal delays  τᵢ  for each embedding cycle  i . w::Int = 0 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. samplesize::Real = 1.0 : Fraction of state space points to be considered (fiducial points v) to average ε★ over, in order to produce  ⟨ε★⟩ . Lower fraction value decreases accuracy as well as computation time. K::Int = 13 : the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic).  ⟨ε★⟩  is computed taking the minimum result over all  k ∈ K . KNN::Int = 3 : the amount of nearest neighbors considered, in order to compute σ k^2 (read algorithm description [`uzal cost ]@ref). If given a vector, the minimum result over all knn ∈ KNN` is returned. L_threshold::Real = 0 : The algorithm breaks, when this threshold is exceeded by  ΔL  in an embedding cycle (set as a positive number, i.e. an absolute value of  ΔL ). α::Real = 0.05 : The significance level for obtaining the continuity statistic p::Real = 0.5 : The p-parameter for the binomial distribution used for the computation of the continuity statistic ⟨ε★⟩. max_cycles = 50 : The algorithm will stop after that many cycles no matter what. econ::Bool = false : Economy-mode for L-statistic computation. Instead of computing L-statistics for time horizons  2:Tw , here we only compute them for  2:2:Tw , see description for further details. verbose = true : Print information about the process. Description The method works iteratively and gradually builds the final embedding vectors  Y . Based on the  ⟨ε★⟩ -statistic (of  pecora ) the algorithm picks an optimal delay value  τᵢ  for each embedding cycle  i . For achieving that, we take the inpute time series  s , denoted as the actual phase space trajectory  Y_actual  and compute the continuity statistic  ⟨ε★⟩ . Each local maxima in  ⟨ε★⟩  is used for constructing a candidate embedding trajectory  Y_trial  with a delay corresponding to that specific peak in  ⟨ε★⟩ . We then compute the  L -statistic (of  uzal_cost ) for  Y_trial  ( L-trial ) and  Y_actual  ( L_actual ) for increasing prediction time horizons (free parameter in the  L -statistic) and save the maximum difference  max(L-trial - L_actual)  as  ΔL  (Note that this is a negative number, since the  L -statistic decreases with better reconstructions). We pick the  τ -value, for which  ΔL  is minimal (=maximum decrease of the overall  L -value) and construct the actual embedding trajectory  Y_actual  (steps 1.-3. correspond to an embedding cycle). We repeat steps 1.-3. with  Y_actual  as input and stop the algorithm when  ΔL  is > 0, i.e. when and additional embedding component would not lead to a lower overall L-value.  Y_actual  ->  Y . In case of multivariate embedding, i.e. when embedding a set of M time series ( s::StateSpaceSet ), in each embedding cycle the continuity statistic  ⟨ε★⟩  gets computed for all M time series available. The optimal delay value  τ  in each embedding cycle is chosen as the peak/ τ -value for which  ΔL  is minimal under all available peaks and under all M  ⟨ε★⟩ 's. In the first embedding cycle there will be M² different  ⟨ε★⟩ 's to consider, since it is not clear a priori which time series of the input should consitute the first component of the embedding vector and form  Y_actual . The range of considered delay values is determined in  τs  and for the nearest neighbor search we respect the Theiler window  w . The final embedding vector is stored in  Y  ( StateSpaceSet ). The chosen delay values for each embedding cycle are stored in  τ_vals  and the according time series numbers chosen for each delay value in  τ_vals  are stored in  ts_vals . For univariate embedding ( s::Vector )  ts_vals  is a vector of ones of length  τ_vals , because there is simply just one timeseries to choose from. The function also returns the  ΔLs -values for each embedding cycle and the continuity statistic  ⟨ε★⟩  as an  Array  of  Vector s. For distance computations the Euclidean norm is used. source"},{"id":483,"pagetitle":"Unified optimal embedding","title":"Low-level functions of unified approach","ref":"/delayembeddings/stable/unified/#Low-level-functions-of-unified-approach","content":" Low-level functions of unified approach"},{"id":484,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.n_statistic","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.n_statistic","content":" DelayEmbeddings.n_statistic  —  Function n_statistic(Y, s; kwargs...) → N, d_E1 Perform one embedding cycle according to the method proposed in  [Garcia2005a]  for a given phase space trajectory  Y  (of type  StateSpaceSet ) and a time series  s (of type Vector ). Return the proposed N-Statistic N and all nearest neighbor distances d_E1 for each point of the input phase space trajectory Y . Note that Y` is a single time series in case of the first embedding cycle. Keyword arguments τs= 0:50 : Considered delay values  τs  (in sampling time units). For each of the  τs 's the N-statistic gets computed. r = 10 : The threshold, which defines the factor of tolerable stretching for the d_E1-statistic (see algorithm description). T::Int = 1 : The forward time step (in sampling units) in order to compute the  d_E2 -statistic (see algorithm description). Note that in the paper this is not a free parameter and always set to  T=1 . w::Int = 0 : Theiler window (neighbors in time with index  w  close to the point, that are excluded from being true neighbors).  w=0  means to exclude only the point itself, and no temporal neighbors. Note that in the paper this is not a free parameter and always  w=0 . metric = Euclidean() : metric used for finding nearest neigbhors in the input phase space trajectory  Y . Description For a range of possible delay values  τs  one constructs a temporary embedding matrix. That is, one concatenates the input phase space trajectory  Y  with the  τ -lagged input time series  s . For each point on the temporary trajectory one computes its nearest neighbor, which is denoted as the  d_E1 -statistic for a specific  τ . Now one considers the distance between the reference point and its nearest neighbor  T  sampling units ahead and calls this statistic  d_E2 .  [Garcia2005a]  strictly use  T=1 , so they forward each reference point and its corresponding nearest neighbor just by one (!) sampling unit. Here it is a free parameter. The  N -statistic is then the fraction of  d_E2 / d_E1 -pairs which exceed a threshold  r . Plotted vs. the considered  τs -values it is proposed to pick the  τ -value for this embedding cycle as the value, where  N  has its first local minimum. source"},{"id":485,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.beta_statistic","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.beta_statistic","content":" DelayEmbeddings.beta_statistic  —  Function beta_statistic(Y::StateSpaceSet, s::Vector) [, τs, w]) → β Compute the β-statistic  β  for input state space trajectory  Y  and a timeseries  s  according to Nichkawde  [Nichkawde2013] , based on estimating derivatives on a projected manifold. For a range of delay values  τs ,  β  gets computed and its maximum over all considered  τs  serves as the optimal delay considered in this embedding cycle. Arguments  τs, w  as in  mdop_embedding . Description The  β -statistic is based on the geometrical idea of maximal unfolding of the reconstructed attractor and is tightly related to the False Nearest Neighbor method ( [Kennel1992] ). In fact the method eliminates the maximum amount of false nearest neighbors in each embedding cycle. The idea is to estimate the absolute value of the directional derivative with respect to a possible new dimension in the reconstruction process, and with respect to the nearest neighbor, for all points of the state space trajectory: ϕ'(τ) = Δϕ d(τ) / Δx d Δx d is simply the Euclidean nearest neighbor distance for a reference point with respect to the given Theiler window  w . Δϕ d(τ) is the distance of the reference point to its nearest neighbor in the one dimensional time series  s , for the specific τ. Δϕ_d(τ) = |s(i+τ)-s(j+τ)|, with i being the index of the considered reference point and j the index of its nearest neighbor. Finally, β  = log β(τ) = ⟨log₁₀ ϕ'(τ)⟩ , with ⟨.⟩ being the mean over all reference points. When one chooses the maximum of  β  over all considered τ's, one obtains the optimal delay value for this embedding cycle. Note that in the first embedding cycle, the input state space trajectory  Y  can also be just a univariate time series. source"},{"id":486,"pagetitle":"Unified optimal embedding","title":"DelayEmbeddings.mdop_maximum_delay","ref":"/delayembeddings/stable/unified/#DelayEmbeddings.mdop_maximum_delay","content":" DelayEmbeddings.mdop_maximum_delay  —  Function mdop_maximum_delay(s, tw = 1:50, samplesize = 1.0)) -> τ_max, L Compute an upper bound for the search of optimal delays, when using  mdop_embedding mdop_embedding  or  beta_statistic beta_statistic . Description The input time series  s  gets embedded with unit lag and increasing dimension, for dimensions (or time windows)  tw  ( RangeObject ). For each of such a time window the  L -statistic from Uzal et al.  [Uzal2011]  will be computed.  samplesize  determines the fraction of points to be considered in the computation of  L  (see  uzal_cost ). When this statistic reaches its global minimum the maximum delay value  τ_max  gets returned. When  s  is a multivariate  StateSpaceSet ,  τ_max  will becomputed for all timeseries of that StateSpaceSet and the maximum value will be returned. The returned  L -statistic has size  (length(tw), size(s,2)) . source Pecora2007 Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007).  A unified approach to attractor reconstruction. Chaos 17(1) . Uzal2011 Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011).  Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223 . Garcia2005a Garcia, S. P., Almeida, J. S. (2005).  Nearest neighbor embedding with different time delays. Physical Review E 71, 037204 . Garcia2005b Garcia, S. P., Almeida, J. S. (2005).  Multivariate phase space reconstruction by nearest neighbor embedding with different time delays. Physical Review E 72, 027205 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Hegger1999 Hegger, Rainer and Kantz, Holger (1999).  Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970 . Kennel1992 Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992).  Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403 . Kraemer2021 Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2021)  A unified and automated approach to attractor reconstruction. New Journal of Physics 23(3), 033017 . Garcia2005a Garcia, S. P., Almeida, J. S. (2005).  Nearest neighbor embedding with different time delays. Physical Review E 71, 037204 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Kennel1992 Kennel, M. B., Brown, R., Abarbanel, H. D. I. (1992).  Determining embedding dimension for state-space reconstruction using a geometrical construction. Phys. Rev. A 45, 3403 . Nichkawde2013 Nichkawde, Chetan (2013).  Optimal state-space reconstruction using derivatives on projected manifold. Physical Review E 87, 022905 . Uzal2011 Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011).  Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223 ."},{"id":489,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.jl","ref":"/fractaldimensions/stable/#FractalDimensions.jl","content":" FractalDimensions.jl"},{"id":490,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions","ref":"/fractaldimensions/stable/#FractalDimensions","content":" FractalDimensions  —  Module FractalDimensions.jl A Julia package that estimates various definitions of fractal dimension from data. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"FractalDimensions\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was part of ChaosTools.jl. Citation If you use this package in a publication, please cite the paper below: @ARTICLE{FractalDimensions.jl,\n  title     = \"Estimating the fractal dimension: a comparative review and open\n               source implementations\",\n  author    = \"Datseris, George and Kottlarz, Inga and Braun, Anton P and\n               Parlitz, Ulrich\",\n  publisher = \"arXiv\",\n  year      =  2021,\n  doi = {10.48550/ARXIV.2109.05937},\n  url = {https://arxiv.org/abs/2109.05937},\n} source"},{"id":491,"pagetitle":"FractalDimensions.jl","title":"Introduction","ref":"/fractaldimensions/stable/#Introduction","content":" Introduction Note This package is accompanying a review paper on estimating the fractal dimension:  https://arxiv.org/abs/2109.05937 . The paper is continuing the discussion of chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. There are numerous methods that one can use to calculate a so-called \"dimension\" of a dataset which in the context of dynamical systems is called the  Fractal dimension . One way to do this is to estimate the  scaling behaviour of some quantity as a size/scale increases . In the  Fractal dimension example  below, one finds the scaling of the correlation sum versus a ball radius. In this case, it approximately holds $ \\log(C) \\approx \\Delta\\log(\\varepsilon) $ for radius  $\\varepsilon$ . The scaling of many other quantities can be estimated as well, such as the generalized entropy, the Higuchi length, or others provided here. To actually find  $\\Delta$ , one needs to find a linearly scaling region in the graph  $\\log(C)$  vs.  $\\log(\\varepsilon)$  and estimate its slope. Hence,  identifying a linear region is central to estimating a fractal dimension . That is why, the section  Linear scaling regions  is of central importance for this documentation."},{"id":492,"pagetitle":"FractalDimensions.jl","title":"Fractal dimension example","ref":"/fractaldimensions/stable/#Fractal-dimension-example","content":" Fractal dimension example In this simplest example we will calculate the fractal dimension of the  chaotic attractor of the Hénon map  (for default parameters). For this example, we will generate the data on the spot: using DynamicalSystemsBase # for simulating dynamical systems\nusing CairoMakie           # for plotting\n\nhenon_rule(x, p, n) = SVector(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nu0 = zeros(2)\np0 = [1.4, 0.3]\nhenon = DeterministicIteratedMap(henon_rule, u0, p0)\n\nX, t = trajectory(henon, 20_000; Ttr = 100)\nscatter(X[:, 1], X[:, 2]; color = (\"black\", 0.01), markersize = 4) instead of simulating the set  X  we could load it from disk, e.g., if there was a text file with two columns as x and y coordinates, we would load it as using DelimitedFiles\nfile = \"path/to/file.csv\"\nM = readdlm(file)    # here `M` is a metrix with two columns\nX = StateSpaceSet(M) # important to convert to a state space set After we have  X , we can start computing a fractal dimension and for this example we will use the  correlationsum . Our goal is to compute the correlation sum of  X  for many different sizes/radii  ε . This is as simple as using FractalDimensions\nες = 2 .^ (-15:0.5:5) # semi-random guess\nCs = correlationsum(X, ες; show_progress = false) 41-element Vector{Float64}:\n 1.8799060046997648e-6\n 2.884855757212139e-6\n 4.514774261286935e-6\n 7.204639768011599e-6\n 1.1174441277936103e-5\n 1.7944102794860256e-5\n 2.8333583320833955e-5\n 4.5037748112594364e-5\n 6.943652817359132e-5\n 0.0001071696415179241\n ⋮\n 0.9486205889705515\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999\n 0.9999999999999999 For a fractal set  X  dynamical systems theory says that there should be an exponential relationship between the correlation sum and the sizes: xs = log2.(ες)\nys = log2.(Cs)\nscatterlines(xs, ys; axis = (ylabel = L\"\\log(C_2)\", xlabel = L\"\\log (\\epsilon)\")) The slope of the linear scaling region of the above plot is the fractal dimension (based on the correlation sum). Given that we  see  the plot, we can estimate where the linear scaling region starts and ends. This is generally done using  LargestLinearRegion  in  slopefit . But first, let's visualize what the method does, as it uses  linear_regions . lrs, slopes = linear_regions(xs, ys, tol = 0.25)\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = L\"\\log(C_2)\", xlabel = L\"\\log (\\epsilon)\")\nfor r in lrs\n    scatterlines!(ax, xs[r], ys[r])\nend\nfig The  LargestLinearRegion  method finds, and computes the slope of, the largest region: Δ = slopefit(xs, ys, LargestLinearRegion()) (1.2318376178087478, 1.2233720116518771, 1.2403032239656184) This result is an approximation of  a  fractal dimension. The whole above pipeline we went through is bundled in  grassberger_proccacia_dim . Similar work is done by  generalized_dim  and many other functions. Be wary when using `xxxxx_dim` As stated clearly by the documentation strings, all pre-made dimension estimating functions (ending in  _dim ) perform a lot of automated steps, each having its own heuristic choices for function default values. They are more like convenient bundles with on-average good defaults, rather than precise functions. You should be careful when considering the validity of the returned number!"},{"id":493,"pagetitle":"FractalDimensions.jl","title":"Linear scaling regions","ref":"/fractaldimensions/stable/#Linear-scaling-regions","content":" Linear scaling regions"},{"id":494,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.slopefit","ref":"/fractaldimensions/stable/#FractalDimensions.slopefit","content":" FractalDimensions.slopefit  —  Function slopefit(x, y [, t::SLopeFit]; kw...) → s, s05, s95 Fit a linear scaling region in the curve of the two  AbstractVectors y  versus  x  using  t  as the estimation method. Return the estimated slope, as well as the confidence intervals for it. The methods  t  that can be used for the estimation are: LinearRegression LargestLinearRegion  (default) AllSlopesDistribution The keyword  ignore_saturation = true  ignores saturation that (sometimes) happens at the start and end of the curve  y(x) , where the curve flattens. The keyword  sat_threshold = 0.01  decides what saturation is: while  abs(y[i]-y[i+1]) < sat_threshold  we are in a saturation regime. Said differently, slopes with value  sat_threshold/dx  with  dx = x[i+1] - x[i]  are neglected. The keyword  ci = 0.95  specifies which quantile (and the 1 - quantile) the confidence interval values are returned at, and by defualt it is 95% (and hence also 5%). source"},{"id":495,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.LinearRegression","ref":"/fractaldimensions/stable/#FractalDimensions.LinearRegression","content":" FractalDimensions.LinearRegression  —  Type LinearRegression <: SLopeFit\nLinearRegression() Standard linear regression fit to all available data. Estimation of the confidence intervals is based om the standard error of the slope following a T-distribution, see: https://stattrek.com/regression/slope-confidence-interval source"},{"id":496,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.LargestLinearRegion","ref":"/fractaldimensions/stable/#FractalDimensions.LargestLinearRegion","content":" FractalDimensions.LargestLinearRegion  —  Type LargestLinearRegion <: SlopeFit\nLargestLinearRegion(; dxi::Int = 1, tol = 0.25) Identify regions where the curve  y(x)  is linear, by scanning the  x -axis every  dxi  indices sequentially (e.g. at  x[1]  to  x[5] ,  x[5]  to  x[10] ,  x[10]  to  x[15]  and so on if  dxi=5 ). If the slope (calculated via linear regression) of a region of width  dxi  is approximatelly equal to that of the previous region, within relative tolerance  tol  and absolute tolerance  0 , then these two regions belong to the same linear region. The largest such region is then used to estimate the slope via standard linear regression of all points belonging to the largest linear region. \"Largest\" here means the region that covers the more extent along the  x -axis. Use  linear_regions  if you wish to obtain the decomposition into linear regions. source"},{"id":497,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.linear_regions","ref":"/fractaldimensions/stable/#FractalDimensions.linear_regions","content":" FractalDimensions.linear_regions  —  Function linear_regions(x, y; dxi, tol) → lrs, tangents Apply the algorithm described by  LargestLinearRegion , and return the indices of  x  that correspond to the linear regions,  lrs , and the  tangents  at each region (obtained via a second linear regression at each accumulated region).  lrs  is hence a vector of  UnitRange s. source"},{"id":498,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.AllSlopesDistribution","ref":"/fractaldimensions/stable/#FractalDimensions.AllSlopesDistribution","content":" FractalDimensions.AllSlopesDistribution  —  Type AllSlopesDistribution <: SlopeFit\nAllSlopesDistribution() Estimate a slope by computing the distribution of all possible slopes that can be estimated from the curve  y(x) , according to the method by  [Deshmukh2021] . The returned slope is the distribution mean and the confidence intervals are simply the corresponding quantiles of the distribution. Not implemented yet, the method is here as a placeholder. source"},{"id":499,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_boxsizes","ref":"/fractaldimensions/stable/#FractalDimensions.estimate_boxsizes","content":" FractalDimensions.estimate_boxsizes  —  Function estimate_boxsizes(X::AbstractStateSpaceSet; kwargs...) → εs Return  k  exponentially spaced values:  εs = base .^ range(lower + w, upper + z; length = k) , that are a good estimate for sizes ε that are used in calculating a  Fractal Dimension . It is strongly recommended to  standardize  input dataset before using this function. Let  d₋  be the minimum pair-wise distance in  X ,  d₋ = dminimum_pairwise_distance(X) . Let  d₊  be the average total length of  X ,  d₊ = mean(ma - mi)  with  mi, ma = minmaxima(X) . Then  lower = log(base, d₋)  and  upper = log(base, d₊) . Because by default  w=1, z=-1 , the returned sizes are an order of mangitude larger than the minimum distance, and an order of magnitude smaller than the maximum distance. Keywords w = 1, z = -1, k = 16  : as explained above. base = MathConstants.e  : the base used in the  log  function. warning = true : Print some warnings for bad estimates. autoexpand = true : If the final estimated range does not cover at least 2 orders of magnitude, it is automatically expanded by setting  w -= we  and  z -= ze . You can set different default values to the keywords  we = w, ze = z . source"},{"id":500,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.minimum_pairwise_distance","ref":"/fractaldimensions/stable/#FractalDimensions.minimum_pairwise_distance","content":" FractalDimensions.minimum_pairwise_distance  —  Function minimum_pairwise_distance(X::StateSpaceSet, kdtree = dimension(X) < 10, metric = Euclidean()) Return  min_d, min_pair : the minimum pairwise distance of all points in the dataset, and the corresponding point pair. The third argument is a switch of whether to use KDTrees or a brute force search. source"},{"id":501,"pagetitle":"FractalDimensions.jl","title":"Generalized (entropy) dimension","ref":"/fractaldimensions/stable/#Generalized-(entropy)-dimension","content":" Generalized (entropy) dimension Based on the definition of the Generalized entropy ( genentropy ), one can calculate an appropriate dimension, called  generalized dimension :"},{"id":502,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.generalized_dim","ref":"/fractaldimensions/stable/#FractalDimensions.generalized_dim","content":" FractalDimensions.generalized_dim  —  Function generalized_dim(X::StateSpaceSet [, sizes]; q = 1, base = 2) -> Δ_q Return the  q  order generalized dimension of  X , by calculating  its histogram-based Rényi entropy for each  ε ∈ sizes . The case of  q = 0  is often called \"capacity\" or \"box-counting\" dimension, while  q = 1  is the \"information\" dimension. Description The returned dimension is approximated by the (inverse) power law exponent of the scaling of the Renyi entropy  $H_q$ , versus the box size  ε , where  ε ∈ sizes : \\[H_q \\approx -\\Delta_q\\log_{b}(\\varepsilon)\\] $H_q$  is calculated using  ComplexityMeasures: Renyi, ValueHistogram, entropy , i.e., by doing a histogram of the data with a given box size. Calling this function performs a lot of automated steps: A vector of box sizes is decided by calling  sizes = estimate_boxsizes(dataset) , if  sizes  is not given. For each element of  sizes  the appropriate entropy is calculated as H = [entropy(Renyi(; q, base), ValueHistogram(ε), data) for ε ∈ sizes] Let  x = -log.(sizes) . The curve  H(x)  is decomposed into linear regions, using  slopefit (x, h)[1] . The biggest linear region is chosen, and a fit for the slope of that region is performed using the function  linear_region , which does a simple linear regression fit using  linreg . This slope is the return value of  generalized_dim . By doing these steps one by one yourself, you can adjust the keyword arguments given to each of these function calls, refining the accuracy of the result. The source code of this function is only 3 lines of code. source"},{"id":503,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.molteno_dim","ref":"/fractaldimensions/stable/#FractalDimensions.molteno_dim","content":" FractalDimensions.molteno_dim  —  Function molteno_dim(X::AbstractStateSpaceSet; k0::Int = 10, q = 1.0, base = 2) Return an estimate of the  generalized_dim  of  X  using the algorithm by  [Molteno1993] . This function is a simple utilization of the probabilities estimated by  molteno_boxing  so see that function for more details. Here the entropy of the probabilities is computed at each size, and a line is fitted in the entropy vs log(size) graph, just like in  generalized_dim . source"},{"id":504,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.molteno_boxing","ref":"/fractaldimensions/stable/#FractalDimensions.molteno_boxing","content":" FractalDimensions.molteno_boxing  —  Function molteno_boxing(X::AbstractStateSpaceSet; k0::Int = 10) → (probs, εs) Distribute  X  into boxes whose size is halved in each step, according to the algorithm by  [Molteno1993] . Division stops if the average number of points per filled box falls below the threshold  k0 . Return  probs , a vector of  Probabilities  of finding points in boxes for different box sizes, and the corresponding box sizes  εs . These outputs are used in  molteno_dim . Description Project the  data  onto the whole interval of numbers that is covered by  UInt64 . The projected data is distributed into boxes whose size decreases by factor 2 in each step. For each box that contains more than one point  2^D  new boxes are created where  D  is the dimension of the data. The process of dividing the data into new boxes stops when the number of points over the number of filled boxes falls below  k0 . The box sizes  εs  are calculated and returned together with the  probs . This algorithm is faster than the traditional approach of using  ValueHistogram(ε::Real) , but it is only suited for low dimensional data since it divides each box into  2^D  new boxes if  D  is the dimension. For large  D  this leads to low numbers of box divisions before the threshold is passed and the divison stops. This results to a low number of data points to fit the dimension to and thereby a poor estimate. source"},{"id":505,"pagetitle":"FractalDimensions.jl","title":"Correlation sum based dimension","ref":"/fractaldimensions/stable/#Correlation-sum-based-dimension","content":" Correlation sum based dimension"},{"id":506,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.grassberger_proccacia_dim","ref":"/fractaldimensions/stable/#FractalDimensions.grassberger_proccacia_dim","content":" FractalDimensions.grassberger_proccacia_dim  —  Function grassberger_proccacia_dim(X::AbstractStateSpaceSet, εs = estimate_boxsizes(data); kwargs...) Use the method of Grassberger and Proccacia [Grassberger1983] , and the correction by Theiler [Theiler1986] , to estimate the correlation dimension  Δ_C  of   X . This function does something extremely simple: cm = correlationsum(data, εs; kwargs...)\nΔ_C = slopefit(rs, ys)(log2.(sizes), log2.(cm))[1] i.e. it calculates  correlationsum  for various radii and then tries to find a linear region in the plot of the log of the correlation sum versus log(ε). See  correlationsum  for the available keywords. See also  takens_best_estimate ,  boxassisted_correlation_dim . source"},{"id":507,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.correlationsum","ref":"/fractaldimensions/stable/#FractalDimensions.correlationsum","content":" FractalDimensions.correlationsum  —  Function correlationsum(X, ε::Real; w = 0, norm = Euclidean(), q = 2) → C_q(ε) Calculate the  q -order correlation sum of  X  ( StateSpaceSet  or timeseries) for a given radius  ε  and  norm . They keyword  show_progress = true  can be used to display a progress bar for large  X . correlationsum(X, εs::AbstractVector; w, norm, q) → C_q(ε) If  εs  is a vector,  C_q  is calculated for each  ε ∈ εs  more efficiently. Multithreading is also enabled over the available threads ( Threads.nthreads() ). The function  boxed_correlationsum  is typically faster if the dimension of  X  is small and if  maximum(εs)  is smaller than the size of  X . Keyword arguments q = 2 : order of the correlation sum norm = Euclidean() : distance norm w = 0 : Theiler window show_progress = true : display a progress bar Description The correlation sum is defined as follows for  q=2 : \\[C_2(\\epsilon) = \\frac{2}{(N-w)(N-w-1)}\\sum_{i=1}^{N}\\sum_{j=1+w+i}^{N}\nB(||X_i - X_j|| < \\epsilon)\\] for as follows for  q≠2 \\[C_q(\\epsilon) = \\left[ \\sum_{i=1}^{N} \\alpha_i\n\\left[\\sum_{j:|i-j| > w} B(||X_i - X_j|| < \\epsilon)\\right]^{q-1}\\right]^{1/(q-1)}\\] where \\[\\alpha_i = 1 / (N (\\max(N-w, i) - \\min(w + 1, i))^{(q-1)})\\] with  $N$  the length of  X  and  $B$  gives 1 if its argument is  true .  w  is the  Theiler window . See the article of Grassberger for the general definition  [Grassberger2007]  and the book \"Nonlinear Time Series Analysis\"  [Kantz2003] , Ch. 6, for a discussion around choosing best values for  w , and Ch. 11.3 for the explicit definition of the q-order correlationsum. Note that the formula in 11.3 is incorrect, but corrected here, indices are adapted to take advantage of all available  points and also note that we immediatelly exponentiate  $C_q$  to  $1/(q-1)$ , so that it scales exponentially as  $C_q \\propto \\varepsilon ^\\Delta_q$  versus the size  $\\varepsilon$ . source"},{"id":508,"pagetitle":"FractalDimensions.jl","title":"Box-assisted version","ref":"/fractaldimensions/stable/#Box-assisted-version","content":" Box-assisted version"},{"id":509,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.boxassisted_correlation_dim","ref":"/fractaldimensions/stable/#FractalDimensions.boxassisted_correlation_dim","content":" FractalDimensions.boxassisted_correlation_dim  —  Function boxassisted_correlation_dim(X::AbstractStateSpaceSet; kwargs...) Use the box-assisted optimizations of  [Bueno2007]  to estimate the correlation dimension  Δ_C  of  X . This function does something extremely simple: εs, Cs = boxed_correlationsum(X; kwargs...)\nslopefit(log2.(εs), log2.(Cs))[1] and hence see  boxed_correlationsum  for more information and available keywords. source"},{"id":510,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.boxed_correlationsum","ref":"/fractaldimensions/stable/#FractalDimensions.boxed_correlationsum","content":" FractalDimensions.boxed_correlationsum  —  Function boxed_correlationsum(X::AbstractStateSpaceSet, εs, r0 = maximum(εs); kwargs...) → Cs Estimate the  correlationsum  for each size  ε ∈ εs  using an optimized algorithm that first distributes data into boxes of size  r0 , and then computes the correlation sum for each box and each neighboring box of each box. This method is much faster than  correlationsum ,  provided that  the box size  r0  is significantly smaller than the attractor length. Good choices for  r0  are  estimate_r0_buenoorovio  or  estimate_r0_theiler . boxed_correlationsum(X::AbstractStateSpaceSet; kwargs...) → εs, Cs In this method the minimum inter-point distance and  estimate_r0_buenoorovio  of  X  are used to estimate suitable  εs  for the calculation, which are also returned. Keyword arguments q = 2  : The order of the correlation sum. P = 2  : The prism dimension. w = 0  : The  Theiler window . show_progress = false  : Whether to display a progress bar for the calculation. norm = Euclidean()  : Distance norm. Description C_q(ε)  is calculated for every  ε ∈ εs  and each of the boxes to then be summed up afterwards. The method of splitting the data into boxes was implemented according to Theiler [Theiler1987] .  w  is the  Theiler window .  P  is the prism dimension. If  P  is unequal to the dimension of the data, only the first  P  dimensions are considered for the box distribution (this is called the prism-assisted version). By default  P  is 2, which is the version suggested by  [Bueno2007] . Alternative for  P  is the  prismdim_theiler . Note that only when  P = dimension(X)  the boxed version is guaranteed to be exact to the original  correlationsum . For any other  P , some point pairs that should have been included may be skipped due to having smaller distance in the remaining dimensions, but larger distance in the first  P  dimensions. source"},{"id":511,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.prismdim_theiler","ref":"/fractaldimensions/stable/#FractalDimensions.prismdim_theiler","content":" FractalDimensions.prismdim_theiler  —  Function prismdim_theiler(X) An algorithm to find the ideal choice of a prism dimension for  boxed_correlationsum  using Theiler's original suggestion. source"},{"id":512,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_r0_buenoorovio","ref":"/fractaldimensions/stable/#FractalDimensions.estimate_r0_buenoorovio","content":" FractalDimensions.estimate_r0_buenoorovio  —  Function estimate_r0_buenoorovio(X::AbstractStateSpaceSet, P = 2) → r0, ε0 Estimate a reasonable size for boxing  X , proposed by Bueno-Orovio and Pérez-García [Bueno2007] , before calculating the correlation dimension as presented by Theiler [Theiler1983] . Return the size  r0  and the minimum interpoint distance  ε0  in the data. If instead of boxes, prisms are chosen everything stays the same but  P  is the dimension of the prism. To do so the dimension  ν  is estimated by running the algorithm by Grassberger and Procaccia [Grassberger1983]  with  √N  points where  N  is the number of total data points. An effective size  ℓ  of the attractor is calculated by boxing a small subset of size  N/10  into boxes of sidelength  r_ℓ  and counting the number of filled boxes  η_ℓ . \\[\\ell = r_\\ell \\eta_\\ell ^{1/\\nu}\\] The optimal number of filled boxes  η_opt  is calculated by minimising the number of calculations. \\[\\eta_\\textrm{opt} = N^{2/3}\\cdot \\frac{3^\\nu - 1}{3^P - 1}^{1/2}.\\] P  is the dimension of the data or the number of edges on the prism that don't span the whole dataset. Then the optimal boxsize  $r_0$  computes as \\[r_0 = \\ell / \\eta_\\textrm{opt}^{1/\\nu}.\\] source"},{"id":513,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_r0_theiler","ref":"/fractaldimensions/stable/#FractalDimensions.estimate_r0_theiler","content":" FractalDimensions.estimate_r0_theiler  —  Function estimate_r0_theiler(X::AbstractStateSpaceSet) → r0, ε0 Estimate a reasonable size for boxing the data  X  before calculating the  boxed_correlationsum  proposed by Theiler [Theiler1987] . Return the boxing size  r0  and minimum inter-point distance in  X ,  ε0 . To do so the dimension is estimated by running the algorithm by Grassberger and Procaccia [Grassberger1983]  with  √N  points where  N  is the number of total data points. Then the optimal boxsize  $r_0$  computes as \\[r_0 = R (2/N)^{1/\\nu}\\] where  $R$  is the size of the chaotic attractor and  $\\nu$  is the estimated dimension. source"},{"id":514,"pagetitle":"FractalDimensions.jl","title":"Fixed mass correlation sum","ref":"/fractaldimensions/stable/#Fixed-mass-correlation-sum","content":" Fixed mass correlation sum"},{"id":515,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.fixedmass_correlation_dim","ref":"/fractaldimensions/stable/#FractalDimensions.fixedmass_correlation_dim","content":" FractalDimensions.fixedmass_correlation_dim  —  Function fixedmass_correlation_dim(X [, max_j]; kwargs...) Use the fixed mass algorithm for computing the correlation sum, and use the result to compute the correlation dimension  Δ_M  of  X . This function does something extremely simple: rs, ys = fixedmass_correlationsum(X, args...; kwargs...)\nslopefit(rs, ys)[1] source"},{"id":516,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.fixedmass_correlationsum","ref":"/fractaldimensions/stable/#FractalDimensions.fixedmass_correlationsum","content":" FractalDimensions.fixedmass_correlationsum  —  Function fixedmass_correlationsum(X [, max_j]; metric = Euclidean(), M = length(X)) → rs, ys A fixed mass algorithm for the calculation of the  correlationsum , and subsequently a fractal dimension  $\\Delta$ , with  max_j  the maximum number of neighbours that should be considered for the calculation. By default  max_j = clamp(N*(N-1)/2, 5, 32)  with  N  the data length. Keyword arguments M  defines the number of points considered for the averaging of distances, randomly subsampling them from  X . metric = Euclidean()  is the distance metric. start_j = 4  computes the equation below starting from  j = start_j . Typically the first  j  values have not converged to the correct scaling of the fractal dimension. Description \"Fixed mass\" algorithms mean that instead of trying to find all neighboring points within a radius, one instead tries to find the max radius containing  j  points. A correlation sum is obtained with this constrain, and equivalently the mean radius containing  k  points. Based on this, one can calculate  $\\Delta$  approximating the information dimension. The implementation here is due to to  [Grassberger1988] , which defines \\[Ψ(j) - \\log N \\sim \\Delta \\times \\overline{\\log \\left( r_{(j)}\\right)}\\] where  $\\Psi(j) = \\frac{\\text{d} \\log Γ(j)}{\\text{d} j}$  is the digamma function,  rs  =  $\\overline{\\log \\left( r_{(j)}\\right)}$  is the mean logarithm of a radius containing  j  neighboring points, and  ys  =  $\\Psi(j) - \\log N$  ( $N$  is the length of the data). The amount of neighbors found  $j$  range from 2 to  max_j . The numbers are also converted to base  $2$  from base  $e$ . $\\Delta$  can be computed by using  linear_region(rs, ys) . source"},{"id":517,"pagetitle":"FractalDimensions.jl","title":"Takens best estimate","ref":"/fractaldimensions/stable/#Takens-best-estimate","content":" Takens best estimate"},{"id":518,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.takens_best_estimate_dim","ref":"/fractaldimensions/stable/#FractalDimensions.takens_best_estimate_dim","content":" FractalDimensions.takens_best_estimate_dim  —  Function takens_best_estimate_dim(X, εmax, metric = Chebyshev(), εmin = 0) Use the \"Takens' best estimate\"  [Takens1985] [Theiler1988]  method for estimating the correlation dimension. The original formula is \\[\\Delta_C \\approx \\frac{C(\\epsilon_\\text{max})}{\\int_0^{\\epsilon_\\text{max}}(C(\\epsilon) / \\epsilon) \\, d\\epsilon}\\] where  $C$  is the  correlationsum  and  $\\epsilon_\\text{max}$  is an upper cutoff. Here we use the later expression \\[\\Delta_C \\approx - \\frac{1}{\\eta},\\quad \\eta = \\frac{1}{(N-1)^*}\\sum_{[i, j]^*}\\log(||X_i - X_j|| / \\epsilon_\\text{max})\\] where the sum happens for all  $i, j$  so that  $i < j$  and  $||X_i - X_j|| < \\epsilon_\\text{max}$ . In the above expression, the bias in the original paper has already been corrected, as suggested in  [Borovkova1999] . According to  [Borovkova1999] , introducing a lower cutoff  εmin  can make the algorithm more stable (no divergence), this option is given but defaults to zero. If  X  comes from a delay coordinates embedding of a timseries  x , a recommended value for  $\\epsilon_\\text{max}$  is  std(x)/4 . You may also use Δ_C, Δu_C, Δl_C = FractalDimensions.takens_best_estimate(args...) to obtain the upper and lower 95% confidence intervals. The intervals are estimated from the log-likelihood function by finding the values of  Δ_C  where the function has fallen by 2 from its maximum, see e.g.  [Barlow]  chapter 5.3. source"},{"id":519,"pagetitle":"FractalDimensions.jl","title":"Kaplan-Yorke dimension","ref":"/fractaldimensions/stable/#Kaplan-Yorke-dimension","content":" Kaplan-Yorke dimension"},{"id":520,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.kaplanyorke_dim","ref":"/fractaldimensions/stable/#FractalDimensions.kaplanyorke_dim","content":" FractalDimensions.kaplanyorke_dim  —  Function kaplanyorke_dim(λs::AbstractVector) Calculate the Kaplan-Yorke dimension, a.k.a. Lyapunov dimension [Kaplan1970]  from the given Lyapunov exponents  λs . Description The Kaplan-Yorke dimension is simply the point where  cumsum(λs)  becomes zero (interpolated): \\[ D_{KY} = k + \\frac{\\sum_{i=1}^k \\lambda_i}{|\\lambda_{k+1}|},\\quad k = \\max_j \\left[ \\sum_{i=1}^j \\lambda_i > 0 \\right].\\] If the sum of the exponents never becomes negative the function will return the length of the input vector. Useful in combination with  lyapunovspectrum  from ChaosTools.jl. source"},{"id":521,"pagetitle":"FractalDimensions.jl","title":"Higuchi dimension","ref":"/fractaldimensions/stable/#Higuchi-dimension","content":" Higuchi dimension"},{"id":522,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.higuchi_dim","ref":"/fractaldimensions/stable/#FractalDimensions.higuchi_dim","content":" FractalDimensions.higuchi_dim  —  Function higuchi_dim(x::AbstractVector [, ks]) Estimate the Higuchi dimension [Higuchi1988]  of the graph of  x . Description The Higuchi dimension is a number  Δ ∈ [1, 2]  that quantifies the roughness of the graph of the function  x(t) , assuming here that  x  is equi-sampled, like in the original paper. The method estimates how the length of the graph increases as a function of the indices difference (which, in this context, is equivalent with differences in  t ). Specifically, we calculate the average length versus  k  as \\[L_m(k) = \\frac{N-1}{\\lfloor \\frac{N-m}{k} \nfloor k^2}\n\\sum_{i=1}^{\\lfloor \\frac{N-m}{k} \\rfloor} |X_N(m+ik)-X_N(m+(i-1)k)| \\\\\n\nL(k) = \\frac{1}{k} \\sum_{m=1}^k L_m(k)\\] and then use  linear_region  in  -log2.(k)  vs  log2.(L)  as per usual when computing a fractal dimension. The algorithm chooses default  ks  to be exponentially spaced in base-2, up to at most  2^8 . A user can provide their own  ks  as a second argument otherwise. Use  FractalDimensions.higuchi_length(x, ks)  to obtain  $L(k)$  directly. source"},{"id":523,"pagetitle":"FractalDimensions.jl","title":"Extreme value value theory dimension","ref":"/fractaldimensions/stable/#Extreme-value-value-theory-dimension","content":" Extreme value value theory dimension"},{"id":524,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_dim","ref":"/fractaldimensions/stable/#FractalDimensions.extremevaltheory_dim","content":" FractalDimensions.extremevaltheory_dim  —  Function extremevaltheory_dim(X::StateSpaceSet, p::Real; kwargs...) → Δ Convenience syntax that returns the mean of the local dimensions of  extremevaltheory_dims_persistences , which approximates a fractal dimension of  X  using extreme value theory and quantile probability  p . See also  extremevaltheory_gpdfit_pvalues  for obtaining confidence on the results. source"},{"id":525,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_dims_persistences","ref":"/fractaldimensions/stable/#FractalDimensions.extremevaltheory_dims_persistences","content":" FractalDimensions.extremevaltheory_dims_persistences  —  Function extremevaltheory_dims_persistences(x::AbstractStateSpaceSet, p::Real; kwargs...) Return the local dimensions  Δloc  and the persistences  θloc  for each point in the given set for quantile probability  p , according to the estimation done via extreme value theory  [Lucarini2016] . The computation is parallelized to available threads ( Threads.nthreads() ). See also  extremevaltheory_gpdfit_pvalues  for obtaining confidence on the results. Keyword arguments show_progress = true : displays a progress bar. estimator = :mm : how to estimate the  σ  parameter of the Generalized Pareto Distribution. The local fractal dimension is  1/σ . The possible values are:  :exp, :mm , as in  estimate_gpd_parameters . compute_persistence = true:  whether to aso compute local persistences  θloc  (also called extremal index). If  false ,  θloc  are  NaN s. allocate_matrix = false : If  true , the code calls a method that attempts to allocate an  N×N  matrix ( N = length(X) ) that stores the pairwise Euclidean distances. This method is faster due to optimizations of  Distances.pairwise  but will error if the computer does not have enough available memory for the matrix allocation. Description For each state space point  $\\mathbf{x}_i$  in  X  we compute  $g_i = -\\log(||\\mathbf{x}_i - \\mathbf{x}_j|| ) \\; \\forall j = 1, \\ldots, N$  with  $||\\cdot||$  the Euclidean distance. Next, we choose an extreme quantile probability  $p$  (e.g., 0.99) for the distribution of  $g_i$ . We compute  $g_p$  as the  $p$ -th quantile of  $g_i$ . Then, we collect the exceedances of  $g_i$ , defined as  $E_i = \\{ g_i - g_p: g_i \\ge g_p \\}$ , i.e., all values of  $g_i$  larger or equal to  $g_p$ , also shifted by  $g_p$ . There are in total  $n = N(1-q)$  values in  $E_i$ . According to extreme value theory, in the limit  $N \\to \\infty$  the values  $E_i$  follow a two-parameter Generalized Pareto Distribution (GPD) with parameters  $\\sigma,\\xi$  (the third parameter  $\\mu$  of the GPD is zero due to the positive-definite construction of  $E$ ). Within this extreme value theory approach, the local dimension  $\\Delta^{(E)}_i$  assigned to state space point  $\\textbf{x}_i$  is given by the inverse of the  $\\sigma$  parameter of the GPD fit to the data [Lucarini2012] ,  $\\Delta^{(E)}_i = 1/\\sigma$ .  $\\sigma$  is estimated according to the  estimator  keyword. source"},{"id":526,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_local_dim_persistence","ref":"/fractaldimensions/stable/#FractalDimensions.extremevaltheory_local_dim_persistence","content":" FractalDimensions.extremevaltheory_local_dim_persistence  —  Function extremevaltheory_local_dim_persistence(X::StateSpaceSet, ζ, p::Real; kw...) Return the local values  Δ, θ  of the fractal dimension and persistence of  X  around a state space point  ζ .  p  and  kw  are as in  extremevaltheory_dims_persistences . source"},{"id":527,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremal_index_sueveges","ref":"/fractaldimensions/stable/#FractalDimensions.extremal_index_sueveges","content":" FractalDimensions.extremal_index_sueveges  —  Function extremal_index_sueveges(y::AbstractVector, p) Compute the extremal index θ of  y  through the Süveges formula for quantile probability  p . source"},{"id":528,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.estimate_gpd_parameters","ref":"/fractaldimensions/stable/#FractalDimensions.estimate_gpd_parameters","content":" FractalDimensions.estimate_gpd_parameters  —  Function estimate_gpd_parameters(X::AbstractVector{<:Real}, estimator::Symbol = :mm) Estimate and return the parameters  σ, ξ  of a Generalized Pareto Distribution fit to  X , assuming that  minimum(X) ≥ 0  and hence the parameter  μ  is 0 (if not, simply shift  X  by its minimum), according to the methods provided in  [Flavio2023] . Optionally choose the estimator, which can be: :exp : Assume the distribution is exponential instead of GP and get  σ  from mean of  X  and set  ξ = 0 . mm : Standing for \"method of moments\", estimants are given by \\[\\xi = (\\bar{x}^2/s^2 - 1)/2, \\quad \\sigma = \\bar{x}(\\bar{x}^2/s^2 + 1)/2\\] with  $\\bar{x}$  the sample mean and  $s^2$  the sample variance. This estimator only exists if the true distribution  ξ  value is < 0.5. source"},{"id":529,"pagetitle":"FractalDimensions.jl","title":"FractalDimensions.extremevaltheory_gpdfit_pvalues","ref":"/fractaldimensions/stable/#FractalDimensions.extremevaltheory_gpdfit_pvalues","content":" FractalDimensions.extremevaltheory_gpdfit_pvalues  —  Function extremevaltheory_gpdfit_pvalues(X, p; kw...) Return various computed quantities that may quantify the significance of the results of  extremevaltheory_dims_persistences (X, p; kw...) , terms of quantifying how well a Generalized Pareto Distribution (GPD) describes exceedences in the input data. Keyword arguments show_progress, estimator  as in  extremevaltheory_dims_persistences TestType = ApproximateOneSampleKSTest : the test type to use. It can be  ApproximateOneSampleKSTest, ExactOneSampleKSTest, CramerVonMises . We noticed that  OneSampleADTest  sometimes yielded nonsensical results: all p-values were equal and were very small ≈ 1e-6. nbins = round(Int, length(X)*(1-p)/20) : number of bins to use when computing the histogram of the exceedances for computing the NRMSE. The default value will use equally spaced bins that are equal to the length of the exceedances divided by 20. Description The function computes the exceedances  $E_i$  for each point  $x_i \\in X$  as in  extremevaltheory_dims_persistences . It returns 5 quantities, all being vectors of length  length(X) : Es , all exceedences, as a vector of vectors. sigmas, xis  the fitted σ, ξ to the GPD fits for each exceedance nrmses  the normalized root mean square distance of the fitted GPD to the histogram of the exceedances pvalues  the pvalues of a statistical test of the appropriateness of the GPD fit The output  nrmses  quantifies the distance between the fitted GPD and the empirical histogram of the exceedances. It is computed as \\[NRMSE = \\sqrt{\\frac{\\sum{(P_j - G_j)^2}{\\sum{(P_j - U)^2}}\\] where  $P_j$  the empirical (observed) probability at bin  $j$ ,  $G_j$  the fitted GPD probability at the midpoint of bin  $j$ , and  $U$  same as  $G_j$  but for the uniform distribution. The divisor of the equation normalizes the expression, so that the error of the empirical distribution is normalized to the error of the empirical distribution with fitting it with the uniform distribution. It is expected that NRMSE < 1. The smaller it is, the better the data are approximated by GPD versus uniform distribution. The output  pvalues  is a vector of p-values.  pvalues[i]  corresponds to the p-value of the hypothesis:  \"The exceedences around point  X[i]  are sampled from a GPD\"  versus the alternative hypothesis that they are not. To extract the p-values, we perform a one-sample hypothesis via HypothesisTests.jl to the fitted GPD. Very small p-values then indicate that the hypothesis should be rejected and the data are not well described by a GPD. This can be an indication that we do not have enough data, or that we choose too high of a quantile probability  p , or that the data are not suitable in general. This p-value based method for significance has been used in  [Faranda2017] , but it is unclear precisely how it was used. For more details on how these quantities may quantify significance, see our review paper. source"},{"id":530,"pagetitle":"FractalDimensions.jl","title":"Theiler window","ref":"/fractaldimensions/stable/#Theiler-window","content":" Theiler window The Theiler window is a concept that is useful when finding neighbors in a dataset that is coming from the sampling of a continuous dynamical system. Itt tries to eliminate spurious \"correlations\" (wrongly counted neighbors) due to a potentially dense sampling of the trajectory. Typically a good choice for  w  coincides with the choice an optimal delay time, see  DelayEmbeddings.estimate_delay , for any of the timeseries of the dataset. For more details, see Chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022."},{"id":531,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSet  reference","ref":"/fractaldimensions/stable/#StateSpaceSet-reference","content":" StateSpaceSet  reference"},{"id":532,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/fractaldimensions/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple."},{"id":533,"pagetitle":"FractalDimensions.jl","title":"StateSpaceSets.standardize","ref":"/fractaldimensions/stable/#StateSpaceSets.standardize","content":" StateSpaceSets.standardize  —  Function standardize(d::StateSpaceSet) → r Create a standardized version of the input set where each column is transformed to have mean 0 and standard deviation 1. standardize(x::AbstractVector{<:Real}) = (x - mean(x))/std(x) Deshmukh2021 Deshmukh et al., Toward automated extraction and characterization of scaling regions in dynamical systems,  Chaos 31, 123102 (2021) . Molteno1993 Molteno, T. C. A.,  Fast O(N) box-counting algorithm for estimating dimensions. Phys. Rev. E 48, R3263(R) (1993) Grassberger1983 Grassberger and Proccacia,  Characterization of strange attractors, PRL 50 (1983)  Theiler1986 Theiler,  Spurious dimension from correlation algorithms applied to limited time-series data. Physical Review A, 34 Grassberger2007 Peter Grassberger (2007)  Grassberger-Procaccia algorithm. Scholarpedia, 2(5):3043. Kantz2003 Kantz, H., & Schreiber, T. (2003).  Nonlinear Time Series Analysis, Cambridge University Press. Bueno2007 Bueno-Orovio and Pérez-García,  Enhanced box and prism assisted algorithms for computing the correlation dimension. Chaos Solitons & Fractrals, 34(5)  Theiler1987 Theiler,  Efficient algorithm for estimating the correlation dimension from a set of discrete points. Physical Review A, 36 Bueno2007 Bueno-Orovio and Pérez-García,  Enhanced box and prism assisted algorithms for computing the correlation dimension. Chaos Solitons & Fractrals, 34(5)  Theiler1987 Theiler,  Efficient algorithm for estimating the correlation dimension from a set of discrete points. Physical Review A, 36 Grassberger1983 Grassberger and Proccacia,  Characterization of strange attractors, PRL 50 (1983)  Theiler1987 Theiler,  Efficient algorithm for estimating the correlation dimension from a set of discrete points. Physical Review A, 36 Grassberger1983 Grassberger and Proccacia,  Characterization of strange attractors, PRL 50 (1983)  Grassberger1988 Peter Grassberger (1988)  Finite sample Corrections to Entropy and Dimension Estimates, Physics Letters A 128(6-7) Takens1985 Takens, On the numerical determination of the dimension of an attractor, in: B.H.W. Braaksma, B.L.J.F. Takens (Eds.), Dynamical Systems and Bifurcations, in: Lecture Notes in Mathematics, Springer, Berlin, 1985, pp. 99–106. Theiler1988 Theiler,  Lacunarity in a best estimator of fractal dimension. Physics Letters A, 133(4–5) Borovkova1999 Borovkova et al.,  Consistency of the Takens estimator for the correlation dimension. The Annals of Applied Probability, 9, 05 1999. Barlow Barlow, R., Statistics - A Guide to the Use of Statistical Methods in the Physical Sciences. Vol 29. John Wiley & Sons, 1993 Kaplan1970 J. Kaplan & J. Yorke,  Chaotic behavior of multidimensional difference equations , Lecture Notes in Mathematics vol.  730 , Springer (1979) Higuchi1988 Higuchi,  Approach to an irregular time series on the basis of the fractal theory ,  Physica D: Nonlinear Phenomena (1988) Lucarini2016 Lucarini et al.,  Extremes and Recurrence in Dynamical Systems  Lucarini2012 Lucarini et al., Universal Behaviour of Extreme Value Statistics for Selected Observables of Dynamical Systems,  Journal of Statistical Physics, 147(1), 63–73.  et al., [Physica D 400 132143 Süveges2007 Süveges. 2007. Likelihood estimation of the extremal index. Extremes, 10.1-2, 41-55, doi: 10.1007/s10687-007-0034-2 Flavio2023 Flavio et al., Stability of attractor local dimension estimates in non-Axiom A dynamical systems,  preprint Faranda2017 Faranda et al. (2017), Dynamical proxies of North Atlantic predictability and extremes,  Scientific Reports, 7"},{"id":538,"pagetitle":"ComplexityMeasures.jl","title":"ComplexityMeasures.jl","ref":"/complexitymeasures/stable/#ComplexityMeasures.jl","content":" ComplexityMeasures.jl"},{"id":539,"pagetitle":"ComplexityMeasures.jl","title":"ComplexityMeasures","ref":"/complexitymeasures/stable/#ComplexityMeasures","content":" ComplexityMeasures  —  Module ComplexityMeasures.jl A Julia package that provides estimators for probabilities, entropies, and other complexity measures, in the context of nonlinear dynamics, nonlinear timeseries analysis, and complex systems. It can be used as a standalone package, or as part of other projects in the JuliaDynamics organization, such as  DynamicalSystems.jl  or  CausalityTools.jl . To install it, run  import Pkg; Pkg.add(\"ComplexityMeasures\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. Previously, this package was called Entropies.jl. source"},{"id":540,"pagetitle":"ComplexityMeasures.jl","title":"Content and terminology","ref":"/complexitymeasures/stable/#terminology","content":" Content and terminology Note The documentation here follows (loosely) chapter 5 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. Before exploring the features of ComplexityMeasures.jl, it is useful to read through this terminology section. Here, we briefly review important complexity-related concepts and names from the scientific literature, and outline how we've structured ComplexityMeasures.jl around these concepts. In these scientific literature, words like  probabilities ,  entropies , and other  complexity measures  are used (and abused) in multiple contexts, and are often used interchangeably to describe similar concepts. The API and documentation of ComplexityMeasures.jl aim to clarify the meaning and usage of these words, and to provide simple ways to obtain probabilities, entropies, or other complexity measures from input data. For ComplexityMeasures.jl  entropies are also complexity measures , while sometimes a distinction is made so that \"complexity measures\" means anything beyond entropy. However we believe the general nonlinear dynamics community agrees with our take, as most papers that introduce different entropy flavors, call them complexity measures. Example:  \"Permutation Entropy: A Natural Complexity Measure for Time Series\"  from Brandt and Pompe 2002."},{"id":541,"pagetitle":"ComplexityMeasures.jl","title":"Probabilities","ref":"/complexitymeasures/stable/#Probabilities","content":" Probabilities Entropies and other complexity measures are typically computed based on  probability distributions , which we simply refer to as \"probabilities\". Probabilities can be obtained from input data in a plethora of different ways. The central API function that returns a probability distribution is  probabilities , which takes in a subtype of  ProbabilitiesEstimator  to specify how the probabilities are computed. All available estimators can be found in the  estimators page ."},{"id":542,"pagetitle":"ComplexityMeasures.jl","title":"Entropies","ref":"/complexitymeasures/stable/#Entropies","content":" Entropies Entropy is an established concept in statistics, information theory, and nonlinear dynamics. However, it is also an umbrella term that may mean several computationally, and sometimes even fundamentally, different quantities. In ComplexityMeasures.jl, we provide the generic function  entropy  that tries to both clarify disparate entropy concepts, while unifying them under a common interface that highlights the modular nature of the word \"entropy\". On the highest level, there are two main types of entropy. Discrete  entropies are functions of  probability mass functions . Computing a discrete entropy boils   down to two simple steps: first estimating a probability distribution, then plugging   the estimated probabilities into an estimator of a so-called \"generalized entropy\" definition.   Internally, this is literally just a few lines of code where we first apply some    ProbabilitiesEstimator  to the input data, and feed the resulting    probabilities  to  entropy  with some  DiscreteEntropyEstimator . Differential/continuous  entropies are functions of    probability density functions ,   which are  integrals . Computing differential entropies therefore rely on estimating   some density functional. For this task, we provide  DifferentialEntropyEstimator s,   which compute entropies via alternate means, without explicitly computing some   probability distribution. For example, the  Correa  estimator computes the   Shannon differential entropy using order statistics. Crucially, many quantities in the nonlinear dynamics literature that are named as entropies, such as \"permutation entropy\" ( entropy_permutation ) and \"wavelet entropy\" ( entropy_wavelet ), are  not really new entropies . They are the good old discrete Shannon entropy ( Shannon ), but calculated with  new probabilities estimators . Even though the names of these methods (e.g. \"wavelet entropy\") sound like names for new entropies, what they actually do is to devise novel ways of calculating probabilities from data, and then plug those probabilities into formal discrete entropy formulas such as the Shannon entropy. These probabilities estimators are of course smartly created so that they elegantly highlight important complexity-related aspects of the data. Names for methods such as \"permutation entropy\" are commonplace, so in ComplexityMeasures.jl we provide convenience functions like  entropy_permutation . However, we emphasize that these functions really aren't anything more than 2-lines-of-code wrappers that call  entropy  with the appropriate  ProbabilitiesEstimator . What are  genuinely different entropies  are different definitions of entropy. And there are a lot of them! Examples are  Shannon  (the classic),  Renyi  or  Tsallis  entropy. These different definitions can be found in  EntropyDefinition ."},{"id":543,"pagetitle":"ComplexityMeasures.jl","title":"Other complexity measures","ref":"/complexitymeasures/stable/#Other-complexity-measures","content":" Other complexity measures Other complexity measures, which strictly speaking don't compute entropies, and may or may not explicitly compute probability distributions, are found in  Complexity measures  page. This includes measures like sample entropy and approximate entropy."},{"id":544,"pagetitle":"ComplexityMeasures.jl","title":"Input data for ComplexityMeasures.jl","ref":"/complexitymeasures/stable/#input_data","content":" Input data for ComplexityMeasures.jl The input data type typically depend on the probability estimator chosen. In general though, the standard DynamicalSystems.jl approach is taken and as such we have three types of input data: Timeseries , which are  AbstractVector{<:Real} , used in e.g. with  WaveletOverlap . Multi-variate timeseries, or datasets, or state space sets , which are  StateSpaceSet s, used e.g. with  NaiveKernel . Spatial data , which are higher dimensional standard  Array s, used e.g. with   SpatialSymbolicPermutation ."},{"id":545,"pagetitle":"ComplexityMeasures.jl","title":"StateSpaceSets.StateSpaceSet","ref":"/complexitymeasures/stable/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple."},{"id":548,"pagetitle":"Complexity measures","title":"Complexity measures","ref":"/complexitymeasures/stable/complexity/#Complexity-measures","content":" Complexity measures In this page we document estimators for complexity measures that are not entropies in the strict mathematical sense. The API is almost identical to  entropy  and is defined by: complexity complexity_normalized ComplexityEstimator"},{"id":549,"pagetitle":"Complexity measures","title":"Complexity measures API","ref":"/complexitymeasures/stable/complexity/#Complexity-measures-API","content":" Complexity measures API"},{"id":550,"pagetitle":"Complexity measures","title":"ComplexityMeasures.complexity","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.complexity","content":" ComplexityMeasures.complexity  —  Function complexity(c::ComplexityEstimator, x) Estimate a complexity measure according to  c  for  input data x , where  c  can be any of the following estimators: ReverseDispersion . ApproximateEntropy . SampleEntropy . MissingDispersionPatterns . source"},{"id":551,"pagetitle":"Complexity measures","title":"ComplexityMeasures.complexity_normalized","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.complexity_normalized","content":" ComplexityMeasures.complexity_normalized  —  Function complexity_normalized(c::ComplexityEstimator, x) → m ∈ [a, b] The same as  complexity , but the result is normalized to the interval  [a, b] , where  [a, b]  depends on  c . source"},{"id":552,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ComplexityEstimator","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.ComplexityEstimator","content":" ComplexityMeasures.ComplexityEstimator  —  Type ComplexityEstimator Supertype for estimators for various complexity measures that are not entropies in the strict mathematical sense. See  complexity  for all available estimators. source"},{"id":553,"pagetitle":"Complexity measures","title":"Approximate entropy","ref":"/complexitymeasures/stable/complexity/#Approximate-entropy","content":" Approximate entropy"},{"id":554,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ApproximateEntropy","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.ApproximateEntropy","content":" ComplexityMeasures.ApproximateEntropy  —  Type ApproximateEntropy <: ComplexityEstimator\nApproximateEntropy([x]; r = 0.2std(x), kwargs...) An estimator for the approximate entropy (ApEn; Pincus, 1991) [Pincus1991]  complexity measure, used with  complexity . The keyword argument  r  is mandatory if an input timeseries  x  is not provided. Keyword arguments r::Real : The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data. m::Int = 2 : The embedding dimension. τ::Int = 1 : The embedding lag. base::Real = MathConstants.e : The base to use for the logarithm. Pincus (1991) uses the   natural logarithm. Description Approximate entropy is defined as \\[ApEn(m ,r) = \\lim_{N \\to \\infty} \\left[ \\phi(x, m, r) - \\phi(x, m + 1, r) \\right].\\] Approximate entropy is estimated for a timeseries  x , by first embedding  x  using embedding dimension  m  and embedding lag  τ , then searching for similar vectors within tolerance radius  r , using the estimator described below, with logarithms to the given  base  (natural logarithm is used in Pincus, 1991). Specifically, for a finite-length timeseries  x , an estimator for  $ApEn(m ,r)$  is \\[ApEn(m, r, N) = \\phi(x, m, r, N) -  \\phi(x, m + 1, r, N),\\] where  N = length(x)  and \\[\\phi(x, k, r, N) =\n\\dfrac{1}{N-(k-1)\\tau} \\sum_{i=1}^{N - (k-1)\\tau}\n\\log{\\left(\n    \\sum_{j = 1}^{N-(k-1)\\tau} \\dfrac{\\theta(d({\\bf x}_i^m, {\\bf x}_j^m) \\leq r)}{N-(k-1)\\tau}\n    \\right)}.\\] Here,  $\\theta(\\cdot)$  returns 1 if the argument is true and 0 otherwise,   $d({\\bf x}_i, {\\bf x}_j)$  returns the Chebyshev distance between vectors   ${\\bf x}_i$  and  ${\\bf x}_j$ , and the  k -dimensional embedding vectors are constructed from the input timeseries  $x(t)$  as \\[{\\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \\ldots, x(i+(k-1)\\tau)).\\] Flexible embedding lag In the original paper, they fix  τ = 1 . In our implementation, the normalization constant is modified to account for embeddings with  τ != 1 . source"},{"id":555,"pagetitle":"Complexity measures","title":"Sample entropy","ref":"/complexitymeasures/stable/complexity/#Sample-entropy","content":" Sample entropy"},{"id":556,"pagetitle":"Complexity measures","title":"ComplexityMeasures.SampleEntropy","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.SampleEntropy","content":" ComplexityMeasures.SampleEntropy  —  Type SampleEntropy([x]; r = 0.2std(x), kwargs...) <: ComplexityEstimator An estimator for the sample entropy complexity measure (Richman & Moorman, 2000) [Richman2000] , used with  complexity  and  complexity_normalized . The keyword argument  r  is mandatory if an input timeseries  x  is not provided. Keyword arguments r::Real : The radius used when querying for nearest neighbors around points. Its value   should be determined from the input data, for example as some proportion of the   standard deviation of the data. m::Int = 1 : The embedding dimension. τ::Int = 1 : The embedding lag. Description An  estimator  for sample entropy using radius  r , embedding dimension  m , and embedding lag  τ  is \\[SampEn(m,r, N) = -\\ln{\\dfrac{A(r, N)}{B(r, N)}}.\\] Here, \\[\\begin{aligned}\nB(r, m, N) = \\sum_{i = 1}^{N-m\\tau} \\sum_{j = 1, j \\neq i}^{N-m\\tau} \\theta(d({\\bf x}_i^m, {\\bf x}_j^m) \\leq r) \\\\\nA(r, m, N) = \\sum_{i = 1}^{N-m\\tau} \\sum_{j = 1, j \\neq i}^{N-m\\tau} \\theta(d({\\bf x}_i^{m+1}, {\\bf x}_j^{m+1}) \\leq r) \\\\\n\\end{aligned},\\] where  $\\theta(\\cdot)$  returns 1 if the argument is true and 0 otherwise, and  $d(x, y)$  computes the Chebyshev distance between  $x$  and  $y$ , and   ${\\bf x}_i^{m}$  and  ${\\bf x}_i^{m+1}$  are  m -dimensional and  m+1 -dimensional embedding vectors, where  k -dimensional embedding vectors are constructed from the input timeseries  $x(t)$  as \\[{\\bf x}_i^k = (x(i), x(i+τ), x(i+2τ), \\ldots, x(i+(k-1)\\tau)).\\] Quoting Richman & Moorman (2002): \"SampEn(m,r,N) will be defined except when B = 0, in which case no regularity has been detected, or when A = 0, which corresponds to a conditional probability of 0 and an infinite value of SampEn(m,r,N)\". In these cases,  NaN  is returned. If computing the normalized measure, then the resulting sample entropy is on  [0, 1] . Flexible embedding lag The original algorithm fixes  τ = 1 . All formulas here are modified to account for any  τ . See also:  entropy_sample . source"},{"id":557,"pagetitle":"Complexity measures","title":"Missing dispersion patterns","ref":"/complexitymeasures/stable/complexity/#Missing-dispersion-patterns","content":" Missing dispersion patterns"},{"id":558,"pagetitle":"Complexity measures","title":"ComplexityMeasures.MissingDispersionPatterns","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.MissingDispersionPatterns","content":" ComplexityMeasures.MissingDispersionPatterns  —  Type MissingDispersionPatterns <: ComplexityEstimator\nMissingDispersionPatterns(est = Dispersion()) An estimator for the number of missing dispersion patterns ( $N_{MDP}$ ), a complexity measure which can be used to detect nonlinearity in time series (Zhou et al., 2022) [Zhou2022] . Used with  complexity  or  complexity_normalized , whose implementation uses  missing_outcomes . Description If used with  complexity ,  $N_{MDP}$  is computed by first symbolising each  xᵢ ∈ x , then embedding the resulting symbol sequence using the dispersion pattern estimator  est , and computing the quantity \\[N_{MDP} = L - N_{ODP},\\] where  L = total_outcomes(est)  (i.e. the total number of possible dispersion patterns), and  $N_{ODP}$  is defined as the number of  occurring  dispersion patterns. If used with  complexity_normalized , then  $N_{MDP}^N = (L - N_{ODP})/L$  is computed. The authors recommend that  total_outcomes(est.symbolization)^est.m << length(x) - est.m*est.τ + 1  to avoid undersampling. Encoding Dispersion 's linear mapping from CDFs to integers is based on equidistant partitioning of the interval  [0, 1] . This is slightly different from Zhou et al. (2022), which uses the linear mapping  $s_i := \\text{round}(y + 0.5)$ . Usage In Zhou et al. (2022),  MissingDispersionPatterns  is used to detect nonlinearity in time series by comparing the  $N_{MDP}$  for a time series  x  to  $N_{MDP}$  values for an ensemble of surrogates of  x . If  $N_{MDP} > q_{MDP}^{WIAAFT}$ , where  $q_{MDP}^{WIAAFT}$  is some  q -th quantile of the surrogate ensemble, then it is taken as evidence for nonlinearity. See also:  Dispersion ,  ReverseDispersion ,  total_outcomes . source"},{"id":559,"pagetitle":"Complexity measures","title":"Reverse dispersion entropy","ref":"/complexitymeasures/stable/complexity/#Reverse-dispersion-entropy","content":" Reverse dispersion entropy"},{"id":560,"pagetitle":"Complexity measures","title":"ComplexityMeasures.ReverseDispersion","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.ReverseDispersion","content":" ComplexityMeasures.ReverseDispersion  —  Type ReverseDispersion <: ComplexityEstimator\nReverseDispersion(; c = 3, m = 2, τ = 1, check_unique = true) Estimator for the reverse dispersion entropy complexity measure (Li et al., 2019) [Li2019] . Description Li et al. (2021) [Li2019]  defines the reverse dispersion entropy as \\[H_{rde} = \\sum_{i = 1}^{c^m} \\left(p_i - \\dfrac{1}{{c^m}} \\right)^2 =\n\\left( \\sum_{i=1}^{c^m} p_i^2 \\right) - \\dfrac{1}{c^{m}}\\] where the probabilities  $p_i$  are obtained precisely as for the  Dispersion  probability estimator. Relative frequencies of dispersion patterns are computed using the given  encoding  scheme , which defaults to encoding using the normal cumulative distribution function (NCDF), as implemented by  GaussianCDFEncoding , using embedding dimension  m  and embedding delay  τ . Recommended parameter values [Li2018]  are  m ∈ [2, 3] ,  τ = 1  for the embedding, and  c ∈ [3, 4, …, 8]  categories for the Gaussian mapping. If normalizing, then the reverse dispersion entropy is normalized to  [0, 1] . The minimum value of  $H_{rde}$  is zero and occurs precisely when the dispersion pattern distribution is flat, which occurs when all  $p_i$ s are equal to  $1/c^m$ . Because  $H_{rde} \\geq 0$ ,  $H_{rde}$  can therefore be said to be a measure of how far the dispersion pattern probability distribution is from white noise. Data requirements The input must have more than one unique element for the default  GaussianEncoding  to be well-defined. Li et al. (2018) recommends that  x  has at least 1000 data points. If  check_unique == true  (default), then it is checked that the input has more than one unique value. If  check_unique == false  and the input only has one unique element, then a  InexactError  is thrown when trying to compute probabilities. source"},{"id":561,"pagetitle":"Complexity measures","title":"Statistical complexity","ref":"/complexitymeasures/stable/complexity/#Statistical-complexity","content":" Statistical complexity"},{"id":562,"pagetitle":"Complexity measures","title":"ComplexityMeasures.StatisticalComplexity","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.StatisticalComplexity","content":" ComplexityMeasures.StatisticalComplexity  —  Type StatisticalComplexity <: ComplexityEstimator\nStatisticalComplexity([x]; kwargs...) An estimator for the statistical complexity and entropy according to Rosso et al. (2007) [Rosso2007] (@ref), used with  complexity . Keyword arguments est::ProbabilitiesEstimator = SymbolicPermutation() : which estimator to use to get the probabilities dist<:SemiMetric = JSDivergence() : the distance measure between the estimated probability   distribution and a uniform distribution with the same maximal number of bins Description Statistical complexity is defined as \\[C_q[P] = \\mathcal{H}_q\\cdot \\mathcal{Q}_q[P],\\] where  $Q_q$  is a \"disequilibrium\" obtained from a distance-measure and  H_q  a disorder measure. In the original paper [Rosso2007] , this complexity measure was defined via an ordinal pattern-based probability distribution, the Shannon entropy and the Jensen-Shannon divergence as a distance measure. This implementation allows for a generalization of the complexity measure as developed in  [Rosso2013] . Here,  $H_q$ can be the (q-order) Shannon-, Renyi or Tsallis entropy and Q_q ` based either on the Euclidean, Wooters, Kullback, q-Kullback, Jensen or q-Jensen distance as \\[Q_q[P] = Q_q^0\\cdot D[P, P_e],\\] where  $D[P, P_e]$  is the distance between the obtained distribution  $P$  and a uniform distribution with the same maximum number of bins, measured by the distance measure  dist . Usage The statistical complexity is exclusively used in combination with the related information measure (entropy).  complexity(c::StatisticalComplexity, x)  returns only the statistical complexity. The entropy can be accessed as a  Ref  value of the struct as x = randn(100)\nc = StatisticalComplexity\ncompl = complexity(c, x)\nentr = c.entr_val[] To obtain both the entropy and the statistical complexity together as a  Tuple , use the wrapper  entropy_complexity . source"},{"id":563,"pagetitle":"Complexity measures","title":"ComplexityMeasures.entropy_complexity","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.entropy_complexity","content":" ComplexityMeasures.entropy_complexity  —  Function entropy_complexity(c::StatisticalComplexity, x) Return both the entropy and the corresponding  StatisticalComplexity . Useful when wanting to plot data on the \"entropy-complexity plane\". See also  entropy_complexity_curves . source"},{"id":564,"pagetitle":"Complexity measures","title":"ComplexityMeasures.entropy_complexity_curves","ref":"/complexitymeasures/stable/complexity/#ComplexityMeasures.entropy_complexity_curves","content":" ComplexityMeasures.entropy_complexity_curves  —  Function entropy_complexity_curves(c::StatisticalComplexity; num_max=1, num_min=1000) -> (min_entropy_complexity, max_entropy_complexity) Calculate the maximum complexity-entropy curve for the statistical complexity according to  [Rosso2007]  for  num_max * total_outcomes(c.est)  different values of the normalized information measure of choice (in case of the maximum complexity curves) and  num_min  different values of the normalized information measure of choice (in case of the minimum complexity curve). Description The way the statistical complexity is designed, there is a minimum and maximum possible complexity for data with a given permutation entropy. The calculation time of the maximum complexity curve grows as  O(total_outcomes(c.est)^2) , and thus takes very long for high numbers of outcomes. This function is inspired by S. Sippels implementation in statcomp  [statcomp] . This function will work with any  ProbabilitiesEstimator  where  total_outcomes (@ref) is known a priori. source Pincus1991 Pincus, S. M. (1991). Approximate entropy as a measure of system complexity. Proceedings of the National Academy of Sciences, 88(6), 2297-2301. Richman2000 Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049. Zhou2022 Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20. Li2019 Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203. Rosso2007 Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., & Fuentes, M. A. (2007).          Distinguishing noise from chaos .         Physical review letters, 99(15), 154102. Rosso2013 Rosso, O. A. (2013) Generalized Statistical Complexity: A New Tool for Dynamical Systems. Rosso2007 Rosso, O. A., Larrondo, H. A., Martin, M. T., Plastino, A., & Fuentes, M. A. (2007).          Distinguishing noise from chaos .         Physical review letters, 99(15), 154102. statcomp Sippel, S., Lange, H., Gans, F. (2019).          statcomp: Statistical Complexity and Information Measures for Time Series Analysis"},{"id":567,"pagetitle":"Convenience functions","title":"Convenience functions","ref":"/complexitymeasures/stable/convenience/#Convenience-functions","content":" Convenience functions We provide a few convenience functions for widely used names for entropy or \"entropy-like\" quantities. Other arbitrary specialized convenience functions can easily be defined in a couple lines of code."},{"id":568,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_permutation","ref":"/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_permutation","content":" ComplexityMeasures.entropy_permutation  —  Function entropy_permutation(x; τ = 1, m = 3, base = 2) Compute the permutation entropy of  x  of order  m  with delay/lag  τ . This function is just a convenience call to: est = SymbolicPermutation(; m, τ)\nentropy(Shannon(base), est, x) See  SymbolicPermutation  for more info. Similarly, one can use  SymbolicWeightedPermutation  or  SymbolicAmplitudeAwarePermutation  for the weighted/amplitude-aware versions. source"},{"id":569,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_wavelet","ref":"/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_wavelet","content":" ComplexityMeasures.entropy_wavelet  —  Function entropy_wavelet(x; wavelet = Wavelets.WT.Daubechies{12}(), base = 2) Compute the wavelet entropy. This function is just a convenience call to: est = WaveletOverlap(wavelet)\nentropy(Shannon(base), est, x) See  WaveletOverlap  for more info. source"},{"id":570,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_dispersion","ref":"/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_dispersion","content":" ComplexityMeasures.entropy_dispersion  —  Function entropy_dispersion(x; base = 2, kwargs...) Compute the dispersion entropy. This function is just a convenience call to: est = Dispersion(kwargs...)\nentropy(Shannon(base), est, x) See  Dispersion  for more info. source"},{"id":571,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_approx","ref":"/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_approx","content":" ComplexityMeasures.entropy_approx  —  Function entropy_approx(x; m = 2, τ = 1, r = 0.2 * Statistics.std(x), base = MathConstants.e) Convenience syntax for computing the approximate entropy (Pincus, 1991) for timeseries  x . This is just a wrapper for  complexity(ApproximateEntropy(; m, τ, r, base), x)  (see also  ApproximateEntropy ). source"},{"id":572,"pagetitle":"Convenience functions","title":"ComplexityMeasures.entropy_sample","ref":"/complexitymeasures/stable/convenience/#ComplexityMeasures.entropy_sample","content":" ComplexityMeasures.entropy_sample  —  Function entropy_sample(x; r = 0.2std(x), m = 2, τ = 1, normalize = true) Convenience syntax for estimating the (normalized) sample entropy (Richman & Moorman, 2000) of timeseries  x . This is just a wrapper for  complexity(SampleEntropy(; r, m, τ, base), x) . See also:  SampleEntropy ,  complexity ,  complexity_normalized ). source"},{"id":575,"pagetitle":"-","title":"Complexity: multiscale","ref":"/complexitymeasures/stable/dev/multiscale/#Complexity:-multiscale","content":" Complexity: multiscale using ComplexityMeasures\nusing CairoMakie\n\nN, a = 2000, 20\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5)) .+ 0.2 .* x\nmaxscale = 10\nhs = ComplexityMeasures.multiscale_normalized(Regular(), SampleEntropy(y), y; maxscale)\n\nfig = Figure()\nax1 = Axis(fig[1,1]; ylabel = \"y\")\nlines!(ax1, t, y; color = Cycled(1));\nax2 = Axis(fig[2, 1]; ylabel = \"Sample entropy (h)\", xlabel = \"Scale\")\nscatterlines!(ax2, 1:maxscale |> collect, hs; color = Cycled(1));\nfig"},{"id":578,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"ComplexityMeasures.jl Dev Docs","ref":"/complexitymeasures/stable/devdocs/#ComplexityMeasures.jl-Dev-Docs","content":" ComplexityMeasures.jl Dev Docs Good practices in developing a code base apply in every Pull Request. The  Good Scientific Code Workshop  is worth checking out for this. All PRs contributing new functionality must be well tested and well documented. You only need to add tests for methods that you  explicitly  extended."},{"id":579,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  ProbabilitiesEstimator","ref":"/complexitymeasures/stable/devdocs/#Adding-a-new-ProbabilitiesEstimator","content":" Adding a new  ProbabilitiesEstimator"},{"id":580,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/complexitymeasures/stable/devdocs/#Mandatory-steps","content":" Mandatory steps Decide on the outcome space and how the estimator will map probabilities to outcomes. Define your type and make it subtype  ProbabilitiesEstimator . Add a docstring to your type following the style of the docstrings of other estimators. If suitable, the estimator may be able to operate based on  Encoding s. If so, it is preferred to implement an  Encoding  subtype and extend the methods  encode  and  decode . This will allow your probabilities estimator to be used with a larger span of entropy and complexity methods without additional effort. Have a look at the file defining  SymbolicPermutation  for an idea of how this works. Implement dispatch for  probabilities_and_outcomes  and your probabilities estimator type. Implement dispatch for  outcome_space  and your probabilities estimator type. The return value of  outcome_space  must be sorted (as in the default behavior of  sort , in ascending order). Add your probabilities estimator type to the table list in the documentation page of probabilities. If you made an encoding, also add it to corresponding table in the encodings section."},{"id":581,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","ref":"/complexitymeasures/stable/devdocs/#Optional-steps","content":" Optional steps You may extend any of the following functions if there are potential performance benefits in doing so: probabilities . By default it calls  probabilities_and_outcomes  and returns the first value. outcomes . By default calls  probabilities_and_outcomes  and returns the second value. total_outcomes . By default it returns the  length  of  outcome_space . This is the function that most typically has performance benefits if implemented explicitly, so most existing estimators extend it by default."},{"id":582,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  DifferentialEntropyEstimator","ref":"/complexitymeasures/stable/devdocs/#Adding-a-new-DifferentialEntropyEstimator","content":" Adding a new  DifferentialEntropyEstimator"},{"id":583,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/complexitymeasures/stable/devdocs/#Mandatory-steps-2","content":" Mandatory steps Define your type and make it subtype  DifferentialEntropyEstimator . Add a docstring to your type following the style of the docstrings of other estimators.  This docstring should contain the formula(s)/integral(s) which it estimates, and a  reference to relevant  EntropyDefinition (s). Implement dispatch for  entropy  with the relevant  EntropyDefinition .  If your estimator works for multiple entropies, implement one method for   entropy  for each of them."},{"id":584,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Tests","ref":"/complexitymeasures/stable/devdocs/#Tests","content":" Tests You need to add tests verifying that your estimator actually convergences, within some reasonable tolerance (that you define), to the true entropy of data from some known distribution. Have a look in the tests for existing estimators for inspiration (you can just copy-paste one of the existing tests, or make them more elaborate if you want to)."},{"id":585,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Adding a new  EntropyDefinition","ref":"/complexitymeasures/stable/devdocs/#Adding-a-new-EntropyDefinition","content":" Adding a new  EntropyDefinition"},{"id":586,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Mandatory steps","ref":"/complexitymeasures/stable/devdocs/#Mandatory-steps-3","content":" Mandatory steps Define your entropy definition type and make it subtype  EntropyDefinition . Implement dispatch for  entropy (def::YourType, p::Probabilities) Add a docstring to your type following the style of the docstrings of other entropy  definitions, and should include the mathematical definition of the entropy. Add your entropy definition type to the list of definitions in the   docs/src/entropies.md  documentation page. Add a reference to your entropy definition in the docstring for   EntropyDefinition ."},{"id":587,"pagetitle":"ComplexityMeasures.jl Dev Docs","title":"Optional steps","ref":"/complexitymeasures/stable/devdocs/#Optional-steps-2","content":" Optional steps If the maximum value of your entropy type is analytically computable for a probability  distribution with a known number of elements, implementing dispatch for   entropy_maximum  automatically enables  entropy_normalized  for your  type."},{"id":590,"pagetitle":"Encodings","title":"Encodings","ref":"/complexitymeasures/stable/encodings/#Encodings","content":" Encodings"},{"id":591,"pagetitle":"Encodings","title":"Encodings API","ref":"/complexitymeasures/stable/encodings/#Encodings-API","content":" Encodings API Some probability estimators first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\". The encodings API is defined by: Encoding encode decode"},{"id":592,"pagetitle":"Encodings","title":"ComplexityMeasures.Encoding","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.Encoding","content":" ComplexityMeasures.Encoding  —  Type Encoding The supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions  encode  and  decode . Some probability estimators utilize encodings internally. Current available encodings are: OrdinalPatternEncoding . GaussianCDFEncoding . RectangularBinEncoding . source"},{"id":593,"pagetitle":"Encodings","title":"ComplexityMeasures.encode","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.encode","content":" ComplexityMeasures.encode  —  Function encode(c::Encoding, χ) -> i::Int Encode an element  χ ∈ x  of input data  x  (those given to  probabilities ) using encoding  c . The special value of  -1  is reserved as a return value for inappropriate elements  χ  that cannot be encoded according to  c . source"},{"id":594,"pagetitle":"Encodings","title":"ComplexityMeasures.decode","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.decode","content":" ComplexityMeasures.decode  —  Function decode(c::Encoding, i::Int) -> ω Decode an encoded element  i  into the outcome  ω ∈ Ω  it corresponds to. Ω  is the  outcome_space  of a probabilities estimator that uses encoding  c . source"},{"id":595,"pagetitle":"Encodings","title":"Available encodings","ref":"/complexitymeasures/stable/encodings/#Available-encodings","content":" Available encodings"},{"id":596,"pagetitle":"Encodings","title":"ComplexityMeasures.OrdinalPatternEncoding","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.OrdinalPatternEncoding","content":" ComplexityMeasures.OrdinalPatternEncoding  —  Type OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand) An encoding scheme that  encode s length- m  vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by  SymbolicPermutation  and similar estimators, see that for a description of the outcome space. The ordinal/permutation pattern of a vector  χ  is simply  sortperm(χ) , which gives the indices that would sort  χ  in ascending order. Description The Lehmer code, as implemented here, is a bijection between the set of  factorial(m)  possible permutations for a length- m  sequence, and the integers  1, 2, …, factorial(m) . The encoding step uses algorithm 1 in Berger et al. (2019) [Berger2019] , which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!). Example julia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3 If you want to encode something that is already a permutation pattern, then you can use the non-exported  permutation_to_integer  function. source"},{"id":597,"pagetitle":"Encodings","title":"ComplexityMeasures.GaussianCDFEncoding","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.GaussianCDFEncoding","content":" ComplexityMeasures.GaussianCDFEncoding  —  Type GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding(; μ, σ, c::Int = 3) An encoding scheme that  encode s a scalar value into one of the integers  sᵢ ∈ [1, 2, …, c]  based on the normal cumulative distribution function (NCDF), and  decode s the  sᵢ  into subintervals of  [0, 1]  (with some loss of information). Notice that the decoding step does not yield an element of any outcome space of the estimators that use  GaussianCDFEncoding  internally, such as  Dispersion . That is because these estimators additionally delay embed the encoded data. Description GaussianCDFEncoding  first maps an input point  $x$   (scalar) to a new real number  $y_ \\in [0, 1]$  by using the normal cumulative distribution function (CDF) with the given mean  μ  and standard deviation  σ , according to the map \\[x \\to y : y = \\dfrac{1}{ \\sigma\n    \\sqrt{2 \\pi}} \\int_{-\\infty}^{x} e^{(-(x - \\mu)^2)/(2 \\sigma^2)} dx.\\] Next, the interval  [0, 1]  is equidistantly binned and enumerated  $1, 2, \\ldots, c$ ,  and  $y$  is linearly mapped to one of these integers using the linear map   $y \\to z : z = \\text{floor}(y(c-1)) + 1$ . Because of the floor operation, some information is lost, so when used with  decode , each decoded  sᵢ  is mapped to a  subinterval  of  [0, 1] . Examples julia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6 source"},{"id":598,"pagetitle":"Encodings","title":"ComplexityMeasures.RectangularBinEncoding","ref":"/complexitymeasures/stable/encodings/#ComplexityMeasures.RectangularBinEncoding","content":" ComplexityMeasures.RectangularBinEncoding  —  Type RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning) An encoding scheme that  encode s points  χ ∈ x  into their histogram bins. The first call signature simply initializes a  FixedRectangularBinning  and then calls the second call signature. See  FixedRectangularBinning  for info on mapping points to bins. source Berger2019 Berger et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023."},{"id":601,"pagetitle":"Entropies","title":"Entropies","ref":"/complexitymeasures/stable/entropies/#entropies","content":" Entropies"},{"id":602,"pagetitle":"Entropies","title":"Entropies API","ref":"/complexitymeasures/stable/entropies/#Entropies-API","content":" Entropies API The entropies API is defined by EntropyDefinition entropy DiscreteEntropyEstimator DifferentialEntropyEstimator Please be sure you have read the  Terminology  section before going through the API here, to have a good idea of the different \"flavors\" of entropies and how they all come together over the common interface of the  entropy  function."},{"id":603,"pagetitle":"Entropies","title":"Entropy definitions","ref":"/complexitymeasures/stable/entropies/#Entropy-definitions","content":" Entropy definitions"},{"id":604,"pagetitle":"Entropies","title":"ComplexityMeasures.EntropyDefinition","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.EntropyDefinition","content":" ComplexityMeasures.EntropyDefinition  —  Type EntropyDefinition EntropyDefinition  is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below. Currently implemented entropy definitions are: Renyi . Tsallis . Shannon , which is a subcase of the above two in the limit  q → 1 . Kaniadakis . Curado . StretchedExponential . These types can be given as inputs to  entropy  or  entropy_normalized . Description Mathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.'s [Amigó2018]  summary paper gives a nice overview. However, for a software implementation computing entropies  in practice , definitions is not really what matters;  estimators matter . Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons. That is why the type  DiscreteEntropyEstimator  exists, which is what is actually given to  entropy . Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of  EntropyDefinition  to the estimator. source"},{"id":605,"pagetitle":"Entropies","title":"ComplexityMeasures.Shannon","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Shannon","content":" ComplexityMeasures.Shannon  —  Type Shannon <: EntropyDefinition\nShannon(; base = 2) The Shannon [Shannon1948]  entropy, used with  entropy  to compute: \\[H(p) = - \\sum_i p[i] \\log(p[i])\\] with the  $\\log$  at the given  base . The maximum value of the Shannon entropy is  $\\log_{base}(L)$ , which is the entropy of the uniform distribution with  $L$  the  total_outcomes . source"},{"id":606,"pagetitle":"Entropies","title":"ComplexityMeasures.Renyi","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Renyi","content":" ComplexityMeasures.Renyi  —  Type Renyi <: EntropyDefinition\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2) The Rényi [Rényi1960]  generalized order- q  entropy, used with  entropy  to compute an entropy with units given by  base  (typically  2  or  MathConstants.e ). Description Let  $p$  be an array of probabilities (summing to 1). Then the Rényi generalized entropy is \\[H_q(p) = \\frac{1}{1-q} \\log \\left(\\sum_i p[i]^q\\right)\\] and generalizes other known entropies, like e.g. the information entropy ( $q = 1$ , see  [Shannon1948] ), the maximum entropy ( $q=0$ , also known as Hartley entropy), or the correlation entropy ( $q = 2$ , also known as collision entropy). The maximum value of the Rényi entropy is  $\\log_{base}(L)$ , which is the entropy of the uniform distribution with  $L$  the  total_outcomes . source"},{"id":607,"pagetitle":"Entropies","title":"ComplexityMeasures.Tsallis","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Tsallis","content":" ComplexityMeasures.Tsallis  —  Type Tsallis <: EntropyDefinition\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2) The Tsallis [Tsallis1988]  generalized order- q  entropy, used with  entropy  to compute an entropy. base  only applies in the limiting case  q == 1 , in which the Tsallis entropy reduces to Shannon entropy. Description The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with  k  standing for the Boltzmann constant. It is defined as \\[S_q(p) = \\frac{k}{q - 1}\\left(1 - \\sum_{i} p[i]^q\\right)\\] The maximum value of the Tsallis entropy is `` $k(L^{1 - q} - 1)/(1 - q)$ , with  $L$  the  total_outcomes . source"},{"id":608,"pagetitle":"Entropies","title":"ComplexityMeasures.Kaniadakis","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Kaniadakis","content":" ComplexityMeasures.Kaniadakis  —  Type Kaniadakis <: EntropyDefinition\nKaniadakis(; κ = 1.0, base = 2.0) The Kaniadakis entropy (Tsallis, 2009) [Tsallis2009] , used with  entropy  to compute \\[H_K(p) = -\\sum_{i=1}^N p_i f_\\kappa(p_i),\\] \\[f_\\kappa (x) = \\dfrac{x^\\kappa - x^{-\\kappa}}{2\\kappa},\\] where if  $\\kappa = 0$ , regular logarithm to the given  base  is used, and 0 probabilities are skipped. source"},{"id":609,"pagetitle":"Entropies","title":"ComplexityMeasures.Curado","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Curado","content":" ComplexityMeasures.Curado  —  Type Curado <: EntropyDefinition\nCurado(; b = 1.0) The Curado entropy (Curado & Nobre, 2004) [Curado2004] , used with  entropy  to compute \\[H_C(p) = \\left( \\sum_{i=1}^N e^{-b p_i} \\right) + e^{-b} - 1,\\] with  b ∈ ℛ, b > 0 , where the terms outside the sum ensures that  $H_C(0) = H_C(1) = 0$ . The maximum entropy for Curado is  $L(1 - \\exp(-b/L)) + \\exp(-b) - 1$  with  $L$  the  total_outcomes . source"},{"id":610,"pagetitle":"Entropies","title":"ComplexityMeasures.StretchedExponential","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.StretchedExponential","content":" ComplexityMeasures.StretchedExponential  —  Type StretchedExponential <: EntropyDefinition\nStretchedExponential(; η = 2.0, base = 2) The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999 [Anteneodo1999] ), used with  entropy  to compute \\[S_{\\eta}(p) = \\sum_{i = 1}^N\n\\Gamma \\left( \\dfrac{\\eta + 1}{\\eta}, - \\log_{base}(p_i) \\right) -\np_i \\Gamma \\left( \\dfrac{\\eta + 1}{\\eta} \\right),\\] where  $\\eta \\geq 0$ ,  $\\Gamma(\\cdot, \\cdot)$  is the upper incomplete Gamma function, and  $\\Gamma(\\cdot) = \\Gamma(\\cdot, 0)$  is the Gamma function. Reduces to  Shannon  entropy for  η = 1.0 . The maximum entropy for  StrechedExponential  is a rather complicated expression involving incomplete Gamma functions (see source code). source"},{"id":611,"pagetitle":"Entropies","title":"Discrete entropy","ref":"/complexitymeasures/stable/entropies/#Discrete-entropy","content":" Discrete entropy"},{"id":612,"pagetitle":"Entropies","title":"ComplexityMeasures.entropy","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}","content":" ComplexityMeasures.entropy  —  Method entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)\nentropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) Compute the  discrete entropy h::Real ∈ [0, ∞) , using the estimator  e , in one of two ways: Directly from existing  Probabilities probs . From input data  x , by first estimating a probability mass function using the provided  ProbabilitiesEstimator , and then computing the entropy from that mass fuction using the provided  DiscreteEntropyEstimator . Instead of providing a  DiscreteEntropyEstimator , an  EntropyDefinition  can be given directly, in which case  MLEntropy  is used as the estimator. If  e  is not provided,  Shannon ()  is used by default. Maximum entropy and normalized entropy All discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the  entropy_maximum . Or, one can use  entropy_normalized  to obtain the normalized form of the entropy (divided by the maximum). Examples x = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss source"},{"id":613,"pagetitle":"Entropies","title":"ComplexityMeasures.entropy_maximum","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.entropy_maximum","content":" ComplexityMeasures.entropy_maximum  —  Function entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x) Return the maximum value of a discrete entropy with the given probabilities estimator and input data  x . Like in  outcome_space , for some estimators the concrete outcome space is known without knowledge of input  x , in which case the function dispatches to  entropy_maximum(e, est) . entropy_maximum(e::EntropyDefinition, L::Int) Same as above, but computed directly from the number of total outcomes  L . source"},{"id":614,"pagetitle":"Entropies","title":"ComplexityMeasures.entropy_normalized","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.entropy_normalized","content":" ComplexityMeasures.entropy_normalized  —  Function entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) → h̃ Return  h̃ ∈ [0, 1] , the normalized discrete entropy of  x , i.e. the value of  entropy  divided by the maximum value for  e , according to the given probabilities estimator. Instead of a discrete entropy estimator, an  EntropyDefinition  can be given as first argument. If  e  is not given, it defaults to  Shannon() . Notice that there is no method  entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities) , because there is no way to know the amount of  possible  events (i.e., the  total_outcomes ) from  probs . source"},{"id":615,"pagetitle":"Entropies","title":"Discrete entropy estimators","ref":"/complexitymeasures/stable/entropies/#Discrete-entropy-estimators","content":" Discrete entropy estimators"},{"id":616,"pagetitle":"Entropies","title":"ComplexityMeasures.DiscreteEntropyEstimator","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.DiscreteEntropyEstimator","content":" ComplexityMeasures.DiscreteEntropyEstimator  —  Type DiscreteEntropyEstimator\nDiscEntropyEst # alias Supertype of all discrete entropy estimators. Currently only the  MLEntropy  estimator is provided, which does not need to be used, as using an  EntropyDefinition  directly in  entropy  is possible. But in the future, more advanced estimators will be added ( #237 ). source"},{"id":617,"pagetitle":"Entropies","title":"ComplexityMeasures.MLEntropy","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.MLEntropy","content":" ComplexityMeasures.MLEntropy  —  Type MLEntropy(e::EntropyDefinition) <: DiscreteEntropyEstimator Standing for \"maximum likelihood entropy\", and also called empirical/naive/plug-in, this estimator calculates the entropy exactly as defined in the given  EntropyDefinition  directly from a probability mass function. source"},{"id":618,"pagetitle":"Entropies","title":"Differential entropy","ref":"/complexitymeasures/stable/entropies/#Differential-entropy","content":" Differential entropy"},{"id":619,"pagetitle":"Entropies","title":"ComplexityMeasures.entropy","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.entropy-Tuple{DifferentialEntropyEstimator, Any}","content":" ComplexityMeasures.entropy  —  Method entropy(est::DifferentialEntropyEstimator, x) → h Approximate the  differential entropy h::Real  using the provided  DifferentialEntropyEstimator  and input data  x . This method doesn't involve explicitly computing (discretized) probabilities first. The overwhelming majority of entropy estimators estimate the Shannon entropy. If some estimator can estimate different  definitions  of entropy (e.g.,  Tsallis ), this is provided as an argument to the estimator itself. See the  table of differential entropy estimators  in the docs for all differential entropy estimators. Examples A standard normal distribution has a base-e differential entropy of  0.5*log(2π) + 0.5  nats. est = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = entropy(est, randn(1_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.001 source"},{"id":620,"pagetitle":"Entropies","title":"Table of differential entropy estimators","ref":"/complexitymeasures/stable/entropies/#table_diff_ent_est","content":" Table of differential entropy estimators The following estimators are  differential  entropy estimators, and can also be used with  entropy . Each  DifferentialEntropyEstimator s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example,  Kraskov  estimates the  Shannon  entropy. Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential KozachenkoLeonenko Nearest neighbors StateSpaceSet ✓ x x x x x Kraskov Nearest neighbors StateSpaceSet ✓ x x x x x Zhu Nearest neighbors StateSpaceSet ✓ x x x x x ZhuSingh Nearest neighbors StateSpaceSet ✓ x x x x x Gao Nearest neighbors StateSpaceSet ✓ x x x x x Goria Nearest neighbors StateSpaceSet ✓ x x x x x Lord Nearest neighbors StateSpaceSet ✓ x x x x x Vasicek Order statistics Vector ✓ x x x x x Ebrahimi Order statistics Vector ✓ x x x x x Correa Order statistics Vector ✓ x x x x x AlizadehArghami Order statistics Vector ✓ x x x x x"},{"id":621,"pagetitle":"Entropies","title":"ComplexityMeasures.DifferentialEntropyEstimator","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.DifferentialEntropyEstimator","content":" ComplexityMeasures.DifferentialEntropyEstimator  —  Type DifferentialEntropyEstimator\nDiffEntropyEst # alias The supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution. See the  table of differential entropy estimators  in the docs for all differential entropy estimators. See  entropy  for usage. source"},{"id":622,"pagetitle":"Entropies","title":"ComplexityMeasures.Kraskov","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Kraskov","content":" ComplexityMeasures.Kraskov  —  Type Kraskov <: DiffEntropyEst\nKraskov(; k::Int = 1, w::Int = 1, base = 2) The  Kraskov  estimator computes the  Shannon  differential  entropy  of a multi-dimensional  StateSpaceSet  using the  k -th nearest neighbor searches method from  [Kraskov2004]  at the given  base . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Kraskov  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] See also:  entropy ,  KozachenkoLeonenko ,  DifferentialEntropyEstimator . source"},{"id":623,"pagetitle":"Entropies","title":"ComplexityMeasures.KozachenkoLeonenko","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.KozachenkoLeonenko","content":" ComplexityMeasures.KozachenkoLeonenko  —  Type KozachenkoLeonenko <: DiffEntropyEst\nKozachenkoLeonenko(; w::Int = 0, base = 2) The  KozachenkoLeonenko  estimator computes the  Shannon  differential  entropy  of a multi-dimensional  StateSpaceSet  in the given  base . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  KozachenkoLeonenko  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))]\\] using the nearest neighbor method from Kozachenko & Leonenko (1987) [KozachenkoLeonenko1987] , as described in Charzyńska and Gambin [Charzyńska2016] . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). In contrast to  Kraskov , this estimator uses only the  closest  neighbor. See also:  entropy ,  Kraskov ,  DifferentialEntropyEstimator . source"},{"id":624,"pagetitle":"Entropies","title":"ComplexityMeasures.Zhu","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Zhu","content":" ComplexityMeasures.Zhu  —  Type Zhu <: DiffEntropyEst\nZhu(; k = 1, w = 0, base = 2) The  Zhu  estimator (Zhu et al., 2015) [Zhu2015]  is an extension to  KozachenkoLeonenko , and computes the  Shannon  differential  entropy  of a multi-dimensional  StateSpaceSet  in the given  base . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Zhu  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))]\\] by approximating densities within hyperrectangles surrounding each point  xᵢ ∈ x  using using  k  nearest neighbor searches.  w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). See also:  entropy ,  KozachenkoLeonenko ,  DifferentialEntropyEstimator . source"},{"id":625,"pagetitle":"Entropies","title":"ComplexityMeasures.ZhuSingh","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.ZhuSingh","content":" ComplexityMeasures.ZhuSingh  —  Type ZhuSingh <: DiffEntropyEst\nZhuSingh(; k = 1, w = 0, base = 2) The  ZhuSingh  estimator (Zhu et al., 2015) [Zhu2015]  computes the  Shannon  differential  entropy  of a multi-dimensional  StateSpaceSet  in the given  base . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  ZhuSingh  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] Like  Zhu , this estimator approximates probabilities within hyperrectangles surrounding each point  xᵢ ∈ x  using using  k  nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003). w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). See also:  entropy ,  DifferentialEntropyEstimator . source"},{"id":626,"pagetitle":"Entropies","title":"ComplexityMeasures.Gao","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Gao","content":" ComplexityMeasures.Gao  —  Type Gao <: DifferentialEntropyEstimator\nGao(; k = 1, w = 0, base = 2, corrected = true) The  Gao  estimator (Gao et al., 2015) computes the  Shannon  differential  entropy , using a  k -th nearest-neighbor approach based on Singh et al. (2003) [Singh2003] . w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Gao et al., 2015 give two variants of this estimator. If  corrected == false , then the uncorrected version is used. If  corrected == true , then the corrected version is used, which ensures that the estimator is asymptotically unbiased. Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  KozachenkoLeonenko  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))]\\] source"},{"id":627,"pagetitle":"Entropies","title":"ComplexityMeasures.Goria","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Goria","content":" ComplexityMeasures.Goria  —  Type Goria <: DifferentialEntropyEstimator\nGoria(; k = 1, w = 0, base = 2) The  Goria  estimator computes the  Shannon  differential  entropy  of a multi-dimensional  StateSpaceSet  in the given  base . Description Assume we have samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Goria  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] Specifically, let  $\\bf{n}_1, \\bf{n}_2, \\ldots, \\bf{n}_N$  be the distance of the samples  $\\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  to their  k -th nearest neighbors. Next, let the geometric mean of the distances be \\[\\hat{\\rho}_k = \\left( \\prod_{i=1}^N \\right)^{\\dfrac{1}{N}}\\] Goria et al. (2005) [Goria2005] 's estimate of Shannon differential entropy is then \\[\\hat{H} = m\\hat{\\rho}_k + \\log(N - 1) - \\psi(k) + \\log c_1(m),\\] where  $c_1(m) = \\dfrac{2\\pi^\\frac{m}{2}}{m \\Gamma(m/2)}$  and  $\\psi$  is the digamma function. source"},{"id":628,"pagetitle":"Entropies","title":"ComplexityMeasures.Lord","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Lord","content":" ComplexityMeasures.Lord  —  Type Lord <: DifferentialEntropyEstimator\nLord(; k = 10, w = 0, base = 2) Lord  estimates the  Shannon  differential  entropy  using a nearest neighbor approach with a local nonuniformity correction (LNC). w  is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to  0 , meaning that only the point itself is excluded when searching for neighbours). Description Assume we have samples  $\\bar{X} = \\{\\bf{x}_1, \\bf{x}_2, \\ldots, \\bf{x}_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}^d$  with support  $\\mathcal{X}$  and density function  $f : \\mathbb{R}^d \\to \\mathbb{R}$ .  Lord  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))],\\] by using the resubstitution formula \\[\\hat{\\bar{X}, k} = -\\mathbb{E}[\\log(f(X))]\n\\approx \\sum_{i = 1}^N \\log(\\hat{f}(\\bf{x}_i)),\\] where  $\\hat{f}(\\bf{x}_i)$  is an estimate of the density at  $\\bf{x}_i$  constructed in a manner such that  $\\hat{f}(\\bf{x}_i) \\propto \\dfrac{k(x_i) / N}{V_i}$ , where  $k(x_i)$  is the number of points in the neighborhood of  $\\bf{x}_i$ , and  $V_i$  is the volume of that neighborhood. While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities  $\\hat{f}(\\bf{x}_i)$ , the  Lord  estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point  xᵢ , estimated using singular value decomposition (SVD) on the  k -th nearest neighbors of  xᵢ . Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes  Lord  a well-suited entropy estimator for a wide range of systems. source"},{"id":629,"pagetitle":"Entropies","title":"ComplexityMeasures.Vasicek","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Vasicek","content":" ComplexityMeasures.Vasicek  —  Type Vasicek <: DiffEntropyEst\nVasicek(; m::Int = 1, base = 2) The  Vasicek  estimator computes the  Shannon  differential  entropy  (in the given  base ) of a timeseries using the method from Vasicek (1976) [Vasicek1976] . The  Vasicek  estimator belongs to a class of differential entropy estimators based on  order statistics , of which Vasicek (1976) was the first. It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Vasicek  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  Vasicek Shannon  differential entropy estimate is then \\[\\hat{H}_V(\\bar{X}, m) =\n\\dfrac{1}{n}\n\\sum_{i = 1}^n \\log \\left[ \\dfrac{n}{2m} (\\bar{X}_{(i+m)} - \\bar{X}_{(i-m)}) \\right]\\] Usage In practice, choice of  m  influences how fast the entropy converges to the true value. For small value of  m , convergence is slow, so we recommend to scale  m  according to the time series length  n  and use  m >= n/100  (this is just a heuristic based on the tests written for this package). See also:  entropy ,  Correa ,  AlizadehArghami ,  Ebrahimi ,  DifferentialEntropyEstimator . source"},{"id":630,"pagetitle":"Entropies","title":"ComplexityMeasures.AlizadehArghami","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.AlizadehArghami","content":" ComplexityMeasures.AlizadehArghami  —  Type AlizadehArghami <: DiffEntropyEst\nAlizadehArghami(; m::Int = 1, base = 2) The  AlizadehArghami estimator computes the  Shannon  differential  entropy  (in the given  base ) of a timeseries using the method from Alizadeh & Arghami (2010) [Alizadeh2010] . The  AlizadehArghami  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  AlizadehArghami  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ : \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp.\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  AlizadehArghami Shannon  differential entropy estimate is then the the  Vasicek  estimate  $\\hat{H}_{V}(\\bar{X}, m, n)$ , plus a correction factor \\[\\hat{H}_{A}(\\bar{X}, m, n) = \\hat{H}_{V}(\\bar{X}, m, n) +\n\\dfrac{2}{n}\\left(m \\log(2) \\right).\\] See also:  entropy ,  Correa ,  Ebrahimi ,  Vasicek ,  DifferentialEntropyEstimator . source"},{"id":631,"pagetitle":"Entropies","title":"ComplexityMeasures.Ebrahimi","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Ebrahimi","content":" ComplexityMeasures.Ebrahimi  —  Type Ebrahimi <: DiffEntropyEst\nEbrahimi(; m::Int = 1, base = 2) The  Ebrahimi  estimator computes the  Shannon entropy  (in the given  base ) of a timeseries using the method from Ebrahimi (1994) [Ebrahimi1994] . The  Ebrahimi  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Ebrahimi  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly, it makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ . The  Ebrahimi Shannon  differential entropy estimate is then \\[\\hat{H}_{E}(\\bar{X}, m) =\n\\dfrac{1}{n} \\sum_{i = 1}^n \\log\n\\left[ \\dfrac{n}{c_i m} (\\bar{X}_{(i+m)} - \\bar{X}_{(i-m)}) \\right],\\] where \\[c_i =\n\\begin{cases}\n    1 + \\frac{i - 1}{m}, & 1 \\geq i \\geq m \\\\\n    2,                    & m + 1 \\geq i \\geq n - m \\\\\n    1 + \\frac{n - i}{m} & n - m + 1 \\geq i \\geq n\n\\end{cases}.\\] See also:  entropy ,  Correa ,  AlizadehArghami ,  Vasicek ,  DifferentialEntropyEstimator . source"},{"id":632,"pagetitle":"Entropies","title":"ComplexityMeasures.Correa","ref":"/complexitymeasures/stable/entropies/#ComplexityMeasures.Correa","content":" ComplexityMeasures.Correa  —  Type Correa <: DiffEntropyEst\nCorrea(; m::Int = 1, base = 2) The  Correa  estimator computes the  Shannon  differential  entropy  (in the given `base) of a timeseries using the method from Correa (1995) [Correa1995] . The  Correa  estimator belongs to a class of differential entropy estimators based on  order statistics . It only works for  timeseries  input. Description Assume we have samples  $\\bar{X} = \\{x_1, x_2, \\ldots, x_N \\}$  from a continuous random variable  $X \\in \\mathbb{R}$  with support  $\\mathcal{X}$  and density function $f : \\mathbb{R} \\to \\mathbb{R}$ .  Correa  estimates the  Shannon  differential entropy \\[H(X) = \\int_{\\mathcal{X}} f(x) \\log f(x) dx = \\mathbb{E}[-\\log(f(X))].\\] However, instead of estimating the above integral directly,  Correa  makes use of the equivalent integral, where  $F$  is the distribution function for  $X$ , \\[H(X) = \\int_0^1 \\log \\left(\\dfrac{d}{dp}F^{-1}(p) \\right) dp\\] This integral is approximated by first computing the  order statistics  of  $\\bar{X}$  (the input timeseries), i.e.  $x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}$ , ensuring that end points are included. The  Correa  estimate of  Shannon  differential entropy is then \\[H_C(\\bar{X}, m, n) =\n\\dfrac{1}{n} \\sum_{i = 1}^n \\log\n\\left[ \\dfrac{ \\sum_{j=i-m}^{i+m}(\\bar{X}_{(j)} -\n\\tilde{X}_{(i)})(j - i)}{n \\sum_{j=i-m}^{i+m} (\\bar{X}_{(j)} - \\tilde{X}_{(i)})^2}\n\\right],\\] where \\[\\tilde{X}_{(i)} = \\dfrac{1}{2m + 1} \\sum_{j = i - m}^{i + m} X_{(j)}.\\] See also:  entropy ,  AlizadehArghami ,  Ebrahimi ,  Vasicek ,  DifferentialEntropyEstimator . source Amigó2018 Amigó, J. M., Balogh, S. G., & Hernández, S. (2018). A brief review of generalized entropies.  Entropy, 20(11), 813. Shannon1948 C. E. Shannon, Bell Systems Technical Journal  27 , pp 379 (1948) Rényi1960 A. Rényi,  Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability , pp 547 (1960) Shannon1948 C. E. Shannon, Bell Systems Technical Journal  27 , pp 379 (1948) Tsallis1988 Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487. Tsallis2009 Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1. Curado2004 Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106. Anteneodo1999 Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089. Kraskov2004 Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138. Charzyńska2016 Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13. KozachenkoLeonenko1987 Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16. Zhu2015 Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201. Zhu2015 Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201. Singh2003 Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321. Gao2015 Gao, S., Ver Steeg, G., & Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR. Singh2003 Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321. Goria2005 Goria, M. N., Leonenko, N. N., Mergel, V. V., & Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297. Lord2015 Lord, W. M., Sun, J., & Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(3), 033114. Vasicek1976 Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59. Alizadeh2010 Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS). Ebrahimi1994 Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234. Correa1995 Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449."},{"id":635,"pagetitle":"ComplexityMeasures.jl Examples","title":"ComplexityMeasures.jl Examples","ref":"/complexitymeasures/stable/examples/#ComplexityMeasures.jl-Examples","content":" ComplexityMeasures.jl Examples"},{"id":636,"pagetitle":"ComplexityMeasures.jl Examples","title":"Probabilities: kernel density","ref":"/complexitymeasures/stable/examples/#Probabilities:-kernel-density","content":" Probabilities: kernel density Here, we draw some random points from a 2D normal distribution. Then, we use kernel density estimation to associate a probability to each point  p , measured by how many points are within radius  1.5  of  p . Plotting the actual points, along with their associated probabilities estimated by the KDE procedure, we get the following surface plot. using ComplexityMeasures\nusing CairoMakie\nusing Distributions: MvNormal\n\n𝒩 = MvNormal([1, -4], 2)\nN = 500\nD = StateSpaceSet(sort([rand(𝒩) for i = 1:N]))\nx, y = columns(D)\np = probabilities(NaiveKernel(1.5), D)\nfig, ax = scatter(D[:, 1], D[:, 2], zeros(N);\n    markersize=8, axis=(type = Axis3,)\n)\nsurface!(ax, x, y, p.p)\nax.zlabel = \"P\"\nax.zticklabelsvisible = false\nfig"},{"id":637,"pagetitle":"ComplexityMeasures.jl Examples","title":"Probabilities: KL-divergence of histograms","ref":"/complexitymeasures/stable/examples/#Probabilities:-KL-divergence-of-histograms","content":" Probabilities: KL-divergence of histograms In this example we show how simple it is to compute the  KL-divergence  (or any other distance function for probability distributions) using ComplexityMeasures.jl. For simplicity, we will compute the KL-divergence between the  ValueHistogram s of two timeseries. Note that it is  crucial  to use  allprobabilities  instead of  probabilities . using ComplexityMeasures\n\nN = 1000\nt = range(0, 20π; length=N)\nx = @. clamp(sin(t), -0.5, 1)\ny = @. sin(t + cos(2t))\n\nr = -1:0.1:1\nest = ValueHistogram(FixedRectangularBinning(r))\npx = allprobabilities(est, x)\npy = allprobabilities(est, y)\n\n# Visualize\nusing CairoMakie\nbins = r[1:end-1] .+ step(r)/2\nfig, ax = barplot(bins, px; label = L\"p_x\")\nbarplot!(ax, bins, py; label = L\"p_y\")\naxislegend(ax; labelsize = 30)\nfig using StatsBase: kldivergence\n\nkldivergence(px, py) kldivergence(py, px) ( Inf  because there are events with 0 probability in  px )"},{"id":638,"pagetitle":"ComplexityMeasures.jl Examples","title":"Differential entropy: estimator comparison","ref":"/complexitymeasures/stable/examples/#Differential-entropy:-estimator-comparison","content":" Differential entropy: estimator comparison Here, we compare how the nearest neighbor differential entropy estimators ( Kraskov ,  KozachenkoLeonenko ,  Zhu  and  ZhuSingh ) converge towards the true entropy value for increasing time series length. ComplexityMeasures.jl also provides entropy estimators based on  order statistics . These estimators are only defined for scalar-valued vectors, in this example, so we compute these estimates separately, and add these estimators ( Vasicek ,  Ebrahimi ,  AlizadehArghami  and  Correa ) to the comparison. Input data are from a normal 1D distribution  $\\mathcal{N}(0, 1)$ , for which the true entropy is  0.5*log(2π) + 0.5  nats when using natural logarithms. using ComplexityMeasures\nusing CairoMakie, Statistics\nnreps = 30\nNs = [100:100:500; 1000:1000:5000]\ne = Shannon(; base = MathConstants.e)\n\n# --------------------------\n# kNN estimators\n# --------------------------\nw = 0 # Theiler window of 0 (only exclude the point itself during neighbor searches)\nknn_estimators = [\n    # with k = 1, Kraskov is virtually identical to\n    # Kozachenko-Leonenko, so pick a higher number of neighbors for Kraskov\n    Kraskov(; k = 3, base = ℯ, w),\n    KozachenkoLeonenko(; base = ℯ, w),\n    Zhu(; k = 3, base = ℯ, w),\n    ZhuSingh(; k = 3, base = ℯ, w),\n    Gao(; k = 3, base = ℯ, corrected = false, w),\n    Gao(; k = 3, base = ℯ, corrected = true, w),\n    Goria(; k = 3, w, base = ℯ),\n    Lord(; k = 20, w, base = ℯ) # more neighbors for accurate ellipsoid estimation\n]\n\n# Test each estimator `nreps` times over time series of varying length.\nHs_uniform_knn = [[zeros(nreps) for N in Ns] for e in knn_estimators]\nfor (i, est) in enumerate(knn_estimators)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) |> StateSpaceSet\n        for (k, N) in enumerate(Ns)\n            Hs_uniform_knn[i][k][j] = entropy(est, pts[1:N])\n        end\n    end\nend\n\n# --------------------------\n# Order statistic estimators\n# --------------------------\n\n# Just provide types here, they are instantiated inside the loop\nestimators_os = [Vasicek, Ebrahimi, AlizadehArghami, Correa]\nHs_uniform_os = [[zeros(nreps) for N in Ns] for e in estimators_os]\nfor (i, est_os) in enumerate(estimators_os)\n    for j = 1:nreps\n        pts = randn(maximum(Ns)) # raw timeseries, not a `StateSpaceSet`\n        for (k, N) in enumerate(Ns)\n            m = floor(Int, N / 100) # Scale `m` to timeseries length\n            est = est_os(; m, base = ℯ) # Instantiate estimator with current `m`\n            Hs_uniform_os[i][k][j] = entropy(est, pts[1:N])\n        end\n    end\nend\n\n# -------------\n# Plot results\n# -------------\nfig = Figure(resolution = (700, 11 * 200))\nlabels_knn = [\"KozachenkoLeonenko\", \"Kraskov\", \"Zhu\", \"ZhuSingh\", \"Gao (not corrected)\",\n    \"Gao (corrected)\", \"Goria\", \"Lord\"]\nlabels_os = [\"Vasicek\", \"Ebrahimi\", \"AlizadehArghami\", \"Correa\"]\n\nfor (i, e) in enumerate(knn_estimators)\n    Hs = Hs_uniform_knn[i]\n    ax = Axis(fig[i,1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_knn[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs); alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfor (i, e) in enumerate(estimators_os)\n    Hs = Hs_uniform_os[i]\n    ax = Axis(fig[i + length(knn_estimators),1]; ylabel = \"h (nats)\")\n    lines!(ax, Ns, mean.(Hs); color = Cycled(i), label = labels_os[i])\n    band!(ax, Ns, mean.(Hs) .+ std.(Hs), mean.(Hs) .- std.(Hs), alpha = 0.5,\n        color = (Main.COLORS[i], 0.5))\n    hlines!(ax, [(0.5*log(2π) + 0.5)], color = :black, lw = 5, linestyle = :dash)\n    ylims!(1.2, 1.6)\n    axislegend()\nend\n\nfig All estimators approach the true differential entropy, but those based on order statistics are negatively biased for small sample sizes."},{"id":639,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: permutation entropy","ref":"/complexitymeasures/stable/examples/#Discrete-entropy:-permutation-entropy","content":" Discrete entropy: permutation entropy This example plots permutation entropy for time series of the chaotic logistic map. Entropy estimates using  SymbolicWeightedPermutation  and  SymbolicAmplitudeAwarePermutation  are added here for comparison. The entropy behaviour can be parallelized with the  ChaosTools.lyapunov  of the map. using DynamicalSystemsBase, CairoMakie\n\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nrs = 3.4:0.001:4\nN_lyap, N_ent = 100000, 10000\nm, τ = 6, 1 # Symbol size/dimension and embedding lag\n\n# Generate one time series for each value of the logistic parameter r\nhs_perm, hs_wtperm, hs_ampperm = [zeros(length(rs)) for _ in 1:4]\n\nfor (i, r) in enumerate(rs)\n    ds.p[1] = r\n\n    x, t = trajectory(ds, N_ent)\n    ## `x` is a 1D dataset, need to recast into a timeseries\n    x = columns(x)[1]\n    hs_perm[i] = entropy(SymbolicPermutation(; m, τ), x)\n    hs_wtperm[i] = entropy(SymbolicWeightedPermutation(; m, τ), x)\n    hs_ampperm[i] = entropy(SymbolicAmplitudeAwarePermutation(; m, τ), x)\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; ylabel = L\"h_6 (SP)\")\nlines!(a1, rs, hs_perm; color = Cycled(2))\na2 = Axis(fig[2,1]; ylabel = L\"h_6 (WT)\")\nlines!(a2, rs, hs_wtperm; color = Cycled(3))\na3 = Axis(fig[3,1]; ylabel = L\"h_6 (SAAP)\", xlabel = L\"r\")\nlines!(a3, rs, hs_ampperm; color = Cycled(4))\n\nfor a in (a1,a2,a3)\n    hidexdecorations!(a, grid = false)\nend\nfig"},{"id":640,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: wavelet entropy","ref":"/complexitymeasures/stable/examples/#Discrete-entropy:-wavelet-entropy","content":" Discrete entropy: wavelet entropy The scale-resolved wavelet entropy should be lower for very regular signals (most of the energy is contained at one scale) and higher for very irregular signals (energy spread more out across scales). using CairoMakie\nN, a = 1000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = sin.(t);\ny = sin.(t .+ cos.(t/0.5));\nz = sin.(rand(1:15, N) ./ rand(1:10, N))\n\nh_x = entropy_wavelet(x)\nh_y = entropy_wavelet(y)\nh_z = entropy_wavelet(z)\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig"},{"id":641,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropies: properties","ref":"/complexitymeasures/stable/examples/#Discrete-entropies:-properties","content":" Discrete entropies: properties Here, we show the sensitivity of the various entropies to variations in their parameters."},{"id":642,"pagetitle":"ComplexityMeasures.jl Examples","title":"Curado entropy","ref":"/complexitymeasures/stable/examples/#Curado-entropy","content":" Curado entropy Here, we reproduce Figure 2 from Curado & Nobre (2004) [Curado2004] , showing how the  Curado  entropy changes as function of the parameter  a  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures, CairoMakie\nbs = [1.0, 1.5, 2.0, 3.0, 4.0, 10.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\nhs = [[entropy(Curado(; b = b), p) for p in ps] for b in bs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\nfor (i, b) in enumerate(bs)\n    lines!(ax, pp, hs[i], label = \"b=$b\", color = Cycled(i))\nend\naxislegend(ax)\nfig"},{"id":643,"pagetitle":"ComplexityMeasures.jl Examples","title":"Kaniadakis entropy","ref":"/complexitymeasures/stable/examples/#Kaniadakis-entropy","content":" Kaniadakis entropy Here, we show how  Kaniadakis  entropy changes as function of the parameter  a  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures\nusing CairoMakie\n\nprobs = [Probabilities([p, 1-p]) for p in 0.0:0.01:1.0]\nps = collect(0.0:0.01:1.0);\nκs = [-0.99, -0.66, -0.33, 0, 0.33, 0.66, 0.99];\nHs = [[entropy(Kaniadakis(κ = κ), p) for p in probs] for κ in κs];\n\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"p\", ylabel = \"H(p)\")\n\nfor (i, H) in enumerate(Hs)\n    lines!(ax, ps, H, label = \"$(κs[i])\")\nend\n\naxislegend()\n\nfig"},{"id":644,"pagetitle":"ComplexityMeasures.jl Examples","title":"Stretched exponential entropy","ref":"/complexitymeasures/stable/examples/#Stretched-exponential-entropy","content":" Stretched exponential entropy Here, we reproduce the example from Anteneodo & Plastino (1999) [Anteneodo1999] , showing how the stretched exponential entropy changes as function of the parameter  η  for a range of two-element probability distributions given by  Probabilities([p, 1 - p] for p in 1:0.0:0.01:1.0) . using ComplexityMeasures, SpecialFunctions, CairoMakie\nηs = [0.01, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 3.0]\nps = [Probabilities([p, 1 - p]) for p = 0.0:0.01:1.0]\n\nhs_norm = [[entropy(StretchedExponential( η = η), p) / gamma((η + 1)/η) for p in ps] for η in ηs]\nfig = Figure()\nax = Axis(fig[1,1]; xlabel = \"p\", ylabel = \"H(p)\")\npp = [p[1] for p in ps]\n\nfor (i, η) in enumerate(ηs)\n    lines!(ax, pp, hs_norm[i], label = \"η=$η\")\nend\naxislegend(ax)\nfig"},{"id":645,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: dispersion entropy","ref":"/complexitymeasures/stable/examples/#dispersion_example","content":" Discrete entropy: dispersion entropy Here we compute dispersion entropy (Rostaghi et al. 2016) [Rostaghi2016] , using the use the  Dispersion  probabilities estimator, for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example is adapted from Li et al. (2021) [Li2019] . using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_de = Dispersion(c = c, m = m, τ = 1)\nfor (i, window) in enumerate(windows)\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = '●', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig"},{"id":646,"pagetitle":"ComplexityMeasures.jl Examples","title":"Discrete entropy: normalized entropy for comparing different signals","ref":"/complexitymeasures/stable/examples/#Discrete-entropy:-normalized-entropy-for-comparing-different-signals","content":" Discrete entropy: normalized entropy for comparing different signals When comparing different signals or signals that have different length, it is best to normalize entropies so that the \"complexity\" or \"disorder\" quantification is directly comparable between signals. Here is an example based on the wavelet entropy example where we use the spectral entropy instead of the wavelet entropy: using ComplexityMeasures\nN1, N2, a = 101, 10001, 10\n\nfor N in (N1, N2)\n    local t = LinRange(0, 2*a*π, N)\n    local x = sin.(t) # periodic\n    local y = sin.(t .+ cos.(t/0.5)) # periodic, complex spectrum\n    local z = sin.(rand(1:15, N) ./ rand(1:10, N)) # random\n\n    for q in (x, y, z)\n        h = entropy(PowerSpectrum(), q)\n        n = entropy_normalized(PowerSpectrum(), q)\n        println(\"entropy: $(h), normalized: $(n).\")\n    end\nend ┌ Warning: Assignment to `h` in soft scope is ambiguous because a global variable by the same name exists: `h` will be treated as a new local. Disambiguate by using `local h` to suppress this warning or `global h` to assign to the existing global variable.\n└ @ examples.md:399\n┌ Warning: Assignment to `n` in soft scope is ambiguous because a global variable by the same name exists: `n` will be treated as a new local. Disambiguate by using `local n` to suppress this warning or `global n` to assign to the existing global variable.\n└ @ examples.md:400\nentropy: 0.3131510976800182, normalized: 0.05520585619046334.\nentropy: 1.2651873361559445, normalized: 0.22304169026158022.\nentropy: 3.3659756638764153, normalized: 0.593392677902843.\nentropy: 7.578007377271937e-5, normalized: 6.166997744620343e-6.\nentropy: 0.8397257036159663, normalized: 0.06833704775520649.\nentropy: 5.355065646190726, normalized: 0.4357963263720242. You see that while the direct entropy values of noisy signal changes strongly with  N  but they are almost the same for the normalized version. For the regular signals, the entropy decreases nevertheless because the noise contribution of the Fourier computation becomes less significant."},{"id":647,"pagetitle":"ComplexityMeasures.jl Examples","title":"Spatiotemporal permutation entropy","ref":"/complexitymeasures/stable/examples/#Spatiotemporal-permutation-entropy","content":" Spatiotemporal permutation entropy Usage of a  SpatialSymbolicPermutation  estimator is straightforward. Here we get the spatial permutation entropy of a 2D array (e.g., an image): using ComplexityMeasures\nx = rand(50, 50) # some image\nstencil = [1 1; 0 1] # or one of the other ways of specifying stencils\nest = SpatialSymbolicPermutation(stencil, x)\nh = entropy(est, x) 2.5843260012478164 To apply this to timeseries of spatial data, simply loop over the call, e.g.: data = [rand(50, 50) for i in 1:10] # e.g., evolution of a 2D field of a PDE\nest = SpatialSymbolicPermutation(stencil, first(data))\nh_vs_t = map(d -> entropy(est, d), data) 10-element Vector{Float64}:\n 2.5836291678454884\n 2.5839762014171113\n 2.5841763476147976\n 2.5845930792336476\n 2.5839958390303277\n 2.5845559027938667\n 2.5824996445101918\n 2.5836810324854493\n 2.583287917067638\n 2.5833511582992017 Computing any other generalized spatiotemporal permutation entropy is trivial, e.g. with  Renyi : x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialSymbolicPermutation(stencil, x)\nentropy(Renyi(q = 2), est, x) 1.5562847261635089"},{"id":648,"pagetitle":"ComplexityMeasures.jl Examples","title":"Spatial discrete entropy: Fabio","ref":"/complexitymeasures/stable/examples/#Spatial-discrete-entropy:-Fabio","content":" Spatial discrete entropy: Fabio Let's see how the normalized permutation and dispersion entropies increase for an image that gets progressively more noise added to it. using ComplexityMeasures\nusing Distributions: Uniform\nusing CairoMakie\nusing Statistics\nusing TestImages, ImageTransformations, CoordinateTransformations, Rotations\n\nimg = testimage(\"fabio_grey_256\")\nrot = warp(img, recenter(RotMatrix(-3pi/2), center(img));)\noriginal = Float32.(rot)\nnoise_levels = collect(0.0:0.25:1.0) .* std(original) * 5 # % of 1 standard deviation\n\nnoisy_imgs = [i == 1 ? original : original .+ rand(Uniform(0, nL), size(original))\n    for (i, nL) in enumerate(noise_levels)]\n\n# a 2x2 stencil (i.e. dispersion/permutation patterns of length 4)\nstencil = ((2, 2), (1, 1))\n\nest_disp = SpatialDispersion(stencil, original; c = 5, periodic = false)\nest_perm = SpatialSymbolicPermutation(stencil, original; periodic = false)\nhs_disp = [entropy_normalized(est_disp, img) for img in noisy_imgs]\nhs_perm = [entropy_normalized(est_perm, img) for img in noisy_imgs]\n\n# Plot the results\nfig = Figure(size = (800, 1000))\nax = Axis(fig[1, 1:length(noise_levels)],\n    xlabel = \"Noise level\",\n    ylabel = \"Normalized entropy\")\nscatterlines!(ax, noise_levels, hs_disp, label = \"Dispersion\")\nscatterlines!(ax, noise_levels, hs_perm, label = \"Permutation\")\nylims!(ax, 0, 1.05)\naxislegend(position = :rb)\nfor (i, nl) in enumerate(noise_levels)\n    ax_i = Axis(fig[2, i])\n    image!(ax_i, Float32.(noisy_imgs[i]), label = \"$nl\")\n    hidedecorations!(ax_i)  # hides ticks, grid and lables\n    hidespines!(ax_i)  # hide the frame\nend\nfig While the normalized  SpatialSymbolicPermutation  entropy quickly approaches its maximum value, the normalized  SpatialDispersion  entropy much better resolves the increase in entropy as the image gets noiser. This can probably be explained by the fact that the number of possible states (or  total_outcomes ) for any given  stencil  is larger for  SpatialDispersion  than for  SpatialSymbolicPermutation , so the dispersion approach is much less sensitive to noise addition (i.e. noise saturation over the possible states is slower for  SpatialDispersion )."},{"id":649,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: reverse dispersion entropy","ref":"/complexitymeasures/stable/examples/#Complexity:-reverse-dispersion-entropy","content":" Complexity: reverse dispersion entropy Here, we compare regular dispersion entropy (Rostaghi et al., 2016) [Rostaghi2016] , and reverse dispersion entropy Li et al. (2021) [Li2019]  for a time series consisting of normally distributed noise with a single spike in the middle of the signal. We compute the entropies over a range subsets of the data, using a sliding window consisting of 70 data points, stepping the window 10 time steps at a time. This example reproduces parts of figure 3 in Li et al. (2021), but results here are not exactly the same as in the original paper, because their examples are based on randomly generated numbers and do not provide code that specify random number seeds. using ComplexityMeasures\nusing Random\nusing CairoMakie\nusing Distributions: Normal\n\nn = 1000\nts = 1:n\nx = [i == n ÷ 2 ? 50.0 : 0.0 for i in ts]\nrng = Random.default_rng()\ns = rand(rng, Normal(0, 1), n)\ny = x .+ s\n\nws = 70\nwindows = [t:t+ws for t in 1:10:n-ws]\nrdes = zeros(length(windows))\ndes = zeros(length(windows))\npes = zeros(length(windows))\n\nm, c = 2, 6\nest_rd = ReverseDispersion(; c, m, τ = 1)\nest_de = Dispersion(; c, m, τ = 1)\n\nfor (i, window) in enumerate(windows)\n    rdes[i] = complexity_normalized(est_rd, y[window])\n    des[i] = entropy_normalized(Renyi(), est_de, y[window])\nend\n\nfig = Figure()\n\na1 = Axis(fig[1,1]; xlabel = \"Time step\", ylabel = \"Value\")\nlines!(a1, ts, y)\ndisplay(fig)\n\na2 = Axis(fig[2, 1]; xlabel = \"Time step\", ylabel = \"Value\")\np_rde = scatterlines!([first(w) for w in windows], rdes,\n    label = \"Reverse dispersion entropy\",\n    color = :black,\n    markercolor = :black, marker = '●')\np_de = scatterlines!([first(w) for w in windows], des,\n    label = \"Dispersion entropy\",\n    color = :red,\n    markercolor = :red, marker = 'x', markersize = 20)\n\naxislegend(position = :rc)\nylims!(0, max(maximum(pes), 1))\nfig"},{"id":650,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: missing dispersion patterns","ref":"/complexitymeasures/stable/examples/#Complexity:-missing-dispersion-patterns","content":" Complexity: missing dispersion patterns using ComplexityMeasures\nusing CairoMakie\nusing DynamicalSystemsBase\nusing TimeseriesSurrogates\n\nest = MissingDispersionPatterns(Dispersion(m = 3, c = 7))\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1-x[1]))\nsys = DeterministicIteratedMap(logistic_rule, [0.6], [4.0])\nLs = collect(100:100:1000)\nnL = length(Ls)\nnreps = 30 # should be higher for real applications\nmethod = WLS(IAAFT(), rescale = true)\n\nr_det, r_noise = zeros(length(Ls)), zeros(length(Ls))\nr_det_surr, r_noise_surr = [zeros(nreps) for L in Ls], [zeros(nreps) for L in Ls]\ny = rand(maximum(Ls))\n\nfor (i, L) in enumerate(Ls)\n    # Deterministic time series\n    x, t = trajectory(sys, L - 1, Ttr = 5000)\n    x = columns(x)[1] # remember to make it `Vector{<:Real}\n    sx = surrogenerator(x, method)\n    r_det[i] = complexity_normalized(est, x)\n    r_det_surr[i][:] = [complexity_normalized(est, sx()) for j = 1:nreps]\n\n    # Random time series\n    r_noise[i] = complexity_normalized(est, y[1:L])\n    sy = surrogenerator(y[1:L], method)\n    r_noise_surr[i][:] = [complexity_normalized(est, sy()) for j = 1:nreps]\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel = \"Time series length (L)\",\n    ylabel = \"# missing dispersion patterns (normalized)\"\n)\n\nlines!(ax, Ls, r_det, label = \"logistic(x0 = 0.6; r = 4.0)\", color = :black)\nlines!(ax, Ls, r_noise, label = \"Uniform noise\", color = :red)\nfor i = 1:nL\n    if i == 1\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black,\n            label = \"WIAAFT surrogates (logistic)\")\n         boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red,\n            label = \"WIAAFT surrogates (noise)\")\n    else\n        boxplot!(ax, fill(Ls[i], nL), r_det_surr[i]; width = 50, color = :black)\n        boxplot!(ax, fill(Ls[i], nL), r_noise_surr[i]; width = 50, color = :red)\n    end\nend\naxislegend(position = :rc)\nylims!(0, 1.1)\n\nfig We don't need to actually to compute the quantiles here to see that for the logistic map, across all time series lengths, the  $N_{MDP}$  values are above the extremal values of the  $N_{MDP}$  values for the surrogate ensembles. Thus, we conclude that the logistic map time series has nonlinearity (well, of course). For the univariate noise time series, there is considerable overlap between  $N_{MDP}$  for the surrogate distributions and the original signal, so we can't claim nonlinearity for this signal. Of course, to robustly reject the null hypothesis, we'd need to generate a sufficient number of surrogate realizations, and actually compute quantiles to compare with."},{"id":651,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: approximate entropy","ref":"/complexitymeasures/stable/examples/#Complexity:-approximate-entropy","content":" Complexity: approximate entropy Here, we reproduce the Henon map example with  $R=0.8$  from Pincus (1991), comparing our values with relevant values from table 1 in Pincus (1991). We use  DiscreteDynamicalSystem  from  DynamicalSystemsBase  to represent the map, and use the  trajectory  function from the same package to iterate the map for different initial conditions, for multiple time series lengths. Finally, we summarize our results in box plots and compare the values to those obtained by Pincus (1991). using ComplexityMeasures\nusing DynamicalSystemsBase\nusing DelayEmbeddings\nusing CairoMakie\n\n# Equation 13 in Pincus (1991)\nfunction henon_rule(u, p, n)\n    R = p[1]\n    x, y = u\n    dx = R*y + 1 - 1.4*x^2\n    dy = 0.3*R*x\n    return SVector(dx, dy)\nend\n\nfunction henon(; u₀ = rand(2), R = 0.8)\n    DeterministicIteratedMap(henon_rule, u₀, [R])\nend\n\nts_lengths = [300, 1000, 2000, 3000]\nnreps = 100\napens_08 = [zeros(nreps) for i = 1:length(ts_lengths)]\n\n# For some initial conditions, the Henon map as specified here blows up,\n# so we need to check for infinite values.\ncontainsinf(x) = any(isinf.(x))\n\nc = ApproximateEntropy(r = 0.05, m = 2)\n\nfor (i, L) in enumerate(ts_lengths)\n    k = 1\n    while k <= nreps\n        sys = henon(u₀ = rand(2), R = 0.8)\n        t = trajectory(sys, L; Ttr = 5000)[1]\n\n        if !any([containsinf(tᵢ) for tᵢ in t])\n            x, y = columns(t)\n            apens_08[i][k] = complexity(c, x)\n            k += 1\n        end\n    end\nend\n\nfig = Figure()\n\n# Example time series\na1 = Axis(fig[1,1]; xlabel = \"Time (t)\", ylabel = \"Value\")\nsys = henon(u₀ = [0.5, 0.1], R = 0.8)\nx, y = columns(trajectory(sys, 100, Ttr = 500))\nlines!(a1, 1:length(x), x, label = \"x\")\nlines!(a1, 1:length(y), y, label = \"y\")\n\n# Approximate entropy values, compared to those of the original paper (black dots).\na2 = Axis(fig[2, 1];\n    xlabel = \"Time series length (L)\",\n    ylabel = \"ApEn(m = 2, r = 0.05)\")\n\n# hacky boxplot, but this seems to be how it's done in Makie at the moment\nn = length(ts_lengths)\nfor i = 1:n\n    boxplot!(a2, fill(ts_lengths[i], n), apens_08[i];\n        width = 200)\nend\n\nscatter!(a2, ts_lengths, [0.337, 0.385, NaN, 0.394];\n    label = \"Pincus (1991)\", color = :black)\nfig"},{"id":652,"pagetitle":"ComplexityMeasures.jl Examples","title":"Complexity: sample entropy","ref":"/complexitymeasures/stable/examples/#Complexity:-sample-entropy","content":" Complexity: sample entropy Completely regular signals should have sample entropy approaching zero, while less regular signals should have higher sample entropy. using ComplexityMeasures\nusing CairoMakie\nN, a = 2000, 10\nt = LinRange(0, 2*a*π, N)\n\nx = repeat([-5:5 |> collect; 4:-1:-4 |> collect], N ÷ 20);\ny = sin.(t .+ cos.(t/0.5));\nz = rand(N)\n\nh_x, h_y, h_z = map(t -> complexity(SampleEntropy(t), t), (x, y, z))\n\nfig = Figure()\nax = Axis(fig[1,1]; ylabel = \"x\")\nlines!(ax, t, x; color = Cycled(1), label = \"h=$(h=round(h_x, sigdigits = 5))\");\nay = Axis(fig[2,1]; ylabel = \"y\")\nlines!(ay, t, y; color = Cycled(2), label = \"h=$(h=round(h_y, sigdigits = 5))\");\naz = Axis(fig[3,1]; ylabel = \"z\", xlabel = \"time\")\nlines!(az, t, z; color = Cycled(3), label = \"h=$(h=round(h_z, sigdigits = 5))\");\nfor a in (ax, ay, az); axislegend(a); end\nfor a in (ax, ay); hidexdecorations!(a; grid=false); end\nfig Next, we compare the sample entropy obtained for different values of the radius  r  for uniform noise, normally distributed noise, and a periodic signal. using ComplexityMeasures\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\nN = 2000\nx_U = rand(N)\nx_N = rand(Normal(0, 3), N)\nx_periodic = repeat(rand(20), N ÷ 20)\n\nx_U .= (x_U .- mean(x_U)) ./ std(x_U)\nx_N .= (x_N .- mean(x_N)) ./ std(x_N)\nx_periodic .= (x_periodic .- mean(x_periodic)) ./ std(x_periodic)\n\nrs = 10 .^ range(-1, 0, length = 30)\nbase = 2\nm = 2\nhs_U = [complexity_normalized(SampleEntropy(m = m, r = r), x_U) for r in rs]\nhs_N = [complexity_normalized(SampleEntropy(m = m, r = r), x_N) for r in rs]\nhs_periodic = [complexity_normalized(SampleEntropy(m = m, r = r), x_periodic) for r in rs]\n\nfig = Figure()\n# Time series\na1 = Axis(fig[1,1]; xlabel = \"r\", ylabel = \"Sample entropy\")\nlines!(a1, rs, hs_U, label = \"Uniform noise, U(0, 1)\")\nlines!(a1, rs, hs_N, label = \"Gaussian noise, N(0, 1)\")\nlines!(a1, rs, hs_periodic, label = \"Periodic signal\")\naxislegend()\nfig"},{"id":653,"pagetitle":"ComplexityMeasures.jl Examples","title":"Statistical complexity of iterated maps","ref":"/complexitymeasures/stable/examples/#Statistical-complexity-of-iterated-maps","content":" Statistical complexity of iterated maps In this example, we reproduce parts of Fig. 1 in Rosso et al. (2007): We compute the statistical complexity of the Henon, logistic and Schuster map, as well as that of k-noise. using ComplexityMeasures\nusing Distances\nusing DynamicalSystemsBase\nusing CairoMakie\nusing FFTW\nusing Statistics\n\nN = 2^15\n\nfunction logistic(x0=0.4; r = 4.0)\n    return DeterministicIteratedMap(logistic_rule, SVector(x0), [r])\nend\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nlogistic_jacob(x, p, n) = @inbounds SMatrix{1,1}(p[1]*(1 - 2x[1]))\n\nfunction henon(u0=zeros(2); a = 1.4, b = 0.3)\n    return DeterministicIteratedMap(henon_rule, u0, [a,b])\nend\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon_jacob(x, p, n) = SMatrix{2,2}(-2*p[1]*x[1], p[2], 1.0, 0.0)\n\nfunction schuster(x0=0.5, z=3.0/2)\n    return DeterministicIteratedMap(schuster_rule, SVector(x0), [z])\nend\nschuster_rule(x, p, n) = @inbounds SVector((x[1]+x[1]^p[1]) % 1)\n\n# generate noise with power spectrum that falls like 1/f^k\nfunction k_noise(k=3)\n    function f(N)\n        x = rand(Float64, N)\n        # generate power spectrum of random numbers and multiply by f^(-k/2)\n        x_hat = fft(x) .* abs.(vec(fftfreq(length(x)))) .^ (-k/2)\n        # set to zero for frequency zero\n        x_hat[1] = 0\n        return real.(ifft(x_hat))\n    end\n    return f\nend\n\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=L\"H_S\", ylabel=L\"C_{JS}\")\n\nm, τ = 6, 1\nm_kwargs = (\n        (color=:transparent,\n        strokecolor=:red,\n        marker=:utriangle,\n        strokewidth=2),\n        (color=:transparent,\n        strokecolor=:blue,\n        marker=:rect,\n        strokewidth=2),\n        (color=:magenta,\n        marker=:circle),\n        (color=:blue,\n        marker=:rect)\n    )\n\nn = 100\n\nc = StatisticalComplexity(\n    dist=JSDivergence(),\n    est=SymbolicPermutation(; m, τ),\n    entr=Renyi()\n)\nfor (j, (ds_gen, sym, ds_name)) in enumerate(zip(\n        (logistic, henon, schuster, k_noise),\n        (:utriangle, :rect, :dtriangle, :diamond),\n        (\"Logistic map\", \"Henon map\", \"Schuster map\", \"k-noise (k=3)\"),\n    ))\n\n    if j < 4\n        dim = dimension(ds_gen())\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            ic = rand(dim) * 0.3\n            ds = ds_gen(SVector{dim}(ic))\n            x, t = trajectory(ds, N, Ttr=100)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    else\n        ds = ds_gen()\n        hs, cs = zeros(n), zeros(n)\n        for k in 1:n\n            x = ds(N)\n            hs[k], cs[k] = entropy_complexity(c, x[:, 1])\n        end\n        scatter!(ax, mean(hs), mean(cs); label=\"$ds_name\", markersize=25, m_kwargs[j]...)\n    end\nend\n\nmin_curve, max_curve = entropy_complexity_curves(c)\nlines!(ax, min_curve; color=:black)\nlines!(ax, max_curve; color=:black)\naxislegend(; position=:lt)\nfig Curado2004 Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106. Anteneodo1999 Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089. Rostaghi2016 Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614. Li2019 Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203. Rostaghi2016 Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614. Li2019 Li, Y., Gao, X., & Wang, L. (2019). Reverse dispersion entropy: a new complexity measure for sensor signal. Sensors, 19(23), 5203. Zhou2022 Zhou, Q., Shang, P., & Zhang, B. (2022). Using missing dispersion patterns to detect determinism and nonlinearity in time series data. Nonlinear Dynamics, 1-20."},{"id":656,"pagetitle":"Multiscale","title":"Multiscale","ref":"/complexitymeasures/stable/multiscale/#Multiscale","content":" Multiscale"},{"id":657,"pagetitle":"Multiscale","title":"Multiscale API","ref":"/complexitymeasures/stable/multiscale/#Multiscale-API","content":" Multiscale API The multiscale API is defined by the functions multiscale multiscale_normalized downsample which dispatch any of the  MultiScaleAlgorithm s listed below. Missing docstring. Missing docstring for  MultiScaleAlgorithm . Check Documenter's build log for details."},{"id":658,"pagetitle":"Multiscale","title":"ComplexityMeasures.Regular","ref":"/complexitymeasures/stable/multiscale/#ComplexityMeasures.Regular","content":" ComplexityMeasures.Regular  —  Type Regular <: MultiScaleAlgorithm\nRegular(; f::Function = Statistics.mean) The original multi-scale algorithm for multiscale entropy analysis (Costa et al., 2022) [Costa2002] , which yields a single downsampled time series per scale  s . Description Given a scalar-valued input time series  x , the  Regular  multiscale algorithm downsamples and coarse-grains  x  by splitting it into non-overlapping windows of length  s , and then constructing a new downsampled time series  $D_t(s, f)$  by applying the function  f  to each of the resulting length- s  windows. The downsampled time series  D_t(s)  with  t ∈ [1, 2, …, L] , where  L = floor(N / s) , is given by: \\[\\{ D_t(s, f)  \\}_{t = 1}^{L} = \\left\\{ f \\left( \\bf x_t \\right) \\right\\}_{t = 1}^{L} =\n\\left\\{\n    {f\\left( (x_i)_{i = (t - 1)s + 1}^{ts} \\right)}\n\\right\\}_{t = 1}^{L}\\] where  f  is some summary statistic applied to the length- ts-((t - 1)s + 1)  tuples  xₖ . Different choices of  f  have yield different multiscale methods appearing in the literature. For example: f == Statistics.mean  yields the original first-moment multiscale sample entropy (Costa   et al., 2002) [Costa2002] . f == Statistics.var  yields the generalized multiscale sample entropy (Costa &   Goldberger, 2015) [Costa2015] , which uses the second-moment (variance) instead of the   mean. See also:  Composite . source"},{"id":659,"pagetitle":"Multiscale","title":"ComplexityMeasures.Composite","ref":"/complexitymeasures/stable/multiscale/#ComplexityMeasures.Composite","content":" ComplexityMeasures.Composite  —  Type Composite <: MultiScaleAlgorithm\nComposite(; f::Function = Statistics.mean) Composite multi-scale algorithm for multiscale entropy analysis (Wu et al., 2013) [Wu2013] , used, with  multiscale  to compute, for example, composite multiscale entropy (CMSE). Description Given a scalar-valued input time series  x , the composite multiscale algorithm, like  Regular , downsamples and coarse-grains  x  by splitting it into non-overlapping windows of length  s , and then constructing downsampled time series by applying the function  f  to each of the resulting length- s  windows. However, Wu et al. (2013) [Wu2013]  realized that for each scale  s , there are actually  s  different ways of selecting windows, depending on where indexing starts/ends. These  s  different downsampled time series  D_t(s, f)  at each scale  s  are constructed as follows: \\[\\{ D_{k}(s) \\} = \\{ D_{t, k}(s) \\}_{t = 1}^{L}, = \\{ f \\left( \\bf x_{t, k} \\right) \\} =\n\\left\\{\n    {f\\left( (x_i)_{i = (t - 1)s + k}^{ts + k - 1} \\right)}\n\\right\\}_{t = 1}^{L},\\] where  L = floor((N - s + 1) / s)  and  1 ≤ k ≤ s , such that  $D_{i, k}(s)$  is the  i -th element of the  k -th downsampled time series at scale  s . Finally, compute  $\\dfrac{1}{s} \\sum_{k = 1}^s g(D_{k}(s))$ , where  g  is some summary function, for example  entropy  or  complexity . Relation to Regular The downsampled time series  $D_{t, 1}(s)$  constructed using the composite multiscale method is equivalent to the downsampled time series  $D_{t}(s)$  constructed using the  Regular  method, for which  k == 1  is fixed, such that only a single time series is returned. See also:  Regular . source Missing docstring. Missing docstring for  downsample . Check Documenter's build log for details. Missing docstring. Missing docstring for  multiscale . Check Documenter's build log for details. Missing docstring. Missing docstring for  multiscale_normalized . Check Documenter's build log for details."},{"id":660,"pagetitle":"Multiscale","title":"Available literature methods","ref":"/complexitymeasures/stable/multiscale/#Available-literature-methods","content":" Available literature methods A non-exhaustive list of literature methods, and the syntax to compute them, are listed below. Please open an issue or make a pull-request to  ComplexityMeasures.jl  if you find a literature method missing from this list, or if you publish a paper based on some new multiscale combination. Method Syntax Reference Refined composite multiscale dispersion entropy multiscale(Composite(), Dispersion(), est, x, normalized = true) Azami et al. (2017) [Azami2017] Multiscale sample entropy (first moment) multiscale(Regular(f = mean), SampleEntropy(), x) Costa et al. (2002) [Costa2002] Generalized multiscale sample entropy (second moment) multiscale(Regular(f = std), SampleEntropy(),  x) Costa et al. (2015) [Costa2015] Costa2002 Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102. Costa2015 Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. EntropyDefinition, 17(3), 1197-1203. Wu2013 Wu, S. D., Wu, C. W., Lin, S. G., Wang, C. C., & Lee, K. Y. (2013). Time series analysis using composite multiscale entropy. Entropy, 15(3), 1069-1084. Azami2017 Azami, H., Rostaghi, M., Abásolo, D., & Escudero, J. (2017). Refined composite multiscale dispersion entropy and its application to biomedical signals. IEEE Transactions on Biomedical Engineering, 64(12), 2872-2879. Costa2002 Costa, M., Goldberger, A. L., & Peng, C. K. (2002). Multiscale entropy analysis of complex physiologic time series. Physical review letters, 89(6), 068102. Costa2015 Costa, M. D., & Goldberger, A. L. (2015). Generalized multiscale entropy analysis: Application to quantifying the complex volatility of human heartbeat time series. EntropyDefinition, 17(3), 1197-1203."},{"id":663,"pagetitle":"Probabilities","title":"Probabilities","ref":"/complexitymeasures/stable/probabilities/#Probabilities","content":" Probabilities"},{"id":664,"pagetitle":"Probabilities","title":"Probabilities API","ref":"/complexitymeasures/stable/probabilities/#Probabilities-API","content":" Probabilities API The probabilities API is defined by ProbabilitiesEstimator probabilities probabilities_and_outcomes and related functions that you will find in the following documentation blocks:"},{"id":665,"pagetitle":"Probabilities","title":"Probabilitities","ref":"/complexitymeasures/stable/probabilities/#Probabilitities","content":" Probabilitities"},{"id":666,"pagetitle":"Probabilities","title":"ComplexityMeasures.ProbabilitiesEstimator","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator","content":" ComplexityMeasures.ProbabilitiesEstimator  —  Type ProbabilitiesEstimator The supertype for all probabilities estimators. In ComplexityMeasures.jl, probability mass functions are estimated from data by defining a set of possible outcomes  $\\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_L \\}$ , and assigning to each outcome  $\\omega_i$  a probability  $p(\\omega_i)$ , such that  $\\sum_{i=1}^N p(\\omega_i) = 1$ . It is the role of a  ProbabilitiesEstimator  to Define  $\\Omega$ , the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. Define how probabilities  $p_i = p(\\omega_i)$  are assigned to outcomes  $\\omega_i$  given input data. In practice, probability estimation is done by calling  probabilities  with some input data and one of the implemented probabilities estimators. The result is a  Probabilities p  ( Vector -like), where each element  p[i]  is the probability of the outcome  ω[i] . Use  probabilities_and_outcomes  if you need both the probabilities and the outcomes, and use  outcome_space  to obtain  $\\Omega$  alone.  The cardinality of  $\\Omega$  can be obtained using  total_outcomes . The element type of  $\\Omega$  varies between estimators, but it is guaranteed to be  hashable  and  sortable . This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization). Some estimators can deduce  $\\Omega$  without knowledge of the input, such as  SymbolicPermutation . For others, knowledge of input is necessary for concretely specifying  $\\Omega$ , such as  ValueHistogram  with  RectangularBinning . This only matters for the functions  outcome_space  and  total_outcomes . All currently implemented probability estimators are listed in a nice table in the  probabilities estimators  section of the online documentation. source"},{"id":667,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities","content":" ComplexityMeasures.probabilities  —  Function probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities Compute a probability distribution over the set of possible outcomes defined by the probabilities estimator  est , given input data  x , which is typically an  Array  or a  StateSpaceSet ; see  Input data for ComplexityMeasures.jl . Configuration options are always given as arguments to the chosen estimator. To obtain the outcomes corresponding to these probabilities, use  outcomes . Due to performance optimizations, whether the returned probablities contain  0 s as entries or not depends on the estimator. E.g., in  ValueHistogram 0 s are skipped, while in  PowerSpectrum 0  are not, because we get them for free. Use the function  allprobabilities  for a version with all  0  entries that ensures that given an  est , the indices of  p  will be independent of the input data  x . probabilities(x::Vector_or_Dataset) → p::Probabilities Estimate probabilities by directly counting the elements of  x , assuming that  Ω = sort(unique(x)) , i.e. that the outcome space is the unique elements of  x . This is mostly useful when  x  contains categorical data. See also:  Probabilities ,  ProbabilitiesEstimator . source"},{"id":668,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities!","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities!","content":" ComplexityMeasures.probabilities!  —  Function probabilities!(s, args...) Similar to  probabilities(args...) , but allows pre-allocation of temporarily used containers  s . Only works for certain estimators. See for example  SymbolicPermutation . source"},{"id":669,"pagetitle":"Probabilities","title":"ComplexityMeasures.Probabilities","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.Probabilities","content":" ComplexityMeasures.Probabilities  —  Type Probabilities <: AbstractArray\nProbabilities(x) → p Probabilities  is a simple wrapper around  x::AbstractArray{<:Real, N}  that ensures its values sum to 1, so that  p  can be interpreted as  N -dimensional probability mass function. In most use cases,  p  will be a vector.  p  behaves exactly like its contained data  x  with respect to indexing and iteration. source"},{"id":670,"pagetitle":"Probabilities","title":"Outcomes","ref":"/complexitymeasures/stable/probabilities/#Outcomes","content":" Outcomes"},{"id":671,"pagetitle":"Probabilities","title":"ComplexityMeasures.probabilities_and_outcomes","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.probabilities_and_outcomes","content":" ComplexityMeasures.probabilities_and_outcomes  —  Function probabilities_and_outcomes(est, x) Return  probs, outs , where  probs = probabilities(est, x)  and  outs[i]  is the outcome with probability  probs[i] . The element type of  outs  depends on the estimator.  outs  is a subset of the  outcome_space  of  est . See also  outcomes ,  total_outcomes . source"},{"id":672,"pagetitle":"Probabilities","title":"ComplexityMeasures.outcomes","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes","content":" ComplexityMeasures.outcomes  —  Function outcomes(est::ProbabilitiesEstimator, x) Return all (unique) outcomes contained in  x  according to the given estimator. Equivalent with  probabilities_and_outcomes(x, est)[2] , but for some estimators it may be explicitly extended for better performance. source"},{"id":673,"pagetitle":"Probabilities","title":"ComplexityMeasures.outcome_space","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcome_space","content":" ComplexityMeasures.outcome_space  —  Function outcome_space(est::ProbabilitiesEstimator, x) → Ω Return a sorted container containing all  possible  outcomes of  est  for input  x . For some estimators the concrete outcome space is known without knowledge of input  x , in which case the function dispatches to  outcome_space(est) . In general it is recommended to use the 2-argument version irrespectively of estimator. source"},{"id":674,"pagetitle":"Probabilities","title":"ComplexityMeasures.total_outcomes","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.total_outcomes","content":" ComplexityMeasures.total_outcomes  —  Function total_outcomes(est::ProbabilitiesEstimator, x) Return the length (cardinality) of the outcome space  $\\Omega$  of  est . For some estimators the concrete outcome space is known without knowledge of input  x , in which case the function dispatches to  total_outcomes(est) . In general it is recommended to use the 2-argument version irrespectively of estimator. source"},{"id":675,"pagetitle":"Probabilities","title":"ComplexityMeasures.missing_outcomes","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.missing_outcomes","content":" ComplexityMeasures.missing_outcomes  —  Function missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int Estimate a probability distribution for  x  using the given estimator, then count the number of missing (i.e. zero-probability) outcomes. See also:  MissingDispersionPatterns . source"},{"id":676,"pagetitle":"Probabilities","title":"Overview of probabilities estimators","ref":"/complexitymeasures/stable/probabilities/#probabilities_estimators","content":" Overview of probabilities estimators Any of the following estimators can be used with  probabilities  (in the column \"input data\"  it is assumed that the  eltype  of the input is  <: Real ). Estimator Principle Input data CountOccurrences Count of unique elements Any ValueHistogram Binning (histogram) Vector ,  StateSpaceSet TransferOperator Binning (transfer operator) Vector ,  StateSpaceSet NaiveKernel Kernel density estimation StateSpaceSet SymbolicPermutation Ordinal patterns Vector ,  StateSpaceSet SymbolicWeightedPermutation Ordinal patterns Vector ,  StateSpaceSet SymbolicAmplitudeAwarePermutation Ordinal patterns Vector ,  StateSpaceSet SpatialSymbolicPermutation Ordinal patterns in space Array Dispersion Dispersion patterns Vector SpatialDispersion Dispersion patterns in space Array Diversity Cosine similarity Vector WaveletOverlap Wavelet transform Vector PowerSpectrum Fourier transform Vector"},{"id":677,"pagetitle":"Probabilities","title":"Count occurrences","ref":"/complexitymeasures/stable/probabilities/#Count-occurrences","content":" Count occurrences"},{"id":678,"pagetitle":"Probabilities","title":"ComplexityMeasures.CountOccurrences","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.CountOccurrences","content":" ComplexityMeasures.CountOccurrences  —  Type CountOccurrences() A probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to  probabilities . Outcome space The outcome space is the unique sorted values of the input. Hence, input  x  is needed for a well-defined  outcome_space . source"},{"id":679,"pagetitle":"Probabilities","title":"Histograms","ref":"/complexitymeasures/stable/probabilities/#Histograms","content":" Histograms"},{"id":680,"pagetitle":"Probabilities","title":"ComplexityMeasures.ValueHistogram","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.ValueHistogram","content":" ComplexityMeasures.ValueHistogram  —  Type ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator A probability estimator based on binning the values of the data as dictated by the binning scheme  b  and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is  VisitationFrequency . Available binnings are subtypes of  AbstractBinning . The  ValueHistogram  estimator has a linearithmic time complexity ( n log(n)  for  n = length(x) ) and a linear space complexity ( l  for  l = dimension(x) ). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes  ε  without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered. ValueHistogram(ϵ::Union{Real,Vector}) A convenience method that accepts same input as  RectangularBinning  and initializes this binning directly. Outcomes The outcome space for  ValueHistogram  is the unique bins constructed from  b . Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals  [a, b) . The bins are in data units, not integer (cartesian indices units), and are returned as  SVector s, i.e., same type as input data. For convenience,  outcome_space  returns the outcomes in the same array format as the underlying binning (e.g.,  Matrix  for 2D input). For  FixedRectangularBinning  the  outcome_space  is well-defined from the binning, but for  RectangularBinning  input  x  is needed as well. source"},{"id":681,"pagetitle":"Probabilities","title":"ComplexityMeasures.AbstractBinning","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.AbstractBinning","content":" ComplexityMeasures.AbstractBinning  —  Type AbstractBinning Supertype encompassing  RectangularBinning  and  FixedRectangualrBinning . source"},{"id":682,"pagetitle":"Probabilities","title":"ComplexityMeasures.RectangularBinning","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.RectangularBinning","content":" ComplexityMeasures.RectangularBinning  —  Type RectangularBinning(ϵ, precise = false) <: AbstractBinning Rectangular box partition of state space using the scheme  ϵ , deducing the histogram extent and bin width from the input data. RectangularBinning  is a convenience struct. It is re-cast into  FixedRectangularBinning  once the data are provided, so see that docstring for info on the bin calculation and the meaning of  precise . Binning instructions are deduced from the type of  ϵ  as follows: ϵ::Int  divides each coordinate axis into  ϵ  equal-length intervals  that cover all data. ϵ::Float64  divides each coordinate axis into intervals of fixed size  ϵ , starting  from the axis minima until the data is completely covered by boxes. ϵ::Vector{Int}  divides the i-th coordinate axis into  ϵ[i]  equal-length  intervals that cover all data. ϵ::Vector{Float64}  divides the i-th coordinate axis into intervals of fixed size   ϵ[i] , starting from the axis minima until the data is completely covered by boxes. RectangularBinning  ensures all input data are covered by extending the created ranges if need be. source"},{"id":683,"pagetitle":"Probabilities","title":"ComplexityMeasures.FixedRectangularBinning","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.FixedRectangularBinning","content":" ComplexityMeasures.FixedRectangularBinning  —  Type FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false) Rectangular box partition of state space where the partition along each dimension is explicitly given by each range  ranges , which is a tuple of  AbstractRange  subtypes. Typically, each range is the output of the  range  Base function, e.g.,  ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)) . All ranges must be sorted. The optional second argument  precise  dictates whether Julia Base's  TwicePrecision  is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is  false . Points falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open:  [a, b) .  This means that the last value of each of the ranges dictates the last right-closing value.  This value does  not  belong to the histogram! E.g., if given a range  r = range(0, 1; length = 11) , with  r[end] = 1 , the value  1  is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here  [0.9, 1) )! Equivalently, the size of the histogram is  histsize = map(r -> length(r)-1, ranges) ! FixedRectangularBinning  leads to a well-defined outcome space without knowledge of input data, see  ValueHistogram . source"},{"id":684,"pagetitle":"Probabilities","title":"Symbolic permutations","ref":"/complexitymeasures/stable/probabilities/#Symbolic-permutations","content":" Symbolic permutations"},{"id":685,"pagetitle":"Probabilities","title":"ComplexityMeasures.SymbolicPermutation","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.SymbolicPermutation","content":" ComplexityMeasures.SymbolicPermutation  —  Type SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, τ = 1, lt::Function = ComplexityMeasures.isless_rand) A probabilities estimator based on ordinal permutation patterns. When passed to  probabilities  the output depends on the input data type: Univariate data . If applied to a univariate timeseries ( AbstractVector ), then the timeseries   is first embedded using embedding delay  τ  and dimension  m , resulting in embedding   vectors  $\\{ \\bf{x}_i \\}_{i=1}^{N-(m-1)\\tau}$ . Then, for each  $\\bf{x}_i$ ,   we find its permutation pattern  $\\pi_{i}$ . Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using  CountOccurrences . When giving the resulting probabilities to    entropy , the original permutation entropy is computed  [BandtPompe2002] . Multivariate data . If applied to a an  D -dimensional  StateSpaceSet ,   then no embedding is constructed,  m  must be equal to  D  and  τ  is ignored.   Each vector  $\\bf{x}_i$  of the dataset is mapped   directly to its permutation pattern  $\\pi_{i}$  by comparing the   relative magnitudes of the elements of  $\\bf{x}_i$ .   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy [He2016] , although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of [He2016] ). Internally,  SymbolicPermutation  uses the  OrdinalPatternEncoding  to represent ordinal patterns as integers for efficient computations. See  SymbolicWeightedPermutation  and  SymbolicAmplitudeAwarePermutation  for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see  SpatialSymbolicPermutation . Handling equal values in ordinal patterns In Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution  [Zunino2017] . Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using  lt = ComplexityMeasures.isless_rand . To get the behaviour from Bandt and Pompe (2002), use  lt = Base.isless . Outcome space The outcome space  Ω  for  SymbolicPermutation  is the set of length- m  ordinal patterns (i.e. permutations) that can be formed by the integers  1, 2, …, m . There are  factorial(m)  such patterns. For example, the outcome  [2, 3, 1]  corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [ OrdinalPatternEncoding (@ref). In-place symbolization SymbolicPermutation  also implements the in-place  probabilities!  for  StateSpaceSet  input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example using ComplexityMeasures\nm, N = 2, 100\nest = SymbolicPermutation(; m, τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x) source"},{"id":686,"pagetitle":"Probabilities","title":"ComplexityMeasures.SymbolicWeightedPermutation","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.SymbolicWeightedPermutation","content":" ComplexityMeasures.SymbolicWeightedPermutation  —  Type SymbolicWeightedPermutation <: ProbabilitiesEstimator\nSymbolicWeightedPermutation(; τ = 1, m = 3, lt::Function = ComplexityMeasures.isless_rand) A variant of  SymbolicPermutation  that also incorporates amplitude information, based on the weighted permutation entropy [Fadlallah2013] . The outcome space and keywords are the same as in  SymbolicPermutation . Description For each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern. An implementation note Note: in equation 7, section III, of the original paper, the authors write \\[w_j = \\dfrac{1}{m}\\sum_{k=1}^m (x_{j-(k-1)\\tau} - \\mathbf{\\hat{x}}_j^{m, \\tau})^2.\\] *But given the formula they give for the arithmetic mean, this is  not  the variance of the delay vector  $\\mathbf{x}_i$ , because the indices are mixed:  $x_{j+(k-1)\\tau}$  in the weights formula, vs.  $x_{j+(k+1)\\tau}$  in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries). source"},{"id":687,"pagetitle":"Probabilities","title":"ComplexityMeasures.SymbolicAmplitudeAwarePermutation","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.SymbolicAmplitudeAwarePermutation","content":" ComplexityMeasures.SymbolicAmplitudeAwarePermutation  —  Type SymbolicAmplitudeAwarePermutation <: ProbabilitiesEstimator\nSymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = ComplexityMeasures.isless_rand) A variant of  SymbolicPermutation  that also incorporates amplitude information, based on the amplitude-aware permutation entropy [Azami2016] . The outcome space and keywords are the same as in  SymbolicPermutation . Description Similarly to  SymbolicWeightedPermutation , a weight  $w_i$  is attached to each ordinal pattern extracted from each state (or delay) vector  $\\mathbf{x}_i = (x_1^i, x_2^i, \\ldots, x_m^i)$  as \\[w_i = \\dfrac{A}{m} \\sum_{k=1}^m |x_k^i | + \\dfrac{1-A}{d-1}\n\\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\\] with  $0 \\leq A \\leq 1$ . When  $A=0$  , only internal differences between the elements of  $\\mathbf{x}_i$  are weighted. Only mean amplitude of the state vector elements are weighted when  $A=1$ . With,  $0<A<1$ , a combined weighting is used. source"},{"id":688,"pagetitle":"Probabilities","title":"Dispersion patterns","ref":"/complexitymeasures/stable/probabilities/#Dispersion-patterns","content":" Dispersion patterns"},{"id":689,"pagetitle":"Probabilities","title":"ComplexityMeasures.Dispersion","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.Dispersion","content":" ComplexityMeasures.Dispersion  —  Type Dispersion(; c = 5, m = 2, τ = 1, check_unique = true) A probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016 [Rostaghi2016]  to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series. Recommended parameter values [Li2018]  are  m ∈ [2, 3] ,  τ = 1  for the embedding, and  c ∈ [3, 4, …, 8]  categories for the Gaussian symbol mapping. Description Assume we have a univariate time series  $X = \\{x_i\\}_{i=1}^N$ . First, this time series is encoded into a symbol timeseries  $S$  using the Gaussian encoding  GaussianCDFEncoding  with empirical mean  μ  and empirical standard deviation  σ  (both determined from  $X$ ), and  c  as given to  Dispersion . Then,  $S$  is embedded into an  $m$ -dimensional time series, using an embedding lag of  $\\tau$ , which yields a total of  $N - (m - 1)\\tau$  delay vectors  $z_i$ , or \"dispersion patterns\". Since each element of  $z_i$  can take on  c  different values, and each delay vector has  m  entries, there are  c^m  possible dispersion patterns. This number is used for normalization when computing dispersion entropy. The returned probabilities are simply the frequencies of the unique dispersion patterns present in  $S$  (i.e., the  CountOccurences  of  $S$ ). Outcome space The outcome space for  Dispersion  is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of  $S$ . Data requirements and parameters The input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that  x  has at least 1000 data points. If  check_unique == true  (default), then it is checked that the input has more than one unique value. If  check_unique == false  and the input only has one unique element, then a  InexactError  is thrown when trying to compute probabilities. Why 'dispersion patterns'? Each embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when  $m = 5$  and  $c = 3$ , and use some very imprecise terminology for illustration: When  $c = 3$ , values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector  $[2, 2, 2, 2, 2]$  consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector  $[1, 1, 2, 3, 3]$ , however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean. For a version of this estimator that can be used on high-dimensional arrays, see  SpatialDispersion . source"},{"id":690,"pagetitle":"Probabilities","title":"Transfer operator","ref":"/complexitymeasures/stable/probabilities/#Transfer-operator","content":" Transfer operator"},{"id":691,"pagetitle":"Probabilities","title":"ComplexityMeasures.TransferOperator","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.TransferOperator","content":" ComplexityMeasures.TransferOperator  —  Type TransferOperator <: ProbabilitiesEstimator\nTransferOperator(b::AbstractBinning) A probability estimator based on binning data into rectangular boxes dictated by the given binning scheme  b , then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered). This implementation follows the grid estimator approach in Diego et al. (2019) [Diego2019] . Outcome space The outcome space for  TransferOperator  is the set of unique bins constructed from  b . Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as  SVector s. Bin ordering Bins returned by  probabilities_and_outcomes  are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if b = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries then  probs[i]  is the invariant measure (probability) of the bin  outcomes[i] , which is the  i -th bin visited by the timeseries with nonzero measure. Description The transfer operator  $P^{N}$ is computed as an  N -by- N  matrix of transition probabilities between the states defined by the partition elements, where  N  is the number of boxes in the partition that is visited by the orbit/points. If   $\\{x_t^{(D)} \\}_{n=1}^L$  are the  $L$  different  $D$ -dimensional points over which the transfer operator is approximated,  $\\{ C_{k=1}^N \\}$  are the  $N$  different partition elements (as dictated by  ϵ ) that gets visited by the points, and   $\\phi(x_t) = x_{t+1}$ , then \\[P_{ij} = \\dfrac\n{\\#\\{ x_n | \\phi(x_n) \\in C_j \\cap x_n \\in C_i \\}}\n{\\#\\{ x_m | x_m \\in C_i \\}},\\] where  $\\#$  denotes the cardinal. The element  $P_{ij}$  thus indicates how many points that are initially in box  $C_i$  end up in box  $C_j$  when the points in  $C_i$  are projected one step forward in time. Thus, the row  $P_{ik}^N$  where  $k \\in \\{1, 2, \\ldots, N \\}$  gives the probability of jumping from the state defined by box  $C_i$  to any of the other  $N$  states. It follows that  $\\sum_{k=1}^{N} P_{ik} = 1$  for all  $i$ . Thus,  $P^N$  is a row/right stochastic matrix. Invariant measure estimation from transfer operator The left invariant distribution  $\\mathbf{\\rho}^N$  is a row vector, where  $\\mathbf{\\rho}^N P^{N} = \\mathbf{\\rho}^N$ . Hence,  $\\mathbf{\\rho}^N$  is a row eigenvector of the transfer matrix  $P^{N}$  associated with eigenvalue 1. The distribution  $\\mathbf{\\rho}^N$  approximates the invariant density of the system subject to  binning , and can be taken as a probability distribution over the partition elements. In practice, the invariant measure  $\\mathbf{\\rho}^N$  is computed using  invariantmeasure , which also approximates the transfer matrix. The invariant distribution is initialized as a length- N  random distribution which is then applied to  $P^{N}$ . The resulting length- N  distribution is then applied to  $P^{N}$  again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold. See also:  RectangularBinning ,  invariantmeasure . source"},{"id":692,"pagetitle":"Probabilities","title":"Utility methods/types","ref":"/complexitymeasures/stable/probabilities/#Utility-methods/types","content":" Utility methods/types"},{"id":693,"pagetitle":"Probabilities","title":"ComplexityMeasures.InvariantMeasure","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.InvariantMeasure","content":" ComplexityMeasures.InvariantMeasure  —  Type InvariantMeasure(to, ρ) Minimal return struct for  invariantmeasure  that contains the estimated invariant measure  ρ , as well as the transfer operator  to  from which it is computed (including bin information). See also:  invariantmeasure . source"},{"id":694,"pagetitle":"Probabilities","title":"ComplexityMeasures.invariantmeasure","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.invariantmeasure","content":" ComplexityMeasures.invariantmeasure  —  Function invariantmeasure(x::AbstractStateSpaceSet, binning::RectangularBinning) → iv::InvariantMeasure Estimate an invariant measure over the points in  x  based on binning the data into rectangular boxes dictated by the  binning , then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential. Details on the estimation procedure is found the  TransferOperator  docstring. Example using DynamicalSystems\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\norbit, t = trajectory(ds, 20_000; Ttr = 10)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv) Probabilities and bin information invariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector}) From a pre-computed invariant measure, return the probabilities and associated bins. The element  ρ[i]  is the probability of visitation to the box  bins[i] . Analogous to  binhist . Transfer operator approach vs. naive histogram approach Why bother with the transfer operator instead of using regular histograms to obtain probabilities? In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as  $n \\to \\intfy$ ), which is guaranteed by the ergodic theorem. There is a crucial difference, however: The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the  transition probabilities  between states (see  transfermatrix ). See also:  InvariantMeasure . source"},{"id":695,"pagetitle":"Probabilities","title":"ComplexityMeasures.transfermatrix","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.transfermatrix","content":" ComplexityMeasures.transfermatrix  —  Function transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector}) Return the transfer matrix/operator and corresponding bins. Here,  bins[i]  corresponds to the i-th row/column of the transfer matrix. Thus, the entry  M[i, j]  is the probability of jumping from the state defined by  bins[i]  to the state defined by  bins[j] . See also:  TransferOperator . source"},{"id":696,"pagetitle":"Probabilities","title":"Kernel density","ref":"/complexitymeasures/stable/probabilities/#Kernel-density","content":" Kernel density"},{"id":697,"pagetitle":"Probabilities","title":"ComplexityMeasures.NaiveKernel","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.NaiveKernel","content":" ComplexityMeasures.NaiveKernel  —  Type NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator Estimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995)  [PrichardTheiler1995] . Probabilities  $P(\\mathbf{x}, \\epsilon)$  are assigned to every point  $\\mathbf{x}$  by counting how many other points occupy the space spanned by a hypersphere of radius  ϵ  around  $\\mathbf{x}$ , according to: \\[P_i( X, \\epsilon) \\approx \\dfrac{1}{N} \\sum_{s} B(||X_i - X_j|| < \\epsilon),\\] where  $B$  gives 1 if the argument is  true . Probabilities are then normalized. Keyword arguments method = KDTree : the search structure supported by Neighborhood.jl. Specifically, use  KDTree  to use a tree-based neighbor search, or  BruteForce  for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length. w = 0 : the Theiler window, which excludes indices  $s$  that are within  $|i - s| ≤ w$  from the given point  $x_i$ . metric = Euclidean() : the distance metric. Outcome space The outcome space  Ω  for  NaiveKernel  are the indices of the input data,  eachindex(x) . Hence, input  x  is needed for a well-defined  outcome_space . The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors). source"},{"id":698,"pagetitle":"Probabilities","title":"Timescales","ref":"/complexitymeasures/stable/probabilities/#Timescales","content":" Timescales"},{"id":699,"pagetitle":"Probabilities","title":"ComplexityMeasures.WaveletOverlap","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.WaveletOverlap","content":" ComplexityMeasures.WaveletOverlap  —  Type WaveletOverlap([wavelet]) <: ProbabilitiesEstimator Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001) [Rosso2001] . Input timeseries  x  is needed for a well-defined outcome space. By default the wavelet  Wavelets.WT.Daubechies{12}()  is used. Otherwise, you may choose a wavelet from the  Wavelets  package (it must subtype  OrthoWaveletClass ). Outcome space The outcome space for  WaveletOverlap  are the integers  1, 2, …, N  enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can  view online . As such, this estimator only works for timeseries input and input  x  is needed for a well-defined  outcome_space . source"},{"id":700,"pagetitle":"Probabilities","title":"ComplexityMeasures.PowerSpectrum","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.PowerSpectrum","content":" ComplexityMeasures.PowerSpectrum  —  Type PowerSpectrum() <: ProbabilitiesEstimator Calculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as  spectral entropy , e.g.  [Llanos2016] , [Tian2017] . The closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input. Outcome space The outcome space  Ω  for  PowerSpectrum  is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be  1 . Input  x  is needed for a well-defined  outcome_space . source"},{"id":701,"pagetitle":"Probabilities","title":"Diversity","ref":"/complexitymeasures/stable/probabilities/#Diversity","content":" Diversity"},{"id":702,"pagetitle":"Probabilities","title":"ComplexityMeasures.Diversity","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.Diversity","content":" ComplexityMeasures.Diversity  —  Type Diversity(; m::Int, τ::Int, nbins::Int) A  ProbabilitiesEstimator  based on the cosine similarity. It can be used with  entropy  to compute the diversity entropy of an input timeseries [Wang2020] . The implementation here allows for  τ != 1 , which was not considered in the original paper. Description Diversity probabilities are computed as follows. From the input time series  x , using embedding lag  τ  and embedding dimension  m ,  construct the embedding   $Y = \\{\\bf x_i \\} = \\{(x_{i}, x_{i+\\tau}, x_{i+2\\tau}, \\ldots, x_{i+m\\tau - 1}\\}_{i = 1}^{N-mτ}$ . Compute  $D = \\{d(\\bf x_t, \\bf x_{t+1}) \\}_{t=1}^{N-mτ-1}$ ,  where  $d(\\cdot, \\cdot)$  is the cosine similarity between two  m -dimensional  vectors in the embedding. Divide the interval  [-1, 1]  into  nbins  equally sized subintervals (including the value  +1 ). Construct a histogram of cosine similarities  $d \\in D$  over those subintervals. Sum-normalize the histogram to obtain probabilities. Outcome space The outcome space for  Diversity  is the bins of the  [-1, 1]  interval, and the return configuration is the same as in  ValueHistogram  (left bin edge). source"},{"id":703,"pagetitle":"Probabilities","title":"Spatial estimators","ref":"/complexitymeasures/stable/probabilities/#Spatial-estimators","content":" Spatial estimators"},{"id":704,"pagetitle":"Probabilities","title":"ComplexityMeasures.SpatialSymbolicPermutation","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.SpatialSymbolicPermutation","content":" ComplexityMeasures.SpatialSymbolicPermutation  —  Type SpatialSymbolicPermutation <: ProbabilitiesEstimator\nSpatialSymbolicPermutation(stencil, x; periodic = true) A symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises  SymbolicPermutation  to high-dimensional arrays. The order  m  of the permutation pattern is extracted from the  stencil , see below. SpatialSymbolicPermutation  is based on the 2D and 3D  spatiotemporal permutation entropy  estimators by by Ribeiro et al. (2012) [Ribeiro2012]  and Schlemmer et al. (2018) [Schlemmer2018] ), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for  D -dimensional input array  x , with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions. See below for ways to specify the  stencil . If  periodic = true , then the stencil wraps around at the ends of the array. If  false , then collected regions with indices which exceed the array bounds are skipped. In combination with  entropy  and  entropy_normalized , this probabilities estimator can be used to compute generalized spatiotemporal permutation  EntropyDefinition  of any type. Outcome space The outcome space  Ω  for  SpatialSymbolicPermutation  is the set of length- m  ordinal patterns (i.e. permutations) that can be formed by the integers  1, 2, …, m , ordered lexicographically. There are  factorial(m)  such patterns. Here  m  refers to the number of points included in  stencil . Stencils The  stencil  defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order- m  permutation pattern, which is then mapped to an integer as in  SymbolicPermutation . The  stencil  is moved around the input array, in a sense \"scanning\" the input array, to collect all possible groupings allowed by the boundary condition (periodic or not). Stencils are passed in one of the following three ways: As vectors of  CartesianIndex  which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example  stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]) .  Don't forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of  CartesianIndex ,  m = length(stencil) . As a  D -dimensional array (where  D  matches the dimensionality of the input data)  containing  0 s and  1 s, where if  stencil[index] == 1 , the corresponding pixel is  included, and if  stencil[index] == 0 , it is not included.  To generate the same estimator as in 1., use  stencil = [1 1; 1 1] .  When passing a stencil as a  D -dimensional array,  m = sum(stencil) As a  Tuple  containing two  Tuple s, both of length  D , for  D -dimensional data.  The first tuple specifies the  extent  of the stencil, where  extent[i]   dictates the number of hypervoxels to be included along the  i th axis and  lag[i]   the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here  stencil = ((2, 2), (1, 1)) .  When passing a stencil using  extent  and  lag ,  m = prod(extent) . source"},{"id":705,"pagetitle":"Probabilities","title":"ComplexityMeasures.SpatialDispersion","ref":"/complexitymeasures/stable/probabilities/#ComplexityMeasures.SpatialDispersion","content":" ComplexityMeasures.SpatialDispersion  —  Type SpatialDispersion <: ProbabilitiesEstimator\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n) A dispersion-based probabilities estimator that generalises  Dispersion  for input data that are high-dimensional arrays. SpatialDispersion  is based on Azami et al. (2019) [Azami2019] 's 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for  N -dimensional input data  x , with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions. In combination with  entropy  and  entropy_normalized , this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion  EntropyDefinition  of any type. Arguments stencil . Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see    SpatialSymbolicPermutation . See  SpatialSymbolicPermutation  for   more information about stencils. x::AbstractArray . The input data. Must be provided because we need to know its size   for optimization and bound checking. Keyword arguments periodic::Bool . If  periodic == true , then the stencil should wrap around at the   end of the array. If  periodic = false , then pixels whose stencil exceeds the array   bounds are skipped. c::Int . Determines how many discrete categories to use for the Gaussian encoding. skip_encoding . If  skip_encoding == true ,  encoding  is ignored, and dispersion   patterns are computed directly from  x , under the assumption that  L  is the alphabet   length for  x  (useful for categorical or integer data). Thus, if    skip_encoding == true , then  L  must also be specified. This is useful for   categorical or integer-valued data. L . If  L == nothing  (default), then the number of total outcomes is inferred from    stencil  and  encoding . If  L  is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to  L . Outcome space The outcome space for  SpatialDispersion  is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all  m -dimensional delay vectors whose elements are all possible values in  1:c . There are  c^m  such vectors. Description Estimating probabilities/entropies from higher-dimensional data is conceptually simple. Discretize each value (hypervoxel) in  x  relative to all other values  xᵢ ∈ x  using the  provided  encoding  scheme. Use  stencil  to extract relevant (discretized) points around each hypervoxel. Construct a symbol these points. Take the sum-normalized histogram of the symbol as a probability distribution. Optionally, compute  entropy  or  entropy_normalized  from this  probability distribution. Usage Here's how to compute spatial dispersion entropy using the three different ways of specifying stencils. x = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\nentropy_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\nentropy_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\nentropy_normalized(est, x) To apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.: imgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = entropy_normalized.(Ref(est), imgs) Computing generalized spatiotemporal dispersion entropy is trivial, e.g. with  Renyi : x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\nentropy(Renyi(q = 2), est, x) See also:  SpatialSymbolicPermutation ,  GaussianCDFEncoding ,  symbolize . source BandtPompe2002 Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for timeseries.\" Physical review letters 88.17 (2002): 174102. Zunino2017 Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892. He2016 He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823. Fadlallah2013 Fadlallah, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911. Azami2016 Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51. Rostaghi2016 Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614. Li2018 Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11. Diego2019 Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212. PrichardTheiler1995 Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493. Rosso2001 Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75. Llanos2016 Llanos et al.,  Power spectral entropy as an information-theoretic correlate of manner of articulation in American English ,  The Journal of the Acoustical Society of America 141, EL127 (2017) Tian2017 Tian et al,  Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task ,  Front. Hum. Neurosci. Wang2020 Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429. Ribeiro2012 Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689 Schlemmer2018 Schlemmer et al. (2018). Spatiotemporal Permutation EntropyDefinition as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039 Azami2019 Azami, H., da Silva, L. E. V., Omoto, A. C. M., & Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187."},{"id":710,"pagetitle":"Documentation","title":"TimeseriesSurrogates.jl","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.jl","content":" TimeseriesSurrogates.jl TimeseriesSurrogates  is a Julia package for generating surrogate timeseries. It is part of  JuliaDynamics , a GitHub organization dedicated to creating high quality scientific software. If you are new to this method of surrogate timeseries, feel free to read the  Crash-course into timeseries surrogate tests  page. Please note that timeseries surrogates should not be confused with  surrogate models , such as those provided by  Surrogates.jl ."},{"id":711,"pagetitle":"Documentation","title":"Installation","ref":"/timeseriessurrogates/stable/#Installation","content":" Installation TimeseriesSurrogates.jl is a registered Julia package. To install the latest version, run the following code: import Pkg; Pkg.add(\"TimeseriesSurrogates\")"},{"id":712,"pagetitle":"Documentation","title":"API","ref":"/timeseriessurrogates/stable/#API","content":" API TimeseriesSurrogates.jl API is composed by four names:  surrogate ,  surrogenerator ,  SurrogateTest , and  pvalue . They dispatch on the method to generate surrogates, which is a subtype of  Surrogate . It is recommended to standardize the signal before using these functions, i.e. subtract mean and divide by standard deviation. The function  standardize  does this."},{"id":713,"pagetitle":"Documentation","title":"Generating surrogates","ref":"/timeseriessurrogates/stable/#Generating-surrogates","content":" Generating surrogates"},{"id":714,"pagetitle":"Documentation","title":"TimeseriesSurrogates.surrogate","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.surrogate","content":" TimeseriesSurrogates.surrogate  —  Function surrogate(x, method::Surrogate [, rng]) → s Create a single surrogate timeseries  s  from  x  based on the given  method . If you want to generate multiple surrogates from  x , you should use  surrogenerator  for better performance. source"},{"id":715,"pagetitle":"Documentation","title":"TimeseriesSurrogates.surrogenerator","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.surrogenerator","content":" TimeseriesSurrogates.surrogenerator  —  Function surrogenerator(x, method::Surrogate [, rng]) → sgen::SurrogateGenerator Initialize a generator that creates surrogates of  x  on demand, based on the given  method . This is more efficient than  surrogate , because for most methods some things can be initialized and reused for every surrogate. Optionally you can provide an  rng::AbstractRNG  object that will control the random number generation and hence establish reproducibility of the generated surrogates. By default  Random.default_rng()  is used. The generated surrogates overwrite, in-place, a common vector container. Use  copy  if you need to actually store multiple surrogates. To generate a surrogate, call  sgen  as a function with no arguments, e.g.: sgen = surrogenerator(x, method)\ns = sgen() You can use the generator syntax of Julia to map over surrogates generated by  sg . For example, let  q  be a function returning a discriminatory statistic. To test some null hypothesis with TimeseriesSurrogates.jl you'd do using TimeseriesSurrogates\nq, x # inputs\nmethod = RandomFourier() # some example method\nsgen = surrogenerator(x, method)\nsiter = (sgen() for _ in 1:1000)\nqx = q(x)\nqs = map(q, siter)\n# compare `qx` with quantiles\nusing Statistics: quantile\nq01, q99 = quantile(qs, [0.01, 0.99])\nq01 ≤ qx ≤ q99 # if false, hypothesis can be rejected! source"},{"id":716,"pagetitle":"Documentation","title":"Hypothesis testing","ref":"/timeseriessurrogates/stable/#Hypothesis-testing","content":" Hypothesis testing"},{"id":717,"pagetitle":"Documentation","title":"TimeseriesSurrogates.SurrogateTest","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.SurrogateTest","content":" TimeseriesSurrogates.SurrogateTest  —  Type SurrogateTest(f::Function, x, method::Surrogate; kwargs...) → test Initialize a surrogate test for input data  x , which can be used in  pvalue . The tests requires as input a function  f  that given a timeseries (like  x ) it outputs a real number, and a method of how to generate surrogates.  f  is the function that computes the discriminatory statistic. Once called with  pvalue , the  test  stores the real value  rval  and surrogate values  vals  of the discriminatory statistic in the fields  rval, vals  respectively. SurrogateTest  automates the process described in the documentation page  Performing surrogate hypothesis tests . SurrogateTest  subtypes  HypothesisTest  and is part of the StatsAPI.jl interface. Keywords rng = Random.default_rng() : a random number generator. n::Int = 10_000 : how many surrogates to generate and compute  f  on. threaded = true : Whether to parallelize looping over surrogate computations in  pvalue  to the available threads ( Threads.nthreads() ). source"},{"id":718,"pagetitle":"Documentation","title":"StatsAPI.pvalue","ref":"/timeseriessurrogates/stable/#StatsAPI.pvalue-Tuple{SurrogateTest}","content":" StatsAPI.pvalue  —  Method pvalue(test::SurrogateTest; tail = :left) Return the  p-value  corresponding to the given  SurrogateTest , optionally specifying what kind of tail test to do (one of  :left, :right, :both ). For  SurrogateTest , the p-value is simply the proportion of surrogate statistics that exceed (for  tail = :right ) or subseed ( tail = :left ) the discriminatory statistic computed from the input data. The default value of  tail  assumes that the surrogate data are expected to have higher discriminatory statistic values. This is the case for statistics that quantify entropy. For statistics that quantify autocorrelation, use  tail = :right  instead. source"},{"id":719,"pagetitle":"Documentation","title":"Surrogate methods","ref":"/timeseriessurrogates/stable/#Surrogate-methods","content":" Surrogate methods"},{"id":720,"pagetitle":"Documentation","title":"TimeseriesSurrogates.Surrogate","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.Surrogate","content":" TimeseriesSurrogates.Surrogate  —  Type Supertype of all surrogate methods. source TimeseriesSurrogates.AAFT TimeseriesSurrogates.BlockShuffle TimeseriesSurrogates.CircShift TimeseriesSurrogates.CycleShuffle TimeseriesSurrogates.IAAFT TimeseriesSurrogates.PartialRandomization TimeseriesSurrogates.PartialRandomizationAAFT TimeseriesSurrogates.PseudoPeriodic TimeseriesSurrogates.PseudoPeriodicTwin TimeseriesSurrogates.RandomCascade TimeseriesSurrogates.RandomFourier TimeseriesSurrogates.RandomShuffle TimeseriesSurrogates.Surrogate TimeseriesSurrogates.SurrogateTest TimeseriesSurrogates.TAAFT TimeseriesSurrogates.TFTD TimeseriesSurrogates.TFTDAAFT TimeseriesSurrogates.TFTDIAAFT TimeseriesSurrogates.TFTDRandomFourier TimeseriesSurrogates.TFTS TimeseriesSurrogates.WLS"},{"id":721,"pagetitle":"Documentation","title":"Shuffle-based","ref":"/timeseriessurrogates/stable/#Shuffle-based","content":" Shuffle-based"},{"id":722,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomShuffle","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomShuffle","content":" TimeseriesSurrogates.RandomShuffle  —  Type RandomShuffle() <: Surrogate A random constrained surrogate, generated by shifting values around. Random shuffle surrogates preserve the mean, variance and amplitude  distribution of the original signal. Properties not preserved are  any  temporal information , such as the power spectrum and hence linear  correlations.  The null hypothesis this method can test for is whether the data  are uncorrelated noise, possibly measured via a nonlinear function. Specifically, random shuffle surrogate can test  the null hypothesis that the original signal is produced by independent and  identically distributed random variables[^Theiler1991, ^Lancaster2018].  Beware: random shuffle surrogates do not cover the case of correlated noise [Lancaster2018] .  source"},{"id":723,"pagetitle":"Documentation","title":"TimeseriesSurrogates.BlockShuffle","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.BlockShuffle","content":" TimeseriesSurrogates.BlockShuffle  —  Type BlockShuffle(n::Int; shift = false) A block shuffle surrogate constructed by dividing the time series into  n  blocks of roughly equal width at random indices (end blocks are wrapped around to the start of the time series). If  shift  is  true , then the input signal is circularly shifted by a  random number of steps prior to picking blocks. Block shuffle surrogates roughly preserve short-range temporal properties in the time series (e.g. correlations at lags less than the block length), but break any long-term dynamical information (e.g. correlations beyond the block length). Hence, these surrogates can be used to test any null hypothesis aimed at comparing short-range dynamical properties versus long-range dynamical properties of the signal. source"},{"id":724,"pagetitle":"Documentation","title":"TimeseriesSurrogates.CycleShuffle","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.CycleShuffle","content":" TimeseriesSurrogates.CycleShuffle  —  Type CycleShuffle(n::Int = 7, σ = 0.5) Cycle shuffled surrogates [Theiler1994]  that identify successive local peaks in the data and shuffle the cycles in-between the peaks. Similar to  BlockShuffle , but here the \"blocks\" are defined as follows: The timeseries is smoothened via convolution with a Gaussian ( DSP.gaussian(n, σ) ). Local maxima of the smoothened signal define the peaks, and thus the blocks in between them. The first and last index of timeseries can never be peaks and thus signals that should have peaks very close to start or end of the timeseries may not perform well. In addition, points before the first or after the last peak are never shuffled. The defined blocks are randomly shuffled as in  BlockShuffle . CSS are used to test the null hypothesis that the signal is generated by a periodic oscillator with no dynamical correlation between cycles, i.e. the evolution of cycles is not deterministic. See also  PseudoPeriodic . source"},{"id":725,"pagetitle":"Documentation","title":"TimeseriesSurrogates.CircShift","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.CircShift","content":" TimeseriesSurrogates.CircShift  —  Type CircShift(n) Surrogates that are circularly shifted versions of the original timeseries. n  can be an integer (the surrogate is the original time series shifted  by  n  indices), or any vector of integers, which which means that each  surrogate is shifted by an integer selected randomly among the entries in  n . source"},{"id":726,"pagetitle":"Documentation","title":"Fourier-based","ref":"/timeseriessurrogates/stable/#Fourier-based","content":" Fourier-based"},{"id":727,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomFourier","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomFourier","content":" TimeseriesSurrogates.RandomFourier  —  Type RandomFourier(phases = true) A surrogate that randomizes the Fourier components of the signal in some manner. If  phases==true , the phases are randomized, otherwise the amplitudes are randomized.  FT  is an alias for  RandomFourier . Random Fourier phase surrogates [Theiler1991]  preserve the autocorrelation function, or power spectrum, of the original signal. Random Fourier amplitude surrogates preserve the mean and autocorrelation function but do not preserve the variance of the original. Random amplitude surrogates are not common in the literature, but are provided for convenience. Random phase surrogates can be used to test the null hypothesis that the original signal was produced by a linear Gaussian process  [Theiler1991] . source"},{"id":728,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDRandomFourier","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDRandomFourier","content":" TimeseriesSurrogates.TFTDRandomFourier  —  Type TFTD(phases::Bool = true, fϵ = 0.05) The  TFTDRandomFourier  (or just  TFTD  for short) surrogate was proposed by Lucio et al. (2012) [Lucio2012]  as a combination of truncated Fourier surrogates [Nakamura2006]  ( TFTS ) and detrend-retrend surrogates. The  TFTD  part of the name comes from the fact that it uses a combination of truncated Fourier transforms (TFT) and de-trending and re-trending (D) the time series before and after surrogate generation. Hence, it can be used to generate surrogates also from (strongly) nonstationary time series. Implementation details Here, a best-fit linear trend is removed/added from the signal prior to and after generating the random Fourier signal. In principle, any trend can be removed, but so far, we only provide the linear option. See also:  TFTDAAFT ,  TFTDIAAFT . source"},{"id":729,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PartialRandomization","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.PartialRandomization","content":" TimeseriesSurrogates.PartialRandomization  —  Type PartialRandomization(α = 0.5) PartialRandomization  surrogates [Ortega1998]  are similar to  RandomFourier  phase  surrogates, but during the phase randomization step, instead of drawing phases from  [0, 2π] , phases are drawn from  [0, 2π]*α , where  α ∈ [0, 1] . The authors refers to  α  as the  \"degree\" of phase randomization, where  α = 0  means  0 %  randomization and   α = 1  means  100 %  randomization. source"},{"id":730,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PartialRandomizationAAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.PartialRandomizationAAFT","content":" TimeseriesSurrogates.PartialRandomizationAAFT  —  Type PartialRandomizationAAFT(α = 0.5) PartialRandomizationAAFF  surrogates are similar to  PartialRandomization   surrogates [Ortega1998] , but adds a rescaling step, so that the surrogate has  the same values as the original time series (analogous to the rescaling done for  AAFT  surrogates). Partial randomization surrogates have, to the package authors' knowledge, not been  published in scientific literature. source"},{"id":731,"pagetitle":"Documentation","title":"TimeseriesSurrogates.AAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.AAFT","content":" TimeseriesSurrogates.AAFT  —  Type AAFT() An amplitude-adjusted-fourier-transform (AAFT) surrogate [Theiler1991] . AAFT surrogates have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data. AAFT surrogates can be used to test the null hypothesis that the data come from a monotonic nonlinear transformation of a linear Gaussian process (also called integrated white noise) [Theiler1991] . source"},{"id":732,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TAAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TAAFT","content":" TimeseriesSurrogates.TAAFT  —  Type TAAFT(fϵ) An truncated version of the amplitude-adjusted-fourier-transform surrogate [Theiler1991] [Nakamura2006] . The truncation parameter and phase randomization procedure is identical to  TFTS , but here an additional step of rescaling back to the original data is performed. This preserves the amplitude distribution of the original data. source"},{"id":733,"pagetitle":"Documentation","title":"TimeseriesSurrogates.IAAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.IAAFT","content":" TimeseriesSurrogates.IAAFT  —  Type IAAFT(M = 100, tol = 1e-6, W = 75) An iteratively adjusted amplitude-adjusted-fourier-transform surrogate [SchreiberSchmitz1996] . IAAFT surrogates have the same linear correlation, or periodogram, and also preserves the amplitude distribution of the original data, but are improved relative to AAFT through iterative adjustment (which runs for a maximum of  M  steps). During the iterative adjustment, the periodograms of the original signal and the surrogate are coarse-grained and the powers are averaged over  W  equal-width frequency bins. The iteration procedure ends when the relative deviation between the periodograms is less than  tol  (or when  M  is reached). IAAFT, just as AAFT, can be used to test the null hypothesis that the data come from a monotonic nonlinear transformation of a linear Gaussian process. source"},{"id":734,"pagetitle":"Documentation","title":"Non-stationary","ref":"/timeseriessurrogates/stable/#Non-stationary","content":" Non-stationary"},{"id":735,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTS","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTS","content":" TimeseriesSurrogates.TFTS  —  Type TFTS(fϵ::Real) A truncated Fourier transform surrogate [Nakamura2006]  (TFTS). TFTS surrogates are generated by leaving some frequencies untouched when performing the phase shuffling step (as opposed to randomizing all frequencies, like for  RandomFourier  surrogates). These surrogates were designed to deal with data with irregular fluctuations superimposed over long term trends (by preserving low frequencies) [Nakamura2006] . Hence, TFTS surrogates can be used to test the null hypothesis that the signal is a stationary linear system generated the irregular fluctuations part of the signal [Nakamura2006] . Controlling the truncation of the spectrum The truncation parameter  fϵ ∈ [-1, 0) ∪ (0, 1]  controls which parts of the spectrum are preserved. If  fϵ > 0 , then  fϵ  indicates the ratio of high frequency domain to the entire frequency domain.   For example,  fϵ = 0.5  preserves 50% of the frequency domain (randomizing the higher   frequencies, leaving low frequencies intact). If  fϵ < 0 , then  fϵ  indicates ratio of low frequency domain to the entire frequency domain.   For example,  fϵ = -0.2  preserves 20% of the frequency domain (leaving higher frequencies intact,   randomizing the lower frequencies). If  fϵ ± 1 , then all frequencies are randomized. The method is then equivalent to    RandomFourier . The appropriate value of  fϵ  strongly depends on the data and time series length, and must be manually determined [Nakamura2006] , for example by comparing periodograms for the time series and the surrogates. source"},{"id":736,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTD","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTD","content":" TimeseriesSurrogates.TFTD  —  Type TFTD(phases::Bool = true, fϵ = 0.05) The  TFTDRandomFourier  (or just  TFTD  for short) surrogate was proposed by Lucio et al. (2012) [Lucio2012]  as a combination of truncated Fourier surrogates [Nakamura2006]  ( TFTS ) and detrend-retrend surrogates. The  TFTD  part of the name comes from the fact that it uses a combination of truncated Fourier transforms (TFT) and de-trending and re-trending (D) the time series before and after surrogate generation. Hence, it can be used to generate surrogates also from (strongly) nonstationary time series. Implementation details Here, a best-fit linear trend is removed/added from the signal prior to and after generating the random Fourier signal. In principle, any trend can be removed, but so far, we only provide the linear option. See also:  TFTDAAFT ,  TFTDIAAFT . source"},{"id":737,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDAAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDAAFT","content":" TimeseriesSurrogates.TFTDAAFT  —  Type TFTDAAFT(fϵ = 0.05) TFTDAAFT [Lucio2012]  are similar to  TFTD  surrogates, but also re-scales back to the original values of the time series.  fϵ ∈ (0, 1]  is the fraction of the powerspectrum corresponding to the lowermost frequencies to be preserved. See also:  TFTD ,  TFTDIAAFT . source"},{"id":738,"pagetitle":"Documentation","title":"TimeseriesSurrogates.TFTDIAAFT","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.TFTDIAAFT","content":" TimeseriesSurrogates.TFTDIAAFT  —  Type TFTDIAAFT(fϵ = 0.05; M::Int = 100, tol::Real = 1e-6, W::Int = 75) TFTDIAAFT [Lucio2012]  are similar to  TFTDAAFT , but adds an iterative procedure to better match the periodograms of the surrogate and the original time series, analogously to how  IAAFT  improves upon  AAFT . fϵ ∈ (0, 1]  is the fraction of the powerspectrum corresponding to the lowermost frequencies to be preserved.  M  is the maximum number of iterations.  tol  is the desired maximum relative tolerance between power spectra.  W  is the number of bins into which the periodograms are binned when comparing across iterations. See also:  TFTD ,  TFTDAAFT . source"},{"id":739,"pagetitle":"Documentation","title":"Pseudo-periodic","ref":"/timeseriessurrogates/stable/#Pseudo-periodic","content":" Pseudo-periodic"},{"id":740,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PseudoPeriodic","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.PseudoPeriodic","content":" TimeseriesSurrogates.PseudoPeriodic  —  Type PseudoPeriodic(d, τ, ρ, shift = true) Create surrogates suitable for pseudo-periodic signals. They retain the periodic structure of the signal, while inter-cycle dynamics that are either deterministic or correlated noise are destroyed (for appropriate  ρ  choice). Therefore these surrogates are suitable to test the null hypothesis that the signal is a periodic orbit with uncorrelated noise [Small2001] . Arguments  d, τ, ρ  are as in the paper, the embedding dimension, delay time and noise radius. The method works by performing a delay coordinates embedding from DelayEmbeddings.jl (see that docs for choosing appropriate  d, τ ). For  ρ , we have implemented the method proposed in the paper in the function  noiseradius . The argument  shift  is not discussed in the paper. If  shift=false  we adjust the algorithm so that there is little phase shift between the periodic component of the original and surrogate data. See also  CycleShuffle . source"},{"id":741,"pagetitle":"Documentation","title":"TimeseriesSurrogates.PseudoPeriodicTwin","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.PseudoPeriodicTwin","content":" TimeseriesSurrogates.PseudoPeriodicTwin  —  Type PseudoPeriodicTwin(d::Int, τ::Int, δ = 0.2, ρ = 0.1, metric = Euclidean())\nPseudoPeriodicTwin(δ = 0.2, ρ = 0.1, metric = Euclidean()) A pseudoperiodic twin surrogate [Miralles2015] , which is a fusion of the twin surrogate [Thiel2006]  and the pseudo-periodic surrogate [Small2001] . Input parameters A delay reconstruction of the input timeseries is constructed using embedding dimension  d  and embedding delay  τ . The threshold  δ ∈ (0, 1]  determines which points are \"close\" (neighbors) or not, and is expressed as a fraction of the attractor diameter, as determined by the input data. The authors of the original twin surrogate paper recommend  0.05 ≤ δ ≤ 0.2 [Thiel2006] . If you have pre-embedded your timeseries, and timeseries is already a  ::StateSpaceSet , use the three-argument constructor (so that no delay reconstruction is performed). If you want a surrogate for a scalar-valued timeseries, use the five-argument constructor to also provide the embedding delay  τ  and embedding dimension  d . Null hypothesis Pseudo-periodic twin surrogates generate signals similar to the original data if the original signal is (quasi-)periodic. If the original signal is not (quasi-)periodic, then these surrogates will have different recurrence plots than the original signal, but preserve the overall shape of the attractor. Thus,  PseudoPeriodicTwin  surrogates can be used to test null hypothesis that the observed timeseries (or orbit) is consistent with a quasi-periodic orbit [Miralles2015] . Returns A  d -dimensional surrogate orbit (a  StateSpaceSet ) is returned. Sample the first column of this dataset if a scalar-valued surrogate is desired. source"},{"id":742,"pagetitle":"Documentation","title":"Wavelet-based","ref":"/timeseriessurrogates/stable/#Wavelet-based","content":" Wavelet-based"},{"id":743,"pagetitle":"Documentation","title":"TimeseriesSurrogates.WLS","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.WLS","content":" TimeseriesSurrogates.WLS  —  Type WLS(shufflemethod::Surrogate = IAAFT();\n    f::Union{Nothing, Function} = Statistics.cor,\n    rescale::Bool = true,\n    wt::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{16}()) A wavelet surrogate generated by the following procedure: Compute the wavelet transform of the signal. This results in a set of   detail coefficients over a set of dyadic scales. As in Keylock (2006),   we here use the maximal overlap discrete wavelet transform, or MODWT,  so that the number of coefficients at each scale are the same. Shuffle the detail coefficients at each dyadic scale using the   provided  shufflemethod . See \"Shuffling methods\" below for alternatives. Apply the inverse wavelet transform to the shuffled detail coefficients   to obtain a surrogate time series. Shuffling methods You may choose to use any surrogate from this package to perform the  randomization of the detail coefficients at each dyadic scale. The following methods have been discussed in the literature (more may exist):  Random permutations of wavelet coefficients within each scale (Breakspear et al., 2003). To get this behaviour, use  WLS(x, RandomShuffle(), rescale = false, f = nothing) . Cyclic rotation of wavelet coefficients within each scale (Breakspear et al., 2003).  To get this behaviour, use  WLS(x, Circshift(1:length(x)), rescale = false, f = nothing) . Block resampling of wavelet coefficients within each scale (Breakspear et al., 2003). To get this behaviour, use  WLS(x, BlockShuffle(nblocks, randomize = true), rescale = false, f = nothing) . IAAFT resampling of wavelet coefficients within each scale (Keylock, 2006). To get this behaviour, use  WLS(x, IAAFT(), rescale = true, f = Statistics.cor) .   This method preserves the local mean and variance structure of the signal, but    randomises nonlinear properties of the signal (i.e. Hurst exponents) [Keylock2006] .    These surrogates can therefore be used to test for changes in nonlinear properties    of the original signal. In contrast to IAAFT surrogates, the IAAFT-wavelet surrogates    also preserves nonstationarity. Using other  shufflemethod s does not necessarily   preserve nonstationarity. To deal with nonstationary signals, Keylock (2006) recommends    using a wavelet with a high number of vanishing moments. Thus, our default is to   use a Daubechies wavelet with 16 vanishing moments.  Note: The iterative procedure after    the rank ordering step (step [v] in  [Keylock2006] ) is not performed in    this implementation. The default method and parameters replicate the behaviour of Keylock (2006)'s IAAFT  wavelet surrogates. Error minimization For the  IAAFT  approach introduced in Keylock (2006), detail coefficients  at each level are circularly rotated to minimize an error function. The methods  introduced in Breakspear et al. (2003) do not apply this error minimization. In our implementation, you can turn this option on/off using the  f  parameter of  the  WLS  constructor. If  f = nothing  turns off error minization. If  f  is set  to a two-argument function that computes some statistic, for example   f = Statistics.cor , then detail coefficients at each scale are circularly  rotated until that function is maximized (and hence the \"error\" minimized).  If you want to  minimize  some error function, then instead provide an appropriate  transform of your function. For example, if using the root mean squared deviation, define  rmsd_inv(x, y) = 1 - StatsBase.rmsd(x, y)  and set  f = rmsd_inv . Rescaling If  rescale == true , then surrogate values are mapped onto the  values of the original time series, as in the  AAFT  algorithm. If  rescale == false , surrogate values are not constrained to the  original time series values. If  AAFT  or  IAAFT  shuffling  is used,  rescale  should be set to  true . For other methods, it does not  necessarily need to be. source"},{"id":744,"pagetitle":"Documentation","title":"TimeseriesSurrogates.RandomCascade","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.RandomCascade","content":" TimeseriesSurrogates.RandomCascade  —  Type RandomCascade(paddingmode::String = \"zeros\") A random cascade multifractal wavelet surrogate (Paluš, 2008) [Paluš2008] . If the input signal length is not a power of 2, the signal must be  padded before the surrogate is constructed.  paddingmode  determines  how the signal is padded. Currently supported padding modes:  \"zeros\" . The final surrogate (constructed from the padded signal) is subset to match the length of the original signal. Random cascade surrogate preserve multifractal properties of the input  time series, that is, interactions among dyadic scales and nonlinear  dependencies [Paluš2008] . source"},{"id":745,"pagetitle":"Documentation","title":"Other","ref":"/timeseriessurrogates/stable/#Other","content":" Other AutoRegressive\nShuffleDimensions\nIrregularLombScargle"},{"id":746,"pagetitle":"Documentation","title":"Utilities","ref":"/timeseriessurrogates/stable/#Utilities","content":" Utilities"},{"id":747,"pagetitle":"Documentation","title":"TimeseriesSurrogates.noiseradius","ref":"/timeseriessurrogates/stable/#TimeseriesSurrogates.noiseradius","content":" TimeseriesSurrogates.noiseradius  —  Function noiseradius(x::AbstractVector, d::Int, τ, ρs, n = 1) → ρ Use the proposed* algorithm of [Small2001]  to estimate optimal  ρ  value for  PseudoPeriodic  surrogates, where  ρs  is a vector of possible  ρ  values. *The paper is ambiguous about exactly what to calculate. Here we count how many times we have pairs of length-2 that are identical in  x  and its surrogate, but  are not  also part of pairs of length-3. This function directly returns the arg-maximum of the evaluated distribution of these counts versus  ρ , use  TimeseriesSurrogates._noiseradius  with same arguments to get the actual distribution.  n  means to repeat τhe evaluation  n  times, which increases accuracy. source"},{"id":748,"pagetitle":"Documentation","title":"Visualization","ref":"/timeseriessurrogates/stable/#Visualization","content":" Visualization TimeseriesSurrogates.jl has defined a simple function  surroplot(x, s) . This comes into scope when  using Makie  (you also need a plotting backend). This functionality requires you to be using Julia 1.9 or later versions. Example: using TimeseriesSurrogates\nusing CairoMakie\nx = AR1() # create a realization of a random AR(1) process\nfig = surroplot(x, AAFT())\nsave(\"surroplot.png\", fig); # hide"},{"id":749,"pagetitle":"Documentation","title":"Citing","ref":"/timeseriessurrogates/stable/#Citing","content":" Citing Please use the following BiBTeX entry, or DOI, to cite TimeseriesSurrogates.jl: DOI: https://doi.org/10.21105/joss.04414 BiBTeX: @article{TimeseriesSurrogates.jl,\n    doi = {10.21105/joss.04414},\n    url = {https://doi.org/10.21105/joss.04414},\n    year = {2022},\n    publisher = {The Open Journal},\n    volume = {7},\n    number = {77},\n    pages = {4414},\n    author = {Kristian Agasøster Haaga and George Datseris},\n    title = {TimeseriesSurrogates.jl: a Julia package for generating surrogate data},\n    journal = {Journal of Open Source Software}\n} Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Theiler1994 J. Theiler, On the evidence for low-dimensional chaos in an epileptic electroencephalogram,  Phys. Lett. A 196 Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Ortega1998 Ortega, Guillermo J.; Louis, Enrique (1998). Smoothness Implies Determinism in Time Series: A Measure Based Approach. Physical Review Letters, 81(20), 4345–4348. doi:10.1103/PhysRevLett.81.4345 Ortega1998 Ortega, Guillermo J.; Louis, Enrique (1998). Smoothness Implies Determinism in Time Series: A Measure Based Approach. Physical Review Letters, 81(20), 4345–4348. doi:10.1103/PhysRevLett.81.4345 Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Theiler1991 J. Theiler, S. Eubank, A. Longtin, B. Galdrikian, J. Farmer, Testing for nonlinearity in time series: The method of surrogate data, Physica D 58 (1–4) (1992) 77–94. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. SchreiberSchmitz1996 T. Schreiber; A. Schmitz (1996). \"Improved Surrogate Data for Nonlinearity Tests\".  Phys. Rev. Lett. 77 (4) Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Nakamura2006 Nakamura, Tomomichi, Michael Small, and Yoshito Hirata. \"Testing for nonlinearity in irregular fluctuations with long-term trends.\" Physical Review E 74.2 (2006): 026205. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202. Small2001 Small et al., Surrogate test for pseudoperiodic time series data,  Physical Review Letters, 87(18) Small2001 Small et al., Surrogate test for pseudoperiodic timeseries data,  Physical Review Letters, 87(18) Thiel2006 Thiel, Marco, et al. \"Twin surrogates to test for complex synchronisation.\" EPL (Europhysics Letters) 75.4 (2006): 535. Miralles2015 Miralles, R., et al. \"Characterization of the complexity in short oscillating timeseries: An application to seismic airgun detonations.\" The Journal of the Acoustical Society of America 138.3 (2015): 1595-1603. Breakspear2003 Breakspear, M., Brammer, M., & Robinson, P. A. (2003). Construction of multivariate surrogate sets from nonlinear data using the wavelet transform. Physica D: Nonlinear Phenomena, 182(1-2), 1-22. Keylock2006 C.J. Keylock (2006). \"Constrained surrogate time series with preservation of the mean and variance structure\". Phys. Rev. E. 73: 036707. doi:10.1103/PhysRevE.73.036707. Paluš2008 Paluš, Milan (2008). Bootstrapping Multifractals: Surrogate Data from Random Cascades on Wavelet Dyadic Trees. Physical Review Letters, 101(13), 134101–. doi:10.1103/PhysRevLett.101.134101 Small2001 Small et al., Surrogate test for pseudoperiodic time series data,  Physical Review Letters, 87(18)"},{"id":752,"pagetitle":"Surrogates for irregular timeseries","title":"Surrogates for unevenly sampled time series","ref":"/timeseriessurrogates/stable/collections/irregular_surrogates/#Surrogates-for-unevenly-sampled-time-series","content":" Surrogates for unevenly sampled time series To derive a surrogate for unevenly sampled time series, we can use surrogate methods which which does not explicitly use the time axis like  RandomShuffle  or  BlockShuffle , or we need to use algorithms that take the irregularity of the time axis into account."},{"id":753,"pagetitle":"Surrogates for irregular timeseries","title":"Lomb-Scargle based surrogate","ref":"/timeseriessurrogates/stable/collections/irregular_surrogates/#Lomb-Scargle-based-surrogate","content":" Lomb-Scargle based surrogate The  IrregularLombScargle  surrogate is a form of a constrained surrogate which takes the Lomb-Scargle periodogram, which works on irregularly spaced data, to derive surrogates with similar phase distribution as the original time series. This function uses the simulated annealing algorithm [SchmitzSchreiber1999]  to minimize the Minkowski distance between the original periodogram and the surrogate periodogram. using TimeseriesSurrogates, CairoMakie, Random\n\n# Example data: random AR1 process with a time axis with unevenly\n# spaced time steps\nrng = Random.MersenneTwister(1234)\nx = AR1(n_steps = 300)\nN = length(x)\nt = (1:N) - rand(N)\n\n# Use simulated annealing based on convergence of Lomb-Scargle periodograms\n# The time series is relatively long, so set tolerance a bit higher than default.\nls = IrregularLombScargle(t, n_total = 100000, n_acc = 50000, tol = 5.0)\ns = surrogate(x, ls, rng)\n\nfig, ax = lines(t, x; label = \"original\")\nlines!(ax, t, s; label = \"surrogate\")\naxislegend(ax)\nfig SchmitzSchreiber1999 A.Schmitz T.Schreiber (1999). \"Testing for nonlinearity in unevenly sampled time series\"  Phys. Rev E"},{"id":756,"pagetitle":"Surrogates for nonstationary timeseries","title":"Surrogates for nonstationary time series","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Surrogates-for-nonstationary-time-series","content":" Surrogates for nonstationary time series Several of the methods provided by TimeseriesSurrogates.jl can be used to  construct surrogates for nonstationary time series, which the following examples illustrate."},{"id":757,"pagetitle":"Surrogates for nonstationary timeseries","title":"Truncated Fourier surrogates","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Truncated-Fourier-surrogates","content":" Truncated Fourier surrogates"},{"id":758,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTS","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTS](@ref)","content":" TFTS By retaining the lowermost frequencies of the frequency spectrum,  ( TFTS ) surrogates preserve long-term trends in the signals. using TimeseriesSurrogates\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 5 % lowermost frequencies.\nsurroplot(x, surrogate(x, TFTS(0.05)))"},{"id":759,"pagetitle":"Surrogates for nonstationary timeseries","title":"TAAFT","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TAAFT](@ref)","content":" TAAFT Truncated AAFT surrogates ( TAAFT ) are similar to TFTS surrogates, but also rescales back to the original values of the signal, so that the original signal and the surrogates consists of the same values. This, however, may introduce some bias, as demonstrated below. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 5% of the power spectrum corresponding to the lowest frequencies\ns_taaft_lo = surrogate(x, TAAFT(0.05))\nsurroplot(x, s_taaft_lo) using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Preserve 20% of the power spectrum corresponding to the highest frequencies\ns_taaft_hi = surrogate(x, TAAFT(-0.2))\nsurroplot(x, s_taaft_hi)"},{"id":760,"pagetitle":"Surrogates for nonstationary timeseries","title":"Truncated FT surrogates with trend removal/addition","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#Truncated-FT-surrogates-with-trend-removal/addition","content":" Truncated FT surrogates with trend removal/addition One solution is to combine truncated Fourier surrogates with detrending/retrending.  For time series with strong trends, Lucio et al. (2012) [Lucio2012]  proposes variants  of the truncated Fourier-based surrogates wherein the trend is removed prior to surrogate generation, and then added to the surrogate again after it has been generated.  This yields surrogates quite similar to those obtained when using truncated Fourier  surrogates (e.g.  TFTS ), but reducing the effects of endpoint mismatch that  affects regular truncated Fourier transform based surrogates. In principle, any trend could be removed/added to the signal. For now, the only  option is to remove a best-fit linear trend obtained by ordinary least squares  regression."},{"id":761,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTD","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTD](@ref)","content":" TFTD The  TFTD  surrogate is a random Fourier surrogate where  the lowest frequencies are preserved during surrogate generation, and a  linear trend is removed during preprosessing and added again after the  surrogate has been generated. The  TFTD  surrogates do a decent  job at preserving long term trends. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\ns = surrogate(x, TFTDRandomFourier(true, 0.02))\nsurroplot(x, s)"},{"id":762,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTDAAFT","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTDAAFT](@ref)","content":" TFTDAAFT The detrend-retrend extension of  TAAFT  is the  TFTDAAFT  method. The  TFTDAAFT  method adds a rescaling step to the  TFTD  method, ensuring that the surrogate and the original time series consist of the same values. Long-term trends in the data are also decently preserved by  TFTDAAFT , but like  TFTDAAFT , there is some bias. using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Keep 2 % of lowermost frequencies.\ns = surrogate(x, TFTDAAFT(0.02))\nsurroplot(x, s)"},{"id":763,"pagetitle":"Surrogates for nonstationary timeseries","title":"TFTDIAAFT","ref":"/timeseriessurrogates/stable/collections/nonstationary_surrogates/#[TFTDIAAFT](@ref)","content":" TFTDIAAFT TFTDIAAFT [Lucio2012]  surrogates are similar to  TFTDAAFT  surrogates, but the  TFTDIAAFT [Lucio2012]  method also uses an iterative process to better match the power spectra of the original signal and the surrogate (analogous to how the  IAAFT  method improves upon the  AAFT  method). using TimeseriesSurrogates\n\n# Example signal\nn = 300; a = 0.7; A = 20; σ = 15\nx = cumsum(randn(n)) .+ [(1 + a*i) .+ A*sin(2π/10*i) for i = 1:n] .+\n    [A^2*sin(2π/2*i + π) for i = 1:n] .+ σ .* rand(n).^2;\n\n# Keep 5% of lowermost frequences\ns = surrogate(x, TFTDIAAFT(0.05))\nsurroplot(x, s) Lucio2012 Lucio, J. H., Valdés, R., & Rodríguez, L. R. (2012). Improvements to surrogate data methods for nonstationary time series. Physical Review E, 85(5), 056202."},{"id":766,"pagetitle":"Contributing","title":"Contributing","ref":"/timeseriessurrogates/stable/contributor_guide/#Contributing","content":" Contributing"},{"id":767,"pagetitle":"Contributing","title":"Reporting issues","ref":"/timeseriessurrogates/stable/contributor_guide/#Reporting-issues","content":" Reporting issues If you are having issues with the code, find bugs or otherwise want to report something about the package, please submit an issue at our  GitHub repository . "},{"id":768,"pagetitle":"Contributing","title":"Feature requests","ref":"/timeseriessurrogates/stable/contributor_guide/#Feature-requests","content":" Feature requests If you have requests for a new method but can't implement it yourself, you can also report it as an  issue . The package developers or other volunteers might be able to help with the implementation.  Please mark method requests clearly as \"Method request: my new method...\", and provide a reference to a scientific publication that outlines the algorithm. "},{"id":769,"pagetitle":"Contributing","title":"Pull requests","ref":"/timeseriessurrogates/stable/contributor_guide/#Pull-requests","content":" Pull requests Pull requests for new surrogate methods are very welcome. Ideally, your implementation should use the same API as the existing methods:  Create a  struct  for your surrogate method, e.g.  struct MyNewSurrogateMethod <: Surrogate , that contain the parameters for the method. The docstring for the method should contain a reference to scientific publications detailing the algorithm, as well as the intended purpose of the method, and potential implementation details that differ from the original algorithm.  Implement  surrogenerator(x, method::MyNewSurrogateMethod) , where you pre-compute things for efficiency and return a  SurrogateGenerator  instance. Implement the  SurrogateGenerator{<:MyNewSurrogateMethod}  functor that produces surrogate time series on demand. This is where the precomputed things are used, and the actual algorithm is implemented. Then  surrogate(x, method::Surrogate)  will \"just work\".  If you find this approach difficult and already have a basic implementation of a new surrogate method, the package maintainers may be able to help structuring the code. Let us know in an  issue  or in a pull request!"},{"id":772,"pagetitle":"Utility systems","title":"Utility systems","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#Utility-systems","content":" Utility systems"},{"id":773,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.SNLST","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.SNLST","content":" TimeseriesSurrogates.SNLST  —  Function SNLST(n_steps, x₀, k) Dynamically linear process transformed by a strongly nonlinear static transformation (SNLST) [1] . Equations The system is by the following map: \\[x(t) = k x(t-1) + a(t)\\] with the transformation  $s(t) = x(t)^3$ . References source"},{"id":774,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.randomwalk","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.randomwalk","content":" TimeseriesSurrogates.randomwalk  —  Function randomwalk(n_steps, x₀) Linear random walk (AR(1) process with a unit root) [1] . This is an example of a nonstationary linear process. References source"},{"id":775,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.NSAR2","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.NSAR2","content":" TimeseriesSurrogates.NSAR2  —  Function NSAR2(n_steps, x₀, x₁) Cyclostationary AR(2) process [1] . References source"},{"id":776,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.AR1","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.AR1","content":" TimeseriesSurrogates.AR1  —  Function AR1(; n_steps, x₀, k, rng) Simple AR(1) model given by the following map: \\[x(t+1) = k x(t) + a(t),\\] where  $a(t)$  is a draw from a normal distribution with zero mean and unit variance.  x₀  sets the initial condition and  k  is the tunable parameter in the map.  rng  is a random number generator source"},{"id":777,"pagetitle":"Utility systems","title":"TimeseriesSurrogates.random_cycles","ref":"/timeseriessurrogates/stable/man/exampleprocesses/#TimeseriesSurrogates.random_cycles","content":" TimeseriesSurrogates.random_cycles  —  Function random_cycles(; periods=10 dt=π/20, σ = 0.05, frange = (0.8, 2.0)) Make a timeseries that is composed of  period  full sine wave periods, each with a random frequency in the range given by  frange , and added noise with std  σ . The sampling time is  dt . source 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202 1 Lucio et al., Phys. Rev. E  85 , 056202 (2012).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202"},{"id":780,"pagetitle":"Crash-course in timeseries surrogate testing","title":"Crash-course in timeseries surrogate testing","ref":"/timeseriessurrogates/stable/man/whatisasurrogate/#Crash-course-in-timeseries-surrogate-testing","content":" Crash-course in timeseries surrogate testing Note The summary here follows Sect. 7.4 from  Nonlinear Dynamics  by Datseris and Parlitz."},{"id":781,"pagetitle":"Crash-course in timeseries surrogate testing","title":"What is a surrogate timeseries?","ref":"/timeseriessurrogates/stable/man/whatisasurrogate/#What-is-a-surrogate-timeseries?","content":" What is a surrogate timeseries? A surrogate of a timeseries  x  is another timeseries  s  of equal length to  x . This surrogate  s  is generated from  x  so that it roughly preserves one or many pre-defined properties of  x , but is otherwise randomized. The upper panel in the figure below shows an example of a timeseries and one surrogate realization that preserves its both power spectrum and its amplitude distribution (histogram). Because of this preservation, the time series look similar. using TimeseriesSurrogates, CairoMakie\nx = LinRange(0, 20π, 300) .+ 0.05 .* rand(300)\nts = sin.(x./rand(20:30, 300) + cos.(x))\ns = surrogate(ts, IAAFT())\n\nsurroplot(ts, s)"},{"id":782,"pagetitle":"Crash-course in timeseries surrogate testing","title":"Performing surrogate hypothesis tests","ref":"/timeseriessurrogates/stable/man/whatisasurrogate/#Performing-surrogate-hypothesis-tests","content":" Performing surrogate hypothesis tests A surrogate test is a statistical test of whether a given timeseries satisfies or not a given hypothesis regarding its properties or origin. For example, the first surrogate methods were created to test the hypothesis, whether a given timeseries  x  that appears noisy may be the result of a linear stochastic process or not. If not, it may be a nonlinear process contaminated with observational noise. For the suitable hypothesis to test for, see the documentation strings of provided  Surrogate  methods or, even better, the review from Lancaster et al. (2018) [Lancaster2018] . To perform such a surrogate test, you need to: Decide what hypothesis to test against Pick a surrogate generating  method  that satisfies the chosen hypothesis Pick a suitable discriminatory statistic  q  with  q(x) ∈ Real . It must be a statistic that would obtain sufficiently different values for timeseries satisfying, or not, the chosen hypothesis. Compute  q(s)  for thousands of surrogate realizations  s = surrogate(x, method) Compare  q(x)  with the distribution of  q(s) . If  q(x)  is significantly outside the e.g., 5-95 confidence interval of the distribution, the hypothesis is rejected. This whole process is automated by  SurrogateTest , see the example below."},{"id":783,"pagetitle":"Crash-course in timeseries surrogate testing","title":"An educative example","ref":"/timeseriessurrogates/stable/man/whatisasurrogate/#An-educative-example","content":" An educative example Let's put everything together now to showcase how one would use this package to e.g., distinguish deterministic chaos contaminated with noise from actual stochastic timeseries, using the permutation entropy as a discriminatory statistic. First, let's visualize the timeseries using TimeseriesSurrogates # for surrogate tests\nusing DynamicalSystemsBase # to simulate logistic map\nusing ComplexityMeasures   # to compute permutation entropy\nusing Random: Xoshiro      # for reproducibility\nusing CairoMakie           # for plotting\n\n\n# AR1\nn = 400 # timeseries length\nrng = Xoshiro(1234567)\nx = TimeseriesSurrogates.AR1(; n_steps = n, k = 0.25, rng)\n# Logistic\nlogistic_rule(x, p, n) = @inbounds SVector(p[1]*x[1]*(1 - x[1]))\nds = DeterministicIteratedMap(logistic_rule, [0.4], [4.0])\nY, t = trajectory(ds, n-1)\ny = standardize(Y[:, 1]) .+ 0.5randn(rng, n) # 50% observational noise\n# Plot\nfig, ax1 = lines(y)\nax2, = lines(fig[2,1], x, color = Cycled(2))\nax1.title = \"deterministic + 50%noise\"\nax2.title = \"stochastic AR1\"\nfig Then, let's compute surrogate distributions for both timeseries using the permutation entropy as the discriminatory statistic and  RandomFourier  as the surrogate generation method perment(x) = entropy_normalized(SymbolicPermutation(; m = 3), x)\nmethod = RandomFourier()\n\nfig = Figure()\naxs = [Axis(fig[1, i]) for i in 1:2]\nNsurr = 1000\n\nfor (i, z) in enumerate((y, x))\n    sgen = surrogenerator(z, method)\n    qx = perment(z)\n    qs = map(perment, (sgen() for _ in 1:Nsurr))\n    hist!(axs[i], qs; label = \"pdf of q(s)\", color = Cycled(i))\n    vlines!(axs[i], qx; linewidth = 5, label = \"q(x)\", color = Cycled(3))\n    axislegend(axs[i])\nend\n\nfig we clearly see that the discriminatory value for the deterministic signal is so far out of the distribution that the null hypothesis that the timeseries is stochastic can be discarded with ease. This whole process can be easily automated with  SurrogateTest  as follows: test = SurrogateTest(perment, y, method; n = 1000, rng)\np = pvalue(test)\np < 0.001  # 99.9-th quantile confidence Lancaster2018 Lancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V., & Stefanovska, A. (2018). Surrogate data for hypothesis testing of physical systems. Physics Reports, 748, 1–60. doi:10.1016/j.physrep.2018.06.001"},{"id":786,"pagetitle":"Fourier-based","title":"Fourier-based","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Fourier-based","content":" Fourier-based Fourier based surrogates are a form of constrained surrogates created by taking the Fourier transform of a time series, then shuffling either the phase angles or the amplitudes of the resulting complex numbers. Then, we take the inverse Fourier transform, yielding a surrogate time series."},{"id":787,"pagetitle":"Fourier-based","title":"Random phase","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Random-phase","content":" Random phase using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\nphases = true\ns = surrogate(ts, RandomFourier(phases))\n\nsurroplot(ts, s)"},{"id":788,"pagetitle":"Fourier-based","title":"Random amplitude","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Random-amplitude","content":" Random amplitude using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\nphases = false\ns = surrogate(ts, RandomFourier(phases))\n\nsurroplot(ts, s)"},{"id":789,"pagetitle":"Fourier-based","title":"Partial randomization","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Partial-randomization","content":" Partial randomization"},{"id":790,"pagetitle":"Fourier-based","title":"Without rescaling","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Without-rescaling","content":" Without rescaling PartialRandomization  surrogates are similar to random phase surrogates,  but allows for tuning the \"degree\" of phase randomization. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\n\n# 50 % randomization of the phases\ns = surrogate(ts, PartialRandomization(0.5))\n\nsurroplot(ts, s)"},{"id":791,"pagetitle":"Fourier-based","title":"With rescaling","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#With-rescaling","content":" With rescaling PartialRandomizationAAFT  adds a rescaling step to the  PartialRandomization  surrogates to obtain surrogates that contain the same values as the original time series. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\n\n# 50 % randomization of the phases\ns = surrogate(ts, PartialRandomizationAAFT(0.7))\n\nsurroplot(ts, s)"},{"id":792,"pagetitle":"Fourier-based","title":"Amplitude adjusted Fourier transform (AAFT)","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Amplitude-adjusted-Fourier-transform-(AAFT)","content":" Amplitude adjusted Fourier transform (AAFT) using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\ns = surrogate(ts, AAFT())\n\nsurroplot(ts, s)"},{"id":793,"pagetitle":"Fourier-based","title":"Iterative AAFT (IAAFT)","ref":"/timeseriessurrogates/stable/methods/fourier_surrogates/#Iterative-AAFT-(IAAFT)","content":" Iterative AAFT (IAAFT) The IAAFT surrogates add an iterative step to the AAFT algorithm to improve similarity of the power spectra of the original time series and the surrogates. using TimeseriesSurrogates, CairoMakie\nts = AR1() # create a realization of a random AR(1) process\ns = surrogate(ts, IAAFT())\n\nsurroplot(ts, s)"},{"id":796,"pagetitle":"Multidimensional surrogates","title":"Multidimensional surrogates","ref":"/timeseriessurrogates/stable/methods/multidim/#Multidimensional-surrogates","content":" Multidimensional surrogates Multidimensional surrogates operate typically on input  StateSpaceSet s and output the same type."},{"id":797,"pagetitle":"Multidimensional surrogates","title":"Shuffle dimensions","ref":"/timeseriessurrogates/stable/methods/multidim/#Shuffle-dimensions","content":" Shuffle dimensions This surrogate was made to distinguish multidimensional data with  structure in the state space  from multidimensional noise. Here is a simple application that shows that the distinction is successful for a system that we know a-priori is deterministic and has structure in the state space (a chaotic attractor). using TimeseriesSurrogates\nusing DynamicalSystemsBase\nusing FractalDimensions: correlationsum\nusing CairoMakie\n\n# Create a trajectory from the towel map\nfunction towel_rule(x, p, n)\n    @inbounds x1, x2, x3 = x[1], x[2], x[3]\n    SVector( 3.8*x1*(1-x1) - 0.05*(x2+0.35)*(1-2*x3),\n    0.1*( (x2+0.35)*(1-2*x3) - 1 )*(1 - 1.9*x1),\n    3.78*x3*(1-x3)+0.2*x2 )\nend\nto = DeterministicIteratedMap(towel_rule, [0.1, -0.1, 0.1])\nX = trajectory(to, 10_000; Ttr = 100)[1]\n\ne = 10.0 .^ range(-1, 0; length = 10)\nCX = correlationsum(X, e; w = 5)\n\nle = log10.(e)\nfig, ax = lines(le, log10.(CX))\n\nsg = surrogenerator(X, ShuffleDimensions())\nfor i in 1:10\n    Z = sg()\n    CZ = correlationsum(Z, e)\n    lines!(ax, le, log.(CZ); color = (\"black\", 0.8))\nend\nax.xlabel = \"log(e)\"; ax.ylabel = \"log(C)\"\nfig"},{"id":800,"pagetitle":"Pseudo-periodic","title":"Pseudo-periodic","ref":"/timeseriessurrogates/stable/methods/pps/#Pseudo-periodic","content":" Pseudo-periodic using TimeseriesSurrogates\nt = 0:0.05:20π\nx = @. 4 + 7cos(t) + 2cos(2t + 5π/4)\nx .+= randn(length(x))*0.2\n\n# Optimal d, τ values deduced using DelayEmbeddings.jl\nd, τ = 3, 31\n\n# For ρ you can use `noiseradius`\nρ = 0.11\n\nmethod = PseudoPeriodic(d, τ, ρ, false)\ns = surrogate(x, method)\nsurroplot(x, s)"},{"id":803,"pagetitle":"Pseudo-periodic twin","title":"Pseudo-periodic twin surrogates","ref":"/timeseriessurrogates/stable/methods/ppts/#Pseudo-periodic-twin-surrogates","content":" Pseudo-periodic twin surrogates using TimeseriesSurrogates, CairoMakie\n\n# Example system from the original paper\nn, Δt = 500, 0.05\nf₁, f₂ = sqrt(3), sqrt(5)\nx = [8*sin(2π*f₁*t) + 4*sin(2π*f₂*t) for t = 0:Δt:Δt*n]\n\n# Embedding parameter, neighbor threshold and noise radius\nd, τ = 2, 6\nδ = 0.15\nρ = noiseradius(x, d, τ, 0.02:0.02:0.5)\nmethod = PseudoPeriodicTwin(d, τ, δ, ρ)\n\n# Generate the surrogate, which is a `d`-dimensional dataset.\nsurr_orbit = surrogate(x, method)\n\n# Get scalar surrogate time series from first and second column.\ns1, s2 = columns(surr_orbit)\n\n# Scalar time series versus surrogate time series\nfig = Figure()\nax_ts = Axis(fig[1,1:2]; xlabel = \"time\", ylabel = \"value\")\nlines!(ax_ts, s1; color = :red)\nlines!(ax_ts, x; color = :black)\n\n# Embedding versus surrogate embedding\nX = embed(x, d, τ)\nax2 = Axis(fig[2, 1]; xlabel = \"x(t)\", ylabel = \"x(t-$τ)\")\nlines!(ax2, X[:, 1], X[:, 2]; color = :black)\nscatter!(ax2, X[:, 1], X[:, 2]; color = :black, markersize = 4)\n\nps = Axis(fig[2,2]; xlabel=  \"s(t)\", ylabel = \"s(t-$τ)\")\nlines!(ps, s1, s2; color = :red)\nscatter!(ps, s1, s2; color = :black, markersize = 4)\n\nfig"},{"id":806,"pagetitle":"Shuffle-based","title":"Shuffle-based","ref":"/timeseriessurrogates/stable/methods/randomshuffle/#Shuffle-based","content":" Shuffle-based"},{"id":807,"pagetitle":"Shuffle-based","title":"Random shuffle (RS)","ref":"/timeseriessurrogates/stable/methods/randomshuffle/#Random-shuffle-(RS)","content":" Random shuffle (RS) Randomly shuffled surrogates are simply permutations of the original time series. Thus, they break any correlations in the signal. using TimeseriesSurrogates, CairoMakie\nx = AR1() # create a realization of a random AR(1) process\ns = surrogate(x, RandomShuffle())\nsurroplot(x, s)"},{"id":808,"pagetitle":"Shuffle-based","title":"Block shuffle (BS)","ref":"/timeseriessurrogates/stable/methods/randomshuffle/#Block-shuffle-(BS)","content":" Block shuffle (BS) Randomly shuffled surrogates are generated by dividing the original signal into blocks, then permuting those blocks. Block positions are randomized, and blocks at the end of the signal gets wrapped around to the start of the time series. Thus, they keep short-term correlations within blocks, but destroy any long-term dynamical information in the signal. using TimeseriesSurrogates, CairoMakie\nx = NSAR2(n_steps = 300)\n# We want to divide the signal into 8 blocks.\ns = surrogate(x, BlockShuffle(8))\np = surroplot(x, s)"},{"id":809,"pagetitle":"Shuffle-based","title":"Cycle shuffle (CSS)","ref":"/timeseriessurrogates/stable/methods/randomshuffle/#Cycle-shuffle-(CSS)","content":" Cycle shuffle (CSS) using TimeseriesSurrogates, CairoMakie\nx = random_cycles()\ns = surrogate(x, CycleShuffle())\np = surroplot(x, s)"},{"id":810,"pagetitle":"Shuffle-based","title":"Circular shift","ref":"/timeseriessurrogates/stable/methods/randomshuffle/#Circular-shift","content":" Circular shift using TimeseriesSurrogates, CairoMakie\nx = random_cycles()\ns = surrogate(x, CircShift(1:length(x)))\np = surroplot(x, s)"},{"id":813,"pagetitle":"Wavelet-based","title":"Wavelet surrogates","ref":"/timeseriessurrogates/stable/methods/wls/#Wavelet-surrogates","content":" Wavelet surrogates"},{"id":814,"pagetitle":"Wavelet-based","title":"WLS","ref":"/timeseriessurrogates/stable/methods/wls/#WLS","content":" WLS WLS  surrogates are constructed by taking the maximal overlap  discrete wavelet transform (MODWT) of the signal, shuffling detail  coefficients across dyadic scales, then inverting the transform to  obtain the surrogate. "},{"id":815,"pagetitle":"Wavelet-based","title":"Wavelet-IAAFT (WIAAFT) surrogates","ref":"/timeseriessurrogates/stable/methods/wls/#Wavelet-IAAFT-(WIAAFT)-surrogates","content":" Wavelet-IAAFT (WIAAFT) surrogates In  Keylock (2006) ,  IAAFT shuffling is used, yielding surrogates that preserve the local mean and  variance of the original signal, but randomizes nonlinear properties of the signal. This also preserves nonstationarities in the signal. To construct WIAAFT surrogates, rescaling must be enabled.  Note: the final iterative procedure of the WIAAFT surrogate method, after the rescaling step,  is not performed in our current implementation, so surrogates might differ a bit from results in Keylock (2006). For now, you have to do the iterative rescaling manually if desired. .  using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\n# Rescale surrogate back to original values\nmethod = WLS(IAAFT(), rescale = true)\ns = surrogate(x, method);\np = surroplot(x, s) Even without rescaling, IAAFT shuffling also yields surrogates with local properties  very similar to the original signal. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\n# Don't rescale back to original time series.\nmethod = WLS(IAAFT(), rescale = false)\ns = surrogate(x, method);\np = surroplot(x, s)"},{"id":816,"pagetitle":"Wavelet-based","title":"Other shuffling methods","ref":"/timeseriessurrogates/stable/methods/wls/#Other-shuffling-methods","content":" Other shuffling methods The choice of coefficient shuffling method determines how well and  which properties of the original signal are retained by the surrogates.  There might be use cases where surrogates do not need to perfectly preserve the  autocorrelation of the original signal, so additional shuffling  methods are provided for convenience. Using random shuffling of the detail coefficients does not preserve the  autocorrelation structure of the original signal.  using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\nmethod = WLS(RandomShuffle(), rescale = false)\ns = surrogate(x, method);\np = surroplot(x, s) Block shuffling the detail coefficients better preserve local properties because the shuffling is not completely random, but still does not  preserve the autocorrelation of the original signal. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\ns = surrogate(x, WLS(BlockShuffle(10), rescale = false));\np = surroplot(x, s) Random Fourier phase shuffling the detail coefficients does a decent job at preserving the autocorrelation. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n    [20*sin(2π/30*i) for i = 1:n] .+\n    [20*cos(2π/90*i) for i = 1:n] .+\n    [50*sin(2π/2*i + π) for i = 1:n] .+\n    σ .* rand(n).^2 .+\n    [0.5*t for t = 1:n];\n\ns = surrogate(x, WLS(RandomFourier(), rescale = false));\nsurroplot(x, s) To generate surrogates that preserve linear properties of the original signal, AAFT or IAAFT shuffling is required."},{"id":817,"pagetitle":"Wavelet-based","title":"RandomCascade","ref":"/timeseriessurrogates/stable/methods/wls/#RandomCascade","content":" RandomCascade RandomCascade  surrogates is another wavelet-based method that uses the regular discrete wavelet transform to generate surrogates. using TimeseriesSurrogates, Random\nRandom.seed!(5040)\nn = 500\nσ = 30\nx = cumsum(randn(n)) .+\n     [20*sin(2π/30*i) for i = 1:n] .+\n     [20*cos(2π/90*i) for i = 1:n] .+\n     [50*sin(2π/2*i + π) for i = 1:n] .+\n     σ .* rand(n).^2 .+\n     [0.2*t for t = 1:n];\n\ns = surrogate(x, RandomCascade());\nsurroplot(x, s)"},{"id":822,"pagetitle":"RecurrenceAnalysis.jl","title":"RecurrenceAnalysis.jl","ref":"/recurrenceanalysis/stable/#RecurrenceAnalysis.jl","content":" RecurrenceAnalysis.jl"},{"id":823,"pagetitle":"RecurrenceAnalysis.jl","title":"RecurrenceAnalysis","ref":"/recurrenceanalysis/stable/#RecurrenceAnalysis","content":" RecurrenceAnalysis  —  Module RecurrenceAnalysis.jl A Julia module that offers tools for computing Recurrence Plots and exploring them within the framework of Recurrence Quantification Analysis and Recurrence Network Analysis. It can be used as a standalone package, or as part of  DynamicalSystems.jl . To install it, run  import Pkg; Pkg.add(\"RecurrenceAnalysis\") . All further information is provided in the documentation, which you can either find  online  or build locally by running the  docs/make.jl  file. source References Want to learn more about recurrence quantification analysis? Here are some resources: Chapter 9 of  Nonlinear Dynamics , Datseris & Parlitz, Springer 2022. Webber, C. L., & Marwan, N. (2015). Recurrence Quantification Analysis. (C. L. Webber, & N. Marwan, Eds.). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-07155-8 Marwan, N., Carmen Romano, M., Thiel, M., & Kurths, J. (2007). Recurrence plots for the analysis of complex systems. Physics Reports, 438(5–6), 237–329. https://doi.org/10.1016/j.physrep.2006.11.001"},{"id":826,"pagetitle":"Recurrence Networks","title":"Recurrence Networks","ref":"/recurrenceanalysis/stable/networks/#Recurrence-Networks","content":" Recurrence Networks Recurrence matrices can be reinterpreted as adjacency matrices of  complex networks  embedded in state space, such that each node or vertex of the network corresponds to a point of the timeseries, and the links of the network connect pairs of points that are mutually close the phase space. The relationship between a recurrence matrix  $R$  and its corresponding adjacency matrix  $A$  is: \\[R[i,j] = A[i,j] - \\delta[i,j]\\] i.e. there is an edge in the associated network between every two neighboring points in the phase space, excluding self-connections (points in the  Line Of Identity  or main diagonal of  $R$ ). This definition assumes that  $A$  represents an  undirected graph , so  $R$  must be a symmetric matrix as corresponding to a  RecurrenceMatrix  or a  JointRecurrenceMatrix . While RQA characterizes the properties of line structures in the recurrence plots, which consider dynamical aspects (e.g. continuity of recurrences, length of sequences, etc.), the analysis of recurrence networks does not take into account time information, since network properties are independent of the ordering of vertices. On the other hand, recurrence network analysis (RNA) provides information about geometric characteristics of the state space, like homogeneity of the connections, clustering of points, etc. More details about the theoretical framework of RNA can be found in the following papers: R.V. Donner  et al. \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010) R.V. Donner  et al.  \"Complex Network Analysis of Recurrences\", in: Webber, C.L. & Marwan N. (eds.)  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 101-165 (2015)."},{"id":827,"pagetitle":"Recurrence Networks","title":"Creation and visualization of Recurrence Networks","ref":"/recurrenceanalysis/stable/networks/#Creation-and-visualization-of-Recurrence-Networks","content":" Creation and visualization of Recurrence Networks The  JuliaGraphs  organization provides multiple packages for Julia to create, visualize and analyze complex networks. In particular, the package  LightGraphs  defines the type  SimpleGraph  that can be used to represent undirected networks. Such graphs can be created from symmetric recurrence matrices, as in the following example with a Hénon map: using RecurrenceAnalysis, DynamicalSystemsBase\nusing Graphs: SimpleGraph\n\n# make trajectory of Henon map\nhenon_rule(x, p, n) = SVector(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nu0 = zeros(2)\np0 = [1.4, 0.3]\nhenon = DeterministicIteratedMap(henon_rule, u0, p0)\nX, t = trajectory(henon, 200)\n# Cast it into a recurrence network\nR = RecurrenceMatrix(X, 0.25; metric = Chebyshev())\nnetwork = SimpleGraph(R) {201, 2945} undirected simple Int64 graph There are various plotting tools that can be used to visualize such graphs. For instance, the following plot made with the package  GraphMakie.jl . using GraphMakie, CairoMakie\ngraphplot(network)"},{"id":828,"pagetitle":"Recurrence Networks","title":"Recurrence Network measures","ref":"/recurrenceanalysis/stable/networks/#Recurrence-Network-measures","content":" Recurrence Network measures LightGraphs has a large set of functions to extract local measures (associated to particular vertices or edges) and global coefficients associated to the whole network. For  SimpleGraph s created from recurrence matrices, as the variable  network  in the previous example, the vertices are labelled with numeric indices following the same ordering as the rows or columns of the given matrix. So for instance  degree(network, i)  would give the  degree  of the  i -th point of the timeseries (number of connections with other points), whereas  degree(network)  would give a vector of such measures ordered as the original timeseries. As in RQA, we provide a function that computes a selection of commonly used global RNA measures, directly from the recurrence matrix:"},{"id":829,"pagetitle":"Recurrence Networks","title":"RecurrenceAnalysis.rna","ref":"/recurrenceanalysis/stable/networks/#RecurrenceAnalysis.rna","content":" RecurrenceAnalysis.rna  —  Function rna(R::AbstractRecurrenceMatrix)\nrna(args...; kwargs...) Calculate a set of Recurrence Network parameters. The input  R  can be a symmetric recurrence matrix that is interpreted as the adjacency matrix of an undirected complex network, such that linked vertices are neighboring points in the phase space. Alternatively, the inputs can be a graph object or any valid inputs to the  SimpleGraph  constructor of the  Graphs  package. Return The returned value is a dictionary that contains the following entries, with the corresponding global network properties[1, 2]: :density : edge density, approximately equivalent to the global recurrence rate in the phase space. :transitivity : network transitivity, which describes the global clustering of points following Barrat's and Weigt's formulation [3]. :averagepath : mean value of the shortest path lengths taken over all pairs of connected vertices, related to the average separation between points in the phase. :diameter : maximum value of the shortest path lengths between pairs of connected vertices, related to the phase space diameter. References [1]: R.V. Donner  et al.  \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010)  DOI:10.1088/1367-2630/12/3/033025 [2]: R.V. Donner  et al. , The geometry of chaotic dynamics — a complex network perspective,  Eur. Phys. J.  B 84, 653–672 (2011)  DOI:10.1140/epjb/e2011-10899-1 [3]: A. Barrat & M. Weight, \"On the properties of small-world network models\",  The European Physical Journal B  13, 547–560 (2000)  DOI:10.1007/s100510050067 source Transitivity and global clustering coefficient The concept of clustering coefficient at local level (for individual nodes of the network) is clearly defined as the fraction of connecting nodes that are also connected between them, forming \"triangles\". But at global level it is a source of confusion: the term of \"global clustering coefficient\" was originally used by Watts and Strogatz [1] , referred to the average of local clustering coefficient across all the graph's nodes. But Barrat and Weigt proposed an alternative definition [2]  that characterizes the effective global dimensionality of the system, giving equal weight to all triangles in the network [3] . This second definition is often named with the distinctive term of \"transitivity\", as in the output of  rna , whereas the corresponding function of the  LightGraphs  package is  global_clustering_coefficient . The \"global clustering coefficient\" as by Watts and Strogatz could be obtained as  mean(local_clustering_coefficient(network))  – with  network  being a graph object as in the previous example. (The function  mean  is in the Julia standard library, and can be brought into scope with the command  using Statistics .) 1 D.J. Watts & S.H. Strogatz, \"Collective dynamics of 'small-world' networks\",  Nature 393 (6684), 440–442 (1998)  DOI:10.1038%2F30918 2 A. Barrat & M. Weight, \"On the properties of small-world network models\",  The European Physical Journal B  13, 547–560 (2000)   DOI:10.1007/s100510050067 3 R.V. Donner  et al.  \"Recurrence networks — a novel paradigm for nonlinear time series analysis\",  New Journal of Physics  12, 033025 (2010)  DOI:10.1088/1367-2630/12/3/033025"},{"id":832,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Quantification Analysis","ref":"/recurrenceanalysis/stable/quantification/#Recurrence-Quantification-Analysis","content":" Recurrence Quantification Analysis A  RecurrenceMatrix  can be analyzed in several ways to yield information about the dynamics of the trajectory. All these various  measures  and functions are collectively called \"Recurrence Quantification Analysis\" (RQA). To understand how each measure can be useful, we suggest to see the review articles listed in our documentation strings, namely: N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438 (5-6), 237-329 (2007). N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). You can also check the wikipedia page for  Recurrence quantification analysis . The functions described in this page all accept a recurrence matrix ( x ), see  RecurrenceMatrix ."},{"id":833,"pagetitle":"Recurrence Quantification Analysis","title":"RQA Measures","ref":"/recurrenceanalysis/stable/quantification/#RQA-Measures","content":" RQA Measures"},{"id":834,"pagetitle":"Recurrence Quantification Analysis","title":"All-in-one Bundle","ref":"/recurrenceanalysis/stable/quantification/#All-in-one-Bundle","content":" All-in-one Bundle In case you need all of the RQA-related functions (see below) and you don't want to write 10 lines of code to compute them all (since they are so many) we provide an all-in-one function that computes all of them and returns a dictionary with the results!"},{"id":835,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rqa","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rqa","content":" RecurrenceAnalysis.rqa  —  Function rqa(R; kwargs...) Calculate all RQA parameters of a recurrence matrix  R . See the functions referred to below for the definition of the different parameters and the default values of the arguments. Using this function is much more efficient than calling all individual functions one by one. Return The returned value contains the following entries, which can be retrieved as from a dictionary (e.g.  results[:RR] , etc.): :RR : recurrence rate (see  recurrencerate ) :DET : determinsm (see  determinism ) :L : average length of diagonal structures (see  dl_average ) :Lmax : maximum length of diagonal structures (see  dl_max ) :DIV : divergence (see  divergence ) :ENTR : entropy of diagonal structures (see  dl_entropy ) :TREND : trend of recurrences (see  trend ) :LAM : laminarity (see  laminarity ) :TT : trapping time (see  trappingtime ) :Vmax : maximum length of vertical structures (see  vl_max ) :VENTR : entropy of vertical structures (see  vl_entropy ) :MRT : mean recurrence time (see  meanrecurrencetime ) :RTE  recurrence time entropy (see  rt_entropy ) :NMPRT : number of the most probable recurrence time (see  nmprt ) All the parameters returned by  rqa  are  Float64  numbers, even for parameters like  :Lmax ,  :Vmax  or  :NMPRT  which are integer values. In the case of empty histograms (e.g. no existing vertical lines less than the keyword  lminvert ) the average and maximum values ( :L ,  :Lmax ,  :TT ,  :Vmax ,  :MRT ) are returned as  0.0  but their respective entropies ( :ENTR ,  :VENTR ,  :RTE ) are returned as  NaN . Keyword Arguments Standard keyword arguments are the ones accepted by the functions listed below, i.e.  theiler ,  lmin , and  border : theiler  is used to define a \"Theiler window\" around the central diagonal or \"line of identity\" (LOI): a region of points that are excluded in the calculation of RQA parameters, in order to rule out self-recurrences and apparent recurrences for smooth or high resolution data. The LOI is excluded by default for matrices of the types  RecurrenceMatrix  or  JointRecurrenceMatrix , but it is included for matrices of the type  CrossRecurrenceMatrix .  theiler=0  means that the whole matrix is scanned for lines.  theiler=1  means that the LOI is excluded. In general,  theiler=n  means that the  n  central diagonals are excluded (at both sides of the LOI, i.e. actually  2n-1  diagonals are excluded). lmin  is used to define the minimum line length in the parameters that describe the distributions of diagonal or vertical lines (it is set as 2 by default). border  is used to avoid border effects in the calculation of  :TREND  (cf.  trend ). In addition  theilerdiag ,  lmindiag  may be used to declare specific values that override the values of  theiler  and  lmin  in the calculation of parameters related to diagonal structures. Likewise,  theilervert  and  lminvert  can be used for the calculation of parameters related to vertical structures. The keyword argument  onlydiagonal  ( false  by default) can be set to  true  in order to restrict the analysis to the recurrence rate and the parameters related to diagonal structures ( :RR ,  :DET ,  :L ,  :Lmax ,  :DIV  and  :ENTR ), which makes this function slightly faster. Transitional note on the returned type In older versions, the  rqa  function returned a  NamedTuple , and in future versions it is planned to return a  Dict  instead. In both cases, the results can be indexed with square brackets and  Symbol  keys, as  result[:RR] ,  result[:DET] , etc. However, named tuples can also be indexed with \"dot syntax\", e.g.  result.RR , whereas this will not be possible with dictionaries, and there are other differences in the indexing and iteration of those two types. In order to facilitate the transition between versions, this function currently returns a  RQA  object that essentially works as a dictionary, but can also be indexed with the dot syntax (logging a deprecation warning). The returned type can also be specified as a first argument of  rqa  in order to replicate the output of different versions: rqa(NamedTuple, R...)  to obtain the output of the older version (as in 1.3). rqa(Dict, R...)  to obtain the output of the planned future version. rqa(RQA, R...)  to obtain the default current output (same as  rqa(R...) ) rqa(DT,R...)  to obtain the output as  DT  which is a subtype of  AbstractDict  (e.g.  rqa(OrderedDict,R...)  returns an  OrderedDict ) source Return values for empty histograms It may be the case that for a given recurrence matrix some structures do not exist at all. For example there are recurrence matrices that have no vertical lengths (or no vertical lengths with length less than  lmin ). In such cases the behavior of our RQA pipeline is the following: Quantities that represent maximum or average values are  0.0 . Quantities that represent entropies are  NaN . See also the  @windowed  macro for a windowed version of  rqa ."},{"id":836,"pagetitle":"Recurrence Quantification Analysis","title":"Example","ref":"/recurrenceanalysis/stable/quantification/#Example","content":" Example For example, here are the RQA measures for the same example trajectory we used to make  Simple Recurrence Plots using RecurrenceAnalysis, DynamicalSystemsBase\n\n# Create trajectory of Roessler system\n@inbounds function roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\np0 = [0.15, 0.2, 10.0]\nu0 = ones(3)\nro = CoupledODEs(roessler_rule, u0, p0)\nN = 2000; Δt = 0.05\nX, t = trajectory(ro, N*Δt; Δt, Ttr = 10.0)\n\n# Make a recurrence matrix with fixed threshold\nR = RecurrenceMatrix(X, 5.0)\n\n# Compute RQA measures\nrqa(R) RQA parameters in Dict{Symbol, Float64} with 14 entries:\n  :DIV   => 0.00118624\n  :LAM   => 0.999173\n  :Vmax  => 57.0\n  :VENTR => 3.57043\n  :Lmax  => 843.0\n  :MRT   => 172.716\n  :NMPRT => 2467.0\n  :RR    => 0.0953458\n  :RTE   => 3.78882\n  :TT    => 17.9861\n  :L     => 133.317\n  :ENTR  => 5.49263\n  :DET   => 0.999443\n  :TREND => -0.0228398"},{"id":837,"pagetitle":"Recurrence Quantification Analysis","title":"Classical RQA Measures","ref":"/recurrenceanalysis/stable/quantification/#Classical-RQA-Measures","content":" Classical RQA Measures"},{"id":838,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencerate","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.recurrencerate","content":" RecurrenceAnalysis.recurrencerate  —  Function recurrencerate(R[; theiler]) Calculate the recurrence rate of the recurrence matrix  R . Description The recurrence rate is calculated as: \\[RR = \\frac{1}{S} \\sum R\\] where  $S$  is the size of  R  or the region of  R  with potential recurrent points. There is not a unique definition of that denominator, which is defined as the full size of the matrix in many sources (e.g. [1]), whereas in others it is adjusted to remove the points of the LOI when they are excluded from the count [2,3]. For matrices of type  RecurrenceMatrix  or  JointRecurrenceMatrix , where the points around the central diagonal are usually excluded, the denominator is adjusted to the size of the matrix outside the Theiler window (by default equal to the LOI, and adjustable with the keyword argument  theiler ; see  rqa  for details). For matrices of type  CrossRecurrenceMatrix , where normally all points are analyzed, the denominator is always the full size of the matrix, regardless of the Theiler window that might be defined (none by default). Hint : to reproduce the calculations done following the formulas that use the full size of the matrix in the denominator, use  CrossRecurrenceMatrix(s,s,ε)  to define the recurrence matrix, instead of  RecurrenceMatrix(s,ε) , setting  theiler=1  (or  theiler=n  in general) to explicitly exclude the LOI or other diagonals around it. References [1] : N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438 (5-6), 237-329 (2007).  DOI:10.1016/j.physrep.2006.11.001 [2] : C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC, Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences, 26-94 (2005). URL: https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf [3] : N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). source"},{"id":839,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.determinism","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.determinism","content":" RecurrenceAnalysis.determinism  —  Function determinism(R[; lmin=2, theiler]) Calculate the determinism of the recurrence matrix  R : Description The determinism is calculated as: \\[DET = \\frac{\\sum_{l=lmin}{l P(l)}}{\\sum_{l=1}{l P(l)}} =\n\\frac{\\sum_{l=lmin}{l P(l)}}{\\sum R}\\] where  $l$  stands for the lengths of diagonal lines in the matrix, and  $P(l)$  is the number of lines of length equal to  $l$ . lmin  is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":840,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_average","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_average","content":" RecurrenceAnalysis.dl_average  —  Function dl_average(R[; lmin=2, theiler]) Calculate the average of the diagonal lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":841,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_max","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_max","content":" RecurrenceAnalysis.dl_max  —  Function dl_max(R[; lmin=2, theiler]) Calculate the longest diagonal line contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":842,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.dl_entropy","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.dl_entropy","content":" RecurrenceAnalysis.dl_entropy  —  Function dl_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the diagonal lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":843,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.divergence","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.divergence","content":" RecurrenceAnalysis.divergence  —  Function divergence(R[; theiler]) Calculate the divergence of the recurrence matrix  R  (actually the inverse of  dl_max ). source"},{"id":844,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trend","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.trend","content":" RecurrenceAnalysis.trend  —  Function trend(R[; border=10, theiler]) Calculate the trend of recurrences in the recurrence matrix  R . Description The trend is the slope of the linear regression that relates the density of recurrent points in the diagonals parallel to the LOI and the distance between those diagonals and the LOI. It quantifies the degree of system stationarity, such that in recurrence plots where points \"fade away\" from the central diagonal, the trend will have a negative value. It is calculated as: \\[TREND = 10^3\\frac{\\sum_{d=\\tau}^{\\tilde{N}}\\delta[d]\\left(RR[d]-\\langle RR[d]\\rangle\\right)}{\\sum_{d=\\tau}^{\\tilde{N}}\\delta[d]^2}\\] where  $RR[d]$  is the local recurrence rate of the diagonal  $d$ ,  $\\delta[d]$  is a balanced measure of the distance between that diagonal and the LOI,  $\\tau$  is the Theiler window (number of central diagonals that are excluded), and  $\\tilde{N}$  is the number of the outmost diagonal that is included. This parameter is expressed in units of variation recurrence rate every 1000 data points, hence the factor  $10^3$  in the formula [1]. The 10 outermost diagonals (counting from the corners of the matrix) are excluded by default to avoid \"border effects\". Use the keyword argument  border  to define a different number of excluded lines, and  theiler  to define the size of the Theiler window (see  rqa  for details). Note : In rectangular cross-recurrence plots (i.e. when the time series that originate them are not of the same length), the limits of the formula for TREND are not clearly defined. For the sake of consistency, this function limits the calculations to the biggest square matrix that contains the LOI. References [1] C.L. Webber & J.P. Zbilut, \"Recurrence Quantification Analysis of Nonlinear Dynamical Systems\", in: Riley MA & Van Orden GC,  Tutorials in Contemporary Nonlinear Methods for the Behavioral Sciences , 2005, 26-94. https://www.nsf.gov/pubs/2005/nsf05057/nmbs/nmbs.pdf source"},{"id":845,"pagetitle":"Recurrence Quantification Analysis","title":"Extended RQA Measures","ref":"/recurrenceanalysis/stable/quantification/#Extended-RQA-Measures","content":" Extended RQA Measures"},{"id":846,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.laminarity","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.laminarity","content":" RecurrenceAnalysis.laminarity  —  Function laminarity(R[; lmin=2, theiler]) Calculate the laminarity of the recurrence matrix  R . Description The laminarity is calculated as: \\[LAM = \\frac{\\sum_{v=lmin}{v P(l)}}{\\sum_{v=1}{v P(v)}} =\n\\frac{\\sum_{v=lmin}{v P(l)}}{\\sum R}\\] where  $v$  stands for the lengths of vertical lines in the matrix, and  $P(v)$  is the number of lines of length equal to  $v$ . lmin  is set to 2 by default, and this calculation rules out all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":847,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.trappingtime","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.trappingtime","content":" RecurrenceAnalysis.trappingtime  —  Function trappingtime(R[; lmin=2, theiler]) Calculate the trapping time of the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). The trapping time is the average of the vertical line structures and thus equal to  vl_average . source"},{"id":848,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_average","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_average","content":" RecurrenceAnalysis.vl_average  —  Function vl_average(R[; lmin=2, theiler]) Calculate the average of the vertical lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":849,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_max","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_max","content":" RecurrenceAnalysis.vl_max  —  Function vl_max(R[; lmin=2, theiler]) Calculate the longest vertical line contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":850,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.vl_entropy","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.vl_entropy","content":" RecurrenceAnalysis.vl_entropy  —  Function vl_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the vertical lines contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":851,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Time Measures","ref":"/recurrenceanalysis/stable/quantification/#Recurrence-Time-Measures","content":" Recurrence Time Measures"},{"id":852,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.meanrecurrencetime","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.meanrecurrencetime","content":" RecurrenceAnalysis.meanrecurrencetime  —  Function meanrecurrencetime(R[; lmin=2, theiler]) Calculate the mean recurrence time of the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Equivalent to  rt_average . source"},{"id":853,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.nmprt","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.nmprt","content":" RecurrenceAnalysis.nmprt  —  Function nmprt(R[; lmin=2, theiler]) Calculate the number of the most probable recurrence time (NMPRT), ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). This number indicates how many times the system has recurred using the recurrence time that appears most frequently, i.e it is the maximum value of the histogram of recurrence times [1]. References [1] : E.J. Ngamga  et al.  \"Recurrence analysis of strange nonchaotic dynamics\",  Physical Review E , 75(3), 036222(1-8) (2007)  DOI:10.1103/physreve.75.036222 source"},{"id":854,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_entropy","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rt_entropy","content":" RecurrenceAnalysis.rt_entropy  —  Function rt_entropy(R[; lmin=2, theiler]) Calculate the Shannon entropy of the recurrence times contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). Notes: This metric was first proposed in the paper \"Exploiting Nonlinear Recurrence  and Fractal Scaling Properties for Voice Disorder Detection\" as Recurrence Period  Density Entropy or Recurrence Probability Density Entropy (RPDE).  It is a normalized dimensionless metric in the range [0,1].  In the 2018 article \"Recurrence threshold selection for obtaining robust recurrence  characteristics in different embedding dimensions\", the indicator RPDE is explicitly  called Recurrence Time Entropy (RTE). Here RPDE and RTE are clearly the same indicator. source"},{"id":855,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.rt_average","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.rt_average","content":" RecurrenceAnalysis.rt_average  —  Function rt_average(R[; lmin=2, theiler]) Calculate the average of the recurrence times contained in the recurrence matrix  R , ruling out the lines shorter than  lmin  (2 by default) and all the points inside the Theiler window (see  rqa  for the default values and usage of the keyword argument  theiler ). source"},{"id":856,"pagetitle":"Recurrence Quantification Analysis","title":"Keyword table","ref":"/recurrenceanalysis/stable/quantification/#Keyword-table","content":" Keyword table Since most of the above functions can be fined tuned with keyword arguments, here is a table summarizing them that could be of use: Argument Default Functions Description theiler 0 for  CrossRecurrenceMatrix , 1 otherwise. recurrencerate ,  determinism ,  *_average ,  *_max ,  *_entropy ,  divergence ,  trend ,  laminarity ,  trappingtime ,   meanrecurrencetime ,  nmprt Theiler window: number of diagonals around the LOI  excluded  from the analysis. The value  0  means that the LOI is  included  in the analysis. Use  1  to exclude the LOI. lmin 2 determinism ,  *_average ,  *_max ,  *_entropy ,  divergence ,  laminarity ,  trappingtime ,   meanrecurrencetime ,  nmprt Minimum length of the recurrent structures (diagonal or vertical) considered in the analysis. border 10 trend Number of diagonals excluded from the analysis near the border of the matrix."},{"id":857,"pagetitle":"Recurrence Quantification Analysis","title":"Recurrence Structures Histograms","ref":"/recurrenceanalysis/stable/quantification/#Recurrence-Structures-Histograms","content":" Recurrence Structures Histograms The functions that we list in this page internally compute histograms of some recurrence structures, like e.g. the vertical lengths. You can access these values directly with the following function:"},{"id":858,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.recurrencestructures","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.recurrencestructures","content":" RecurrenceAnalysis.recurrencestructures  —  Function recurrencestructures(x::AbstractRecurrenceMatrix;\n                         diagonal=true,\n                         vertical=true,\n                         recurrencetimes=true,\n                         kwargs...) Return a dictionary with the histograms of the recurrence structures contained in the recurrence matrix  x , with the keys  \"diagonal\" ,  \"vertical\"  or  \"recurrencetimes\" , depending on what keyword arguments are given as  true . Description Each item of the dictionary is a vector of integers, such that the  i -th element of the vector is the number of lines of length  i  contained in  x . \"diagonal\"  counts the diagonal lines, i.e. the recurrent trajectories. \"vertical\"  counts the vertical lines, i.e. the laminar states. \"recurrencetimes\"  counts the vertical distances between recurrent states,   i.e. the recurrence times. All the points of the matrix are counted by default. The keyword argument  theiler  can be passed to rule out the lines around the main diagonal. See the arguments of the function  rqa  for further details. \"Empty\" histograms are represented always as  [0] . Notice : There is not a unique operational definition of \"recurrence times\". In the analysis of recurrence plots, usually the  \"second type\" of recurrence times as defined by Gao and Cai [1] are considered, i.e. the distance between consecutive (but separated) recurrent structures in the vertical direction of the matrix. But that distance is not uniquely defined when the vertical recurrent structures are longer than one point. The recurrence times calculated here are the distance between the midpoints of consecutive lines, which is a balanced estimator of the Poincaré recurrence times [2]. References [1] J. Gao & H. Cai. \"On the structures and quantification of recurrence plots\".  Physics Letters A , 270(1-2), 75–87 (2000) . [2] N. Marwan & C.L. Webber, \"Mathematical and computational foundations of recurrence quantifications\", in: Webber, C.L. & N. Marwan (eds.),  Recurrence Quantification Analysis. Theory and Best Practices , Springer, pp. 3-43 (2015). source"},{"id":859,"pagetitle":"Recurrence Quantification Analysis","title":"Windowed application","ref":"/recurrenceanalysis/stable/quantification/#Windowed-application","content":" Windowed application"},{"id":860,"pagetitle":"Recurrence Quantification Analysis","title":"RecurrenceAnalysis.windowed","ref":"/recurrenceanalysis/stable/quantification/#RecurrenceAnalysis.windowed","content":" RecurrenceAnalysis.windowed  —  Function windowed(rmat, f, width, step = 1; kwargs...) A convenience function that applies the RQA function  f , such as  determinism , to windowed views of the given recurrence matrix  rmat  with given window  width  and  step . The  kwargs...  are propagated to the call  f(rmat_view; kwargs...) . source"},{"id":863,"pagetitle":"Recurrence Plots","title":"Recurrence Plots","ref":"/recurrenceanalysis/stable/rplots/#Recurrence-Plots","content":" Recurrence Plots"},{"id":864,"pagetitle":"Recurrence Plots","title":"Recurrence Matrices","ref":"/recurrenceanalysis/stable/rplots/#Recurrence-Matrices","content":" Recurrence Matrices A  Recurrence plot  (which refers to the plot of a recurrence matrix) is a way to quantify  recurrences  that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the phase space that it was at some previous time. The central structure used in these recurrences is the (cross-) recurrence matrix: \\[R[i, j] = \\begin{cases}\n1 \\quad \\text{if}\\quad d(x[i], y[j]) \\le \\varepsilon\\\\\n0 \\quad \\text{else}\n\\end{cases}\\] where  $d(x[i], y[j])$  stands for the  distance  between trajectory  $x$  at point  $i$  and trajectory  $y$  at point  $j$ . Both  $x, y$  can be single timeseries, full trajectories or embedded timeseries (which are also trajectories). If  $x\\equiv y$  then  $R$  is called recurrence matrix, otherwise it is called cross-recurrence matrix. There is also the joint-recurrence variant, see below. With  RecurrenceAnalysis  you can use the following functions to access these matrices"},{"id":865,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceMatrix","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceMatrix","content":" RecurrenceAnalysis.RecurrenceMatrix  —  Type RecurrenceMatrix(x, ε; metric = Euclidean(), parallel::Bool) Create a recurrence matrix from trajectory  x  and with recurrence threshold specification  ε .  x  is either a  StateSpaceSet  for multivariate data or an  AbstractVector{<:Real}  for timeseries. If  ε::Real  is given, a  RecurrenceThreshold  is used to specify recurrences. Otherwise, any subtype of  AbstractRecurrenceType  may be given as  ε  instead. The keyword  metric , if given, must be any subtype of  Metric  from  Distances.jl  and defines the metric used to calculate distances for recurrences. By default the Euclidean metric is used, typical alternatives are  Chebyshev(), Cityblock() . The keyword  parallel  decides if the comptutation should be done in parallel using threads. Defaults to  length(x) > 500 && Threads.nthreads() > 1 . Description A (cross-)recurrence matrix is a way to quantify  recurrences  that occur in a trajectory. A recurrence happens when a trajectory visits the same neighborhood on the state space that it was at some previous time. The recurrence matrix is a numeric representation of a recurrence plot, described in detail in  [Marwan2007]  and  [Marwan2015] . It represents a a sparse square matrix of Boolean values that quantifies recurrences in the trajectory, i.e., points where the trajectory returns close to itself. Given trajectories  x, y , and asumming  ε isa Real , the matrix is defined as: R[i,j] = metric(x[i], y[i]) ≤ ε ? true : false with the  metric  being the distance function. The difference between a  RecurrenceMatrix  and a  CrossRecurrenceMatrix  is that in the first case  x === y . Objects of type  <:AbstractRecurrenceMatrix  are displayed as a  recurrenceplot . See also:  CrossRecurrenceMatrix ,  JointRecurrenceMatrix  and use  recurrenceplot  to turn the result of these functions into a plottable format. source"},{"id":866,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.CrossRecurrenceMatrix","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.CrossRecurrenceMatrix","content":" RecurrenceAnalysis.CrossRecurrenceMatrix  —  Type CrossRecurrenceMatrix(x, y, ε; kwargs...) Create a cross recurrence matrix from trajectories  x  and  y . See  RecurrenceMatrix  for possible value for  ε  and  kwargs . The cross recurrence matrix is a bivariate extension of the recurrence matrix. For the time series  x ,  y , of length  n  and  m , respectively, it is a sparse  n×m  matrix of Boolean values. Note that cross recurrence matrices are generally not symmetric irrespectively of  ε . source"},{"id":867,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.JointRecurrenceMatrix","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.JointRecurrenceMatrix","content":" RecurrenceAnalysis.JointRecurrenceMatrix  —  Type JointRecurrenceMatrix(x, y, ε; kwargs...) Create a joint recurrence matrix from trajectories  x  and  y . See  RecurrenceMatrix  for possible values for  ε  and  kwargs . The joint recurrence matrix considers the recurrences of the trajectories of  x  and  y  separately, and looks for points where both recur simultaneously. It is calculated by the element-wise multiplication of the recurrence matrices of  x  and  y . If  x  and  y  are of different length, the recurrences are only calculated until the length of the shortest one. See  RecurrenceMatrix  for details, references and keywords. source JointRecurrenceMatrix(R1::AbstractRecurrenceMatrix, R2::AbstractRecurrenceMatrix) Equivalent with  R1 .* R2 . source"},{"id":868,"pagetitle":"Recurrence Plots","title":"Advanced recurrences specification","ref":"/recurrenceanalysis/stable/rplots/#Advanced-recurrences-specification","content":" Advanced recurrences specification"},{"id":869,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.AbstractRecurrenceType","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.AbstractRecurrenceType","content":" RecurrenceAnalysis.AbstractRecurrenceType  —  Type AbstractRecurrenceType Supertype of all recurrence specification types. Instances of subtypes are given to  RecurrenceMatrix  and similar constructors to specify recurrences. Use  recurrence_threshold  to extract the numeric distance threshold. Possible subtypes are: RecurrenceThreshold(ε::Real) : Recurrences are defined as any point with distance  ≤ ε  from the referrence point. RecurrenceThresholdScaled(ratio::Real, scale::Function) : Here  scale  is a function of the distance matrix  dm  (see  distancematrix ) that is used to scale the value of the recurrence threshold  ε  so that  ε = ratio*scale(dm) . After the new  ε  is obtained, the method works just like the  RecurrenceThreshold . Specialized versions are employed if  scale  is  mean  or  maximum . GlobalRecurrenceRate(q::Real) : Here the number of total recurrence rate over the whole matrix is specified to be a quantile  q ∈ (0,1)  of the  distancematrix . In practice this yields (approximately) a ratio  q  of recurrences out of the total  Nx * Ny  for input trajectories  x, y . LocalRecurrenceRate(r::Real) : The recurrence threhsold here is point-dependent. It is defined so that each point of  x  has a fixed number of  k = r*N  neighbors, with ratio  r  out of the total possible  N . Equivalently, this means that each column of the recurrence matrix will have exactly  k  true entries. Notice that  LocalRecurrenceRate  does not guarantee that the resulting recurrence matrix will be symmetric. source"},{"id":870,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceThreshold","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceThreshold","content":" RecurrenceAnalysis.RecurrenceThreshold  —  Type RecurrenceThreshold(ε::Real) Recurrences are defined as any point with distance  ≤ ε  from the referrence point. See  AbstractRecurrenceType  for more. source"},{"id":871,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.RecurrenceThresholdScaled","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.RecurrenceThresholdScaled","content":" RecurrenceAnalysis.RecurrenceThresholdScaled  —  Type RecurrenceThresholdScaled(ratio::Real, scale::Function) Recurrences are defined as any point with distance  ≤ d  from the referrence point, where  d  is a scaled ratio (specified by  ratio, scale ) of the distance matrix. See  AbstractRecurrenceType  for more. source"},{"id":872,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.GlobalRecurrenceRate","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.GlobalRecurrenceRate","content":" RecurrenceAnalysis.GlobalRecurrenceRate  —  Type GlobalRecurrenceRate(rate::Real) Recurrences are defined as a constant global recurrence rate. See  AbstractRecurrenceType  for more. source"},{"id":873,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.LocalRecurrenceRate","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.LocalRecurrenceRate","content":" RecurrenceAnalysis.LocalRecurrenceRate  —  Type LocalRecurrenceRate(rate::Real) Recurrences are defined as a constant local recurrence rate. See  AbstractRecurrenceType  for more. source"},{"id":874,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.recurrence_threshold","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.recurrence_threshold","content":" RecurrenceAnalysis.recurrence_threshold  —  Function recurrence_threshold(rt::AbstractRecurrenceType, x [, y] [, metric]) → ε Return the calculated distance threshold  ε  for  rt . The output is real, unless  rt isa LocalRecurrenceRate , where  ε isa Vector . source"},{"id":875,"pagetitle":"Recurrence Plots","title":"Simple Recurrence Plots","ref":"/recurrenceanalysis/stable/rplots/#Simple-Recurrence-Plots","content":" Simple Recurrence Plots The recurrence matrices are internally stored as sparse matrices with Boolean values. Typically in the literature one does not sees the plots of the matrices  (hence \"Recurrence Plots\"). By default, when a Recurrence Matrix is created we \"show\" a mini plot of it which is a text-based scatterplot. Here is an example recurrence plot/matrix of a full trajectory of the Roessler system: using RecurrenceAnalysis, DynamicalSystemsBase\n\n# Create trajectory of Roessler system\n@inbounds function roessler_rule(u, p, t)\n    a, b, c = p\n    du1 = -u[2]-u[3]\n    du2 = u[1] + a*u[2]\n    du3 = b + u[3]*(u[1] - c)\n    return SVector(du1, du2, du3)\nend\np0 = [0.15, 0.2, 10.0]\nu0 = ones(3)\nro = CoupledODEs(roessler_rule, u0, p0)\nN = 2000; Δt = 0.05\nX, t = trajectory(ro, N*Δt; Δt, Ttr = 10.0)\n\n# Make a recurrence matrix with fixed threshold\nR = RecurrenceMatrix(X, 5.0)\nrecurrenceplot(R; ascii = true)     (2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold. \n    +------------------------------------------------------------+ \n    |         ..:'.::'       .::'.:'..:'       ..:'.::    ..  .:'| \n    |   ..  .::'.:''   .:' .:'..''.::'   .   .::'.:'    .:'..:'  | \n    | .:':.:'.::' .  .:'.::'.::'.:'    .:'.::'.::'..  ::'.::'.:' | \n    |:'.::'.::'      :.:::.:'    ..    '.::'.::'      :.:::.:'   | \n    | ''' ''' ' .. ..:'''''   ..:'..  .:'''''  ' .. ..:' '''   ..| \n    |         .:'.::'       .::'.:'.::'       .::'.:''    .  .:''| \n    |      .:' .:'..:    ..:'..:'.:''.:'    .:'..:'..:    ..:'.::| \n    |.  ..:'.::'.::'   .::'.::' :'.::'   .::'.::'.:'' ' .::'.:'' | \n    |'.::'.::'.:'    .:::.:'..  .:''   .:::.:::.:'    .:':::'    | \n    |:::::':::'      '.::'.:''  '      ':::'.::'      '.::'..    | \n    |'::' ::'   ..  .::'::'    .. .   .::'.::'   ..  .::'::'    .| \n    |         .:'.::'       .::'.:' .:'       ..:'.::'       .::'| \n    |      .::'.::'.:     .:''.:'.::'..     .::'.:''.: .   .:':.:| \n    |     :'' :'.::'.:  ::' ::'.:':.:'..   :' ::'.::'.:' ::'.::'.| \n    |     .   .:'..:'    .:'.::'.:''.:'    .  ..:'.::'   ..:'.::'| \n    |  ..:'.::'.::'   .::'.::'..'.::'   ..:'.::'.:''   .::'.:''  | \n    |.::'.:::.:'     :::::'.::' :''    :::.:::.:'     :':::'.::  | \n    |::::':::'       .::'.::'   .      .::'.::'       .::'.:''   | \n    | ''  ''  '.:' .:'' ''    .:'..: .:'' '''   .:' .:'' ''    .:| \n    |        .:'..:'.      .::'.:''.:'..      .:'.::'.    . .::'.| \n    |     .::'.::'.:'    .:''.:'.::'.:'    .::'.:':.:'.   .:'..:'| \n    |.  .:':.:'..:'  '.::'.::'  ''.:'   ..:':::'..''  '.::'.::'  | \n    |:::'.::'.:''    :::.::'..  ::'    ::'.::'.:'     :::.:''    | \n    |::::::::'       .::'.::'          .::::::'       .::'.::    | \n    |::::::'..       ::.::'            :::::'.        ::.:'      | \n    +------------------------------------------------------------+  typeof(R) RecurrenceMatrix{RecurrenceThreshold{Float64}} summary(R) \"(2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold.\" The above simple plotting functionality is possible through the package  UnicodePlots . The following function creates the plot:"},{"id":876,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.recurrenceplot","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.recurrenceplot","content":" RecurrenceAnalysis.recurrenceplot  —  Function recurrenceplot([io,] R; minh = 25, maxh = 0.5, ascii, kwargs...) -> u Create a text-based scatterplot representation of a recurrence matrix  R  to be displayed in  io  (by default  stdout ) using  UnicodePlots . The matrix spans at minimum  minh  rows and at maximum  maxh*displaysize(io)[1]  (i.e. by default half the display). As we always try to plot in equal aspect ratio, if the width of the plot is even less, the minimum height is dictated by the width. The keyword  ascii::Bool  can ensure that all elements of the plot are ASCII characters ( true ) or Unicode ( false ). The rest of the  kwargs  are propagated into  UnicodePlots.scatterplot . Notice that the accuracy of this function drops drastically for matrices whose size is significantly bigger than the width and height of the display (assuming each index of the matrix is one character). source Here is the same plot but using Unicode Braille characters recurrenceplot(R; ascii = false)     (2001, 2001) RecurrenceMatrix with 383575 recurrences of type RecurrenceThreshold. \n    ┌────────────────────────────────────────────────────────────┐ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⠞⠃⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁⣠⡶⠋⢀⣤⠞⠁⠀⠀⠀⠀⠀⠀⠀⢀⣤⠞⠁⣀⡴⠟⠀⠀⠀⠀⢀⣤⠀⠀⣀⡴⠛│ \n    │⠀⠀⠀⢀⡀⠀⠀⣠⣴⠟⠁⣠⡶⠋⠁⠀⠀⠀⣠⡴⠋⠀⣠⡾⠋⢀⣠⠘⠉⣀⡴⠟⠁⠀⠀⠀⢀⠀⠀⠀⣠⡴⠟⠁⣠⡾⠋⠀⠀⠀⠀⣠⡶⠋⢀⣠⡾⠋⠀⠀│ \n    │⠀⣠⡶⠋⢁⣤⡾⠋⢀⣴⠾⠋⠀⡀⠀⠀⢠⡾⠋⢀⣴⠟⠋⣀⡴⠟⠁⢠⡾⠋⠀⠀⠀⠀⢠⡾⠋⢀⣴⡾⠋⢀⣴⠟⠁⢀⡀⠀⠀⣴⡾⠋⣀⣴⠟⠁⣠⡴⠋⠀│ \n    │⡿⠋⣠⣴⡿⠋⣠⣶⠟⠁⠀⠀⠀⠀⠀⠀⢈⣠⣾⠟⢁⣠⡾⠋⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠈⣠⣶⠟⠋⣠⣾⠟⠁⠀⠀⠀⠀⠀⠀⢁⣤⡾⠟⢁⣠⡾⠋⠀⠀⠀│ \n    │⠀⠚⠛⠁⠀⠚⠛⠁⠀⠒⠀⣠⡄⠀⢀⣠⠞⠛⠁⠐⠛⠋⠀⠀⠀⢀⣠⠞⠋⢀⡄⠀⠀⣠⡼⠛⠁⠐⠛⠋⠀⠀⠒⠀⣠⠄⠀⢀⣤⠟⠋⠀⠐⠛⠉⠀⠀⠀⢀⣤│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⠞⠉⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁⣠⡾⠋⢀⣴⠞⠉⠀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⣀⡴⠛⠁⠀⠀⠀⠀⡀⠀⠀⣠⡴⠛⠁│ \n    │⠀⠀⠀⠀⠀⠀⣀⡴⠂⠀⣠⡾⠋⢀⣠⠞⠀⠀⠀⠀⢀⣠⡾⠋⢀⣤⠞⠁⣠⡴⠛⠁⣠⡶⠃⠀⠀⠀⠀⣠⡴⠋⢀⣠⡾⠋⢀⣤⠞⠀⠀⠀⠀⢀⣤⠾⠋⢀⣴⠞│ \n    │⡀⠀⠀⢀⣤⡾⠋⢀⣴⠟⠋⢀⡴⠟⠁⠀⠀⠀⣀⣴⠟⠁⣠⡴⠟⠁⠀⠸⠋⢀⣴⠞⠁⠀⠀⠀⢀⣴⠾⠋⣀⣴⠟⠁⣀⡴⠋⠁⠀⠒⠀⣠⣴⠟⠁⣠⡶⠋⠁⠀│ \n    │⠃⣠⣶⠟⠉⣠⣾⠟⠁⣠⡾⠋⠀⠀⠀⠀⣠⣾⠟⢁⣤⡾⠋⢀⡀⠀⠀⢠⡴⠋⠁⠀⠀⠀⢠⣶⠟⢁⣠⡾⠟⢁⣠⠾⠋⠀⠀⠀⠀⣤⡾⠋⢁⣴⡾⠋⠀⠀⠀⠀│ \n    │⣿⠟⣁⣴⡿⠋⣁⣴⠟⠋⠀⠀⠀⠀⠀⠀⠉⣠⣶⠿⠋⣠⡴⠛⠁⠀⠀⠈⠀⠀⠀⠀⠀⠀⠈⣡⣴⡿⠋⣠⣴⠟⠁⠀⠀⠀⠀⠀⠀⠉⣠⣾⠟⠁⣠⡄⠀⠀⠀⠀│ \n    │⠁⠾⠿⠋⠀⠾⠟⠁⠀⠀⠀⢀⡀⠀⠀⣠⡾⠟⠁⠰⠾⠋⠀⠀⠀⠀⣠⡄⠀⢀⠀⠀⠀⣀⡼⠿⠋⠠⠾⠟⠁⠀⠀⠀⣀⡀⠀⠀⣠⠿⠟⠁⠰⠞⠋⠀⠀⠀⠀⣠│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⠋⢀⣴⠞⠉⠀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⢠⡴⠋⠀⣠⡾⠋⠀⠀⠀⠀⠀⠀⠀⢀⣠⠞⠋⢀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⠁│ \n    │⠀⠀⠀⠀⠀⠀⢀⣴⠞⠁⣠⡴⠟⠁⣠⠶⠀⠀⠀⠀⠀⣠⡴⠋⠁⣠⡾⠋⢀⣴⠟⠁⣀⡤⠀⠀⠀⠀⠀⣀⣴⠟⠁⣠⡶⠋⠁⣠⠖⠀⢀⠀⠀⠀⣠⡾⠋⢁⣠⠞│ \n    │⠀⠀⠀⠀⠀⠶⠋⠁⠀⠾⠋⢀⣴⠞⠁⣠⡴⠀⠀⠰⠾⠋⠀⠰⠟⠁⣠⡼⠛⢁⣤⠞⠋⢀⡄⠀⠀⠀⠾⠋⠀⠰⠾⠋⣀⡴⠟⠁⣠⠾⠋⠀⠰⠟⠋⢀⡰⠟⠁⣠│ \n    │⠀⠀⠀⠀⠀⢀⠀⠀⠀⣠⡾⠋⢀⣤⠞⠋⠀⠀⠀⠀⣠⡶⠃⢀⣴⠞⠋⢠⡴⠛⠁⣠⡾⠋⠀⠀⠀⠀⣀⠀⠀⢀⣠⡾⠋⢀⣴⠞⠁⠀⠀⠀⢀⣠⠞⠋⢀⣴⠞⠁│ \n    │⠀⠀⢀⣠⡾⠋⢀⣴⠟⠋⢀⣴⠟⠁⠀⠀⠀⣀⣴⠟⠁⣠⣴⠟⠁⣠⠄⠈⢀⣴⠞⠉⠀⠀⠀⢀⣤⠞⠉⣀⣴⠟⠁⣠⡴⠛⠁⠀⠀⠀⣀⣴⠟⠁⣠⡶⠛⠁⠀⠀│ \n    │⣠⣴⠟⠁⣠⣾⠟⢁⣠⡾⠋⠀⠀⠀⠀⠀⠸⠟⢁⣴⡾⠋⢀⣴⠞⠁⠀⠰⠛⠁⠀⠀⠀⠀⠰⠟⢁⣠⡾⠟⢁⣤⡾⠋⠀⠀⠀⠀⠀⠾⠋⢁⣴⡾⠋⢀⣴⠆⠀⠀│ \n    │⠟⣁⣴⡿⠛⣁⣴⠿⠋⠀⠀⠀⠀⠀⠀⠀⢠⣶⡿⠋⣠⣶⠟⠁⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⢠⣴⡿⠋⣠⣴⠟⠉⠀⠀⠀⠀⠀⠀⠀⣠⣶⠟⠉⣠⡾⠋⠁⠀⠀⠀│ \n    │⠀⠉⠉⠀⠀⠉⠁⠀⠀⠁⢀⡴⠂⠀⣠⡶⠋⠁⠀⠈⠉⠀⠀⠀⠀⣠⡶⠋⢀⣤⠆⠀⣀⡴⠋⠉⠀⠈⠉⠁⠀⠀⠀⣀⡴⠂⠀⣠⡶⠋⠁⠀⠈⠉⠀⠀⠀⠀⣠⠶│ \n    │⠀⠀⠀⠀⠀⠀⠀⠀⣠⠾⠋⢀⣤⠞⠋⢀⠀⠀⠀⠀⠀⠀⢀⣴⠞⠉⣀⡰⠋⠁⣠⡾⠋⢀⡀⠀⠀⠀⠀⠀⠀⣠⠾⠋⢀⣴⠟⠁⢀⠀⠀⠀⠀⠄⠀⢀⣴⠟⠁⣀│ \n    │⠀⠀⠀⠀⠀⢀⣴⠞⠁⣠⡴⠟⠁⣠⡶⠋⠀⠀⠀⠀⣠⡶⠛⠁⣠⡾⠋⢀⣴⠟⠁⣀⡴⠋⠀⠀⠀⠀⣀⣴⠟⠁⣠⡶⠋⢁⣠⠾⠋⢀⠀⠀⠀⣠⡾⠋⢀⣠⠾⠋│ \n    │⡤⠀⠀⣠⡾⠛⢁⣤⡾⠋⢀⣤⠞⠁⠀⠀⠈⢀⣴⡾⠋⢀⣴⠟⠉⠀⠀⠈⠁⣠⠾⠋⠀⠀⠀⢀⣠⡾⠋⢁⣴⡾⠋⢀⡤⠈⠁⠀⠀⠉⢀⣴⠿⠋⣀⣴⠟⠁⠀⠀│ \n    │⣁⣴⡿⠋⣠⣴⠟⠋⣠⡴⠛⠁⠀⠀⠀⠀⣼⠟⢉⣠⡾⠟⠁⣠⠄⠀⠀⢰⠟⠁⠀⠀⠀⠀⢰⡿⠋⣠⣶⠟⠁⣠⡶⠋⠀⠀⠀⠀⠀⣾⠟⢁⣠⡾⠋⠁⠀⠀⠀⠀│ \n    │⡿⢋⣴⣾⠟⢁⣴⡾⠋⠀⠀⠀⠀⠀⠀⠀⢠⣴⡿⠋⣀⣴⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⠟⣁⣴⡿⠋⠀⠀⠀⠀⠀⠀⠀⣠⣴⡿⠋⣀⣴⠆⠀⠀⠀⠀│ \n    │⣾⣿⢟⣡⣾⡿⠋⣀⡀⠀⠀⠀⠀⠀⠀⠀⠻⢋⣤⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢋⣥⣾⠟⠉⣀⠀⠀⠀⠀⠀⠀⠀⠀⠟⢋⣤⡾⠋⠀⠀⠀⠀⠀⠀│ \n    └────────────────────────────────────────────────────────────┘  As you can see, the Unicode based plotting doesn't display nicely everywhere. It does display perfectly in e.g. VSCode, which is where it is the default printing type."},{"id":877,"pagetitle":"Recurrence Plots","title":"Advanced Recurrence Plots","ref":"/recurrenceanalysis/stable/rplots/#Advanced-Recurrence-Plots","content":" Advanced Recurrence Plots A text-based plot is cool, fast and simple. But often one needs the full resolution offered by the data of a recurrence matrix. There are two more ways to plot a recurrence matrix using  RecurrenceAnalysis :"},{"id":878,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.coordinates","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.coordinates","content":" RecurrenceAnalysis.coordinates  —  Function coordinates(R) -> xs, ys Return the coordinates of the recurrence points of  R  (in indices). source"},{"id":879,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.grayscale","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.grayscale","content":" RecurrenceAnalysis.grayscale  —  Function grayscale(R [, bwcode]; width::Int, height::Int, exactsize=false) Transform the recurrence matrix  R  into a full matrix suitable for plotting as a grayscale image. By default it returns a matrix with the same size as  R , but switched axes, containing \"black\" values in the cells that represent recurrent points, and \"white\" values in the empty cells and interpolating in-between for cases with both recurrent and empty cells, see below. The numeric codes for black and white are given in a 2-element tuple as a second optional argument. Its default value is  (0.0, 1.0) , i.e. black is coded as  0.0  (no brightness) and white as  1.0  (full brightness). The type of the elements in the tuple defines the type of the returned matrix. This must be taken into account if, for instance, the image is coded as a matrix of integers corresponding to a grayscale; in such case the black and white codes must be given as numbers of the required integer type. The keyword arguments  width  and  height  can be given to define a custom size of the image. If only one dimension is given, the other is automatically calculated. If both dimensions are given, by default they are adjusted to keep an aspect proportional to the original matrix, such that the returned matrix fits into a matrix of the given dimensions. This automatic adjustment can be disabled by passing the keyword argument  exactsize=true . If the image has different dimensions than  R , the cells of  R  are distributed in a grid with the size of the image, and a gray level between white and black is calculated for each element of the grid, proportional to the number of recurrent points contained in it. The levels of gray are coded as numbers of the same type as the black and white codes. It is advised to use  width, height  arguments for large matrices otherwise plots using functions like e.g.  heatmap  could be misleading. source For example, here is the representation of the above  R  from the Roessler system using both plotting approaches: using CairoMakie\nfig = Figure(resolution = (1000,500))\n\nax = Axis(fig[1,1])\nxs, ys = coordinates(R)\nscatter!(ax, xs, ys; color = :black, markersize = 1)\nax.limits = ((1, size(R, 1)), (1, size(R, 2)));\nax.aspect = 1\n\nax2 = Axis(fig[1,2]; aspect = 1)\nRg = grayscale(R)\nheatmap!(ax2, Rg; colormap = :grays)\nfig and here is exactly the same process, but using a delay embedded trajectory instead using DelayEmbeddings\n\ny = X[:, 2]\nτ = estimate_delay(y, \"mi_min\")\nm = embed(y, 3, τ)\nE = RecurrenceMatrix(m, 5.0; metric = \"euclidean\")\n\nxs, ys = coordinates(E)\nfig, ax = scatter(xs, ys; markersize = 1)\nax.aspect = 1\nfig which justifies why recurrence plots are so fitting to be used in embedded timeseries. Careful when using Recurrence Plots It is easy when using  grayscale  to not change the width/height parameters. The width and height are important when in  grayscale  when the matrix size exceeds the display size! Most plotting libraries may resample arbitrarily or simply limit the displayed pixels, so one needs to be extra careful. Besides graphical problems there are also other potential pitfalls dealing with the conceptual understanding and use of recurrence plots. All of these are summarized in the following paper which we suggest users to take a look at: N. Marwan,  How to avoid potential pitfalls in recurrence plot based data analysis , Int. J. of Bifurcations and Chaos ( arXiv )."},{"id":880,"pagetitle":"Recurrence Plots","title":"Skeletonized Recurrence Plots","ref":"/recurrenceanalysis/stable/rplots/#Skeletonized-Recurrence-Plots","content":" Skeletonized Recurrence Plots The finite size of a recurrence plot can cause border effects in the recurrence quantification-measures  rqa . Also the sampling rate of the data and the chosen recurrence threshold selection method ( fixed ,  fixedrate ,  FAN ) plays a crucial role. They can cause the thickening of diagonal lines in the recurrence matrix. Both problems lead to biased line-based RQA-quantifiers and is discussed in: K.H. Kraemer & N. Marwan,  Border effect corrections for diagonal line based recurrence quantification analysis measures ,  Phys. Lett. A 2019 ."},{"id":881,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.skeletonize","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.skeletonize","content":" RecurrenceAnalysis.skeletonize  —  Function skeletonize(R) → R_skel Skeletonize the  RecurrenceMatrix R  by using the algorithm proposed by Kraemer & Marwan  [Kraemer2019] . This function returns  R_skel , a recurrence matrix, which only consists of diagonal lines of \"thickness\" one. source Consider, e.g. a skeletonized version of a simple sinusoidal: using RecurrenceAnalysis, DelayEmbeddings, CairoMakie\n\ndata = sin.(2*π .* (0:400)./ 60)\nY = embed(data, 3, 15)\n\nR = RecurrenceMatrix(Y, 0.25; fixedrate=true)\nR_skel = skeletonize(R)\n\nfig = Figure(resolution = (1000,600))\nax = Axis(fig[1,1]; title = \"RP of monochromatic signal\")\nheatmap!(ax, grayscale(R); colormap = :grays)\n\nax = Axis(fig[1,2]; title = \"skeletonized RP\")\nheatmap!(ax, grayscale(R_skel); colormap = :grays)\nfig This way spurious diagonal lines get removed from the recurrence matrix, which would otherwise effect the quantification based on these lines."},{"id":882,"pagetitle":"Recurrence Plots","title":"Distance matrix","ref":"/recurrenceanalysis/stable/rplots/#Distance-matrix","content":" Distance matrix The distance function used in  RecurrenceMatrix  and co. can be specified either as any  Metric  instance from  Distances . In addition, the following function returns a matrix with the cross-distances across all points in one or two trajectories:"},{"id":883,"pagetitle":"Recurrence Plots","title":"RecurrenceAnalysis.distancematrix","ref":"/recurrenceanalysis/stable/rplots/#RecurrenceAnalysis.distancematrix","content":" RecurrenceAnalysis.distancematrix  —  Function distancematrix(x [, y = x], metric = \"euclidean\") Create a matrix with the distances between each pair of points of the time series  x  and  y  using  metric . The time series  x  and  y  can be  AbstractDataset s or vectors or matrices with data points in rows. The data point dimensions (or number of columns) must be the same for  x  and  y . The returned value is a  n×m  matrix, with  n  being the length (or number of rows) of  x , and  m  the length of  y . The metric can be any of the  Metric s defined in the  Distances  package  and defaults to  Euclidean() . source"},{"id":884,"pagetitle":"Recurrence Plots","title":"StateSpaceSet  reference","ref":"/recurrenceanalysis/stable/rplots/#StateSpaceSet-reference","content":" StateSpaceSet  reference"},{"id":885,"pagetitle":"Recurrence Plots","title":"StateSpaceSets.StateSpaceSet","ref":"/recurrenceanalysis/stable/rplots/#StateSpaceSets.StateSpaceSet","content":" StateSpaceSets.StateSpaceSet  —  Type StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T} A dedicated interface for sets in a state space. It is an  ordered container of equally-sized points  of length  D . Each point is represented by  SVector{D, T} . The data are a standard Julia  Vector{SVector} , and can be obtained with  vec(ssset::StateSpaceSet) . Typically the order of points in the set is the time direction, but it doesn't have to be. When indexed with 1 index,  StateSpaceSet  is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more. StateSpaceSet  also supports almost all sensible vector operations like  append!, push!, hcat, eachrow , among others. Description of indexing In the following let  i, j  be integers,  typeof(X) <: AbstractStateSpaceSet  and  v1, v2  be  <: AbstractVector{Int}  ( v1, v2  could also be ranges, and for performance benefits make  v2  an  SVector{Int} ). X[i] == X[i, :]  gives the  i th point (returns an  SVector ) X[v1] == X[v1, :] , returns a  StateSpaceSet  with the points in those indices. X[:, j]  gives the  j th variable timeseries (or collection), as  Vector X[v1, v2], X[:, v2]  returns a  StateSpaceSet  with the appropriate entries (first indices being \"time\"/point index, while second being variables) X[i, j]  value of the  j th variable, at the  i th timepoint Use  Matrix(ssset)  or  StateSpaceSet(matrix)  to convert. It is assumed that each  column  of the  matrix  is one variable. If you have various timeseries vectors  x, y, z, ...  pass them like  StateSpaceSet(x, y, z, ...) . You can use  columns(dataset)  to obtain the reverse, i.e. all columns of the dataset in a tuple. Marwan2007 N. Marwan  et al. , \"Recurrence plots for the analysis of complex systems\",  Phys. Reports 438*(5-6), 237-329 (2007) Marwan2015 N. Marwan & C.L. Webber,  Recurrence Quantification Analysis. Theory and Best Practices Springer (2015) Kraemer2019 Kraemer, K.H., Marwan, N. (2019).  Border effect corrections for diagonal line based recurrence quantification analysis measures. Physics Letters A 383(34) ."}]